---
title: "Task 1 RmD"
output:
  pdf_document: default
  html_document: default
date: "2024-11-12"
---

## T1: Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn=-1)
```

```{r parameters}
seed = 443
csv_file_one = "data1.csv"
training_size = 0.8 # Proportion of dataset that is to be split for training
packages = list("class", 
                "randomForest",
                "tree",
                "dplyr",
                "tidyr",
                "gbm", 
                "MASS",
                "ggplot2",
                "pROC",
                "MLmetrics",
                "caret",
                "e1071",
                "glmnet",
                "adabag"
)

# For knn-training
kk_vector = c(1:10) 

# GBM Parameters
num_trees = 1000

## Define grid of lambda (shrinkage) values to evaluate
lambda_grid <- c(0.001, 0.01,0.03,0.05)

## Initialize a vector to store test errors for each lambda
test_errors <- numeric(length(lambda_grid))

## Number of Principal Components to train with
no_prin_comps <- c(2:30)

## Number of folds for Cross-Validation 
no_folds <- 5

## Threshold tuning
thresholds <- seq(0.1, 0.9, 0.1)

## Logistic Ridge/Lasso Tuning
lambda_tuning = c(10^seq(-4, 1, 1))


```

```{r load_packages, include=FALSE}
for (package in packages){
  if(!require(package, character.only = TRUE)){
    install.packages(package, character.only = TRUE)
  } else {
    library(package, character.only = TRUE)
  }
}
```

## T1: Binary classification

```{r train_test_split}
set.seed(seed)

task_one_df <- read.csv(csv_file_one)

# str(task_one_df)
task_one_df$label <- as.factor(task_one_df$label)
task_one_df$label_as_numeric <- as.numeric(task_one_df$label) -1 #Labels cells that are T-Reg cells as 1 and CD4 cells as 0
# Split Data into Test and Train (We should all use the same data)

train_split = as.integer(nrow(task_one_df)*training_size)

train_indices <- sample(1:nrow(task_one_df), train_split)

train_data <- task_one_df[train_indices,]
test_data <- task_one_df[-train_indices,]
```

## T1: Functions 

```{r functions}
fun_calc_balanced_accuracy <- function(confusion_matrix, pos_label, neg_label){
  tp <- confusion_matrix[pos_label, pos_label]
  tn <- confusion_matrix[neg_label, neg_label]
  fp <- confusion_matrix[pos_label, neg_label]
  fn <- confusion_matrix[neg_label, pos_label]
  
  balanced_accuracy <- 0.5 * (tp / (tp + fn) + tn / (tn + fp)) 
  
  return(balanced_accuracy)
}

cv_f1_score <- function(data, lev = NULL, model = NULL) {
  
  precision <- posPredValue(data$pred, data$obs, positive = "TREG")
  
  recall <- sensitivity(data$pred, data$obs, positive = "TREG")
  
  F1 <- ifelse((precision + recall) == 0, 
               0, 
               2 * precision * recall / (precision + recall))
  
  accuracy <- mean(data$pred == data$obs)
  # Return a named vector
  c(Accuracy = accuracy, F1 = F1)
}

```


## T1.1: Exploratory data analysis and summary statistics (Peter?)

```{r EDA}
dim(task_one_df)  # The data has 5471 observations and 4125 variables-- 4124 originally, 1 added (label)
table(task_one_df$label) # 3356 observations are labelled as CD4+T and 2115 are labelled as TREG

mean_table <- task_one_df %>%

  dplyr::select(-label_as_numeric) %>%

  group_by(label) %>%

  summarise(across(everything(), mean, na.rm = TRUE)) %>%

  pivot_longer(cols = -label, names_to = 'genes', values_to = 'mean_value') %>%

  pivot_wider(names_from = label, values_from = mean_value) %>%

  mutate(diff = abs(`CD4+T` - TREG))

  # mutate(pct_diff = 100 * (diff / ((`CD4+T` + TREG)/2) )) ## This doesn't result in very nice graphs, perhaps stick to the absolute diff??


top_3_abs_diff <- mean_table %>% top_n(3, diff) %>% arrange(desc(diff))

# top_3_pct_diff <- mean_table %>% top_n(3, pct_diff) %>% arrange(desc(pct_diff)) # Doesn't result in very good graphs... perhaps remove

par(mfrow=c(1,3))

### Add x-axis label and title
for (var in top_3_abs_diff$genes) {
  formula <- as.formula(paste(var, "~ label"))
  plot <- boxplot(formula, data = task_one_df, col=c("blue", "red"))
  print(plot)
}


# # Doesn't result in very good graphs.. perhaps remove
# for (var in top_3_pct_diff$genes) {
#   formula <- as.formula(paste(var, "~ label"))
#   plot <- boxplot(formula, data = task_one_df, col=c("blue", "red"))
#   print(plot)
#   quantile(task_one_df$var)
# }

## There are decent differences between the ranking of absolute difference in means of groups
## and the ranking of percentage difference in means of groups
## perhaps we create box plots of the top 3-5 of each metric??

## Perhaps do summary(XXX) of a few variables??
## If want to display multiple subgraphs --> par(mfrow=c(x,y))

### Next steps --> 
  ### (1) instead of absolute value difference, include also
  ### percentage change. then plot box plots for the top 5? 10? variables
  ### that appear to have the clearest distinction between the two labels
  ### these specific variables provide a good baseline for what we expect
  ### our subsequent models to choose for the most important fields
  
## Checking if there are any rows that have missing values... Keep this hashed unless you want to check, because it takes a while to run
# for (i in 1:nrow(task_one_df)) {
#   if (any(is.na(task_one_df[i, ]))) {
#     print(paste("Row", i, "contains NA values."))
#   }
# }

# twenty_random_columns <- sample(names(task_one_df)[-1], 20)
# 
# ## Create boxplots for the random sample of columns
# for (column in twenty_random_columns) {
#     p <- ggplot(task_one_df, aes_string(x = "label", y = column)) +
#         geom_jitter(width = 0.2, height = 0) +  # Use jitter to avoid overplotting
#         ggtitle(paste("Scatter Plot of", column, "by Label")) +
#         theme_minimal()
#     print(p)
# }

```

## T1.2: Training and evaluating various classifiers

### Without PCA

#### Linear Discriminant Analysis (LDA) (Imar)

```{r lda_train}
lda_start_time  <- Sys.time()
lda_fit <- lda(label ~ . -label_as_numeric, data=train_data)
lda_end_time  <- Sys.time()
lda_runtime <- lda_end_time - lda_start_time # Time difference of 4.937018 mins

lda_pred <- predict(lda_fit, test_data)$class
```

```{r lda_save, include = FALSE}
write.csv(lda_pred, file = "predictions_1.2/lda_predictions.csv", row.names = TRUE)
```

```{r lda_load, include = FALSE}
lda_pred <- read.csv("predictions_1.2/lda_predictions.csv") %>% 
  
  dplyr::select(x)

lda_pred <- lda_pred[,'x']
```

```{r lda_confusion}
lda_confusion <- table(predicted_label = lda_pred, true_label = test_data$label)
lda_confusion
```

```{r lda_accuracy}
lda_accuracy <- mean(lda_pred == test_data$label)

cat("Accuracy of LDA:", lda_accuracy, "\n")
```

```{r lda_balanced_accuracy}
lda_balanced_accuracy <- fun_calc_balanced_accuracy(lda_confusion, 
                                                    pos_label = 'TREG', 
                                                    neg_label = 'CD4+T')

cat("Balanced Accuracy of LDA:", lda_balanced_accuracy, "\n")
```

```{r lda_f1_score}
lda_f1_score <- F1_Score(y_true= test_data$label, y_pred = lda_pred)
cat("F1 Score of LDA:", lda_f1_score, "\n")
```

```{r lda_roc}
lda_pred_numeric <- ifelse(lda_pred == "TREG", 1, 0)
  
lda_roc_curve <- roc(
  response = test_data$label_as_numeric,
  predictor = lda_pred_numeric, 
  levels = c(0,1),
  direction = "<"
  )

plot(lda_roc_curve, col = "red", main = "ROC Curve for LDA")

lda_auc <- auc(lda_roc_curve)
lda_auc
```

```{r lda_misclass}
lda_misclas <- mean(lda_pred != test_data$label)
cat("Misclassification error rate of LDA:", lda_misclas, "\n")
```

#### Logistic classifier (Imar)

```{r logreg_train}
logreg_start_time <- Sys.time()

logreg_fit <- glm(label_as_numeric ~ . -label,
                  data = train_data,
                  family = binomial)

logreg_run_time <- Sys.time() - logreg_start_time # Time difference of 17.64516 mins

logreg_probs <- predict(logreg_fit, newdata = test_data, type = "response")
logreg_pred <- ifelse(logreg_probs > 0.5, 'TREG', 'CD4+T')
```

```{r logreg_save, include = FALSE}
write.csv(logreg_probs, file = "predictions_1.2/logreg_probabilities.csv", row.names = TRUE)
write.csv(logreg_pred, file = "predictions_1.2/logreg_predictions.csv", row.names = TRUE)
```

```{r logreg_load, include = FALSE}
logreg_pred <- read.csv("predictions_1.2/logreg_predictions.csv") %>% 
  
  dplyr::select(x)

logreg_pred <- logreg_pred[,'x']

logreg_probs <- read.csv("predictions_1.2/logreg_probabilities.csv") %>% 
  
  dplyr::select(x)

logreg_probs <- logreg_probs[,'x']
```

```{r logreg_confusion}
log_confusion <- table(prediction = logreg_pred, truth = test_data$label)
log_confusion
```

```{r logreg_accuracy}
log_accuracy <- mean(logreg_pred == test_data$label)
cat("Accuracy of Logistic Regression:", log_accuracy, "\n")
```

```{r logreg_balanced_accuracy}
log_balanced_accuracy <- fun_calc_balanced_accuracy(log_confusion,
                                                    pos_label = 'TREG',
                                                    neg_label = 'CD4+T')

cat("Balanced Accuracy of Logistic Regression:", log_balanced_accuracy, "\n")
```

```{r logreg_f1_score}
log_f1_score <- F1_Score(y_true= test_data$label, y_pred = logreg_pred)
cat("F1 Score for Logistic Regression:", log_f1_score, "\n")
```

```{r logreg_ROC_AUC}
roc_curve_log <- roc(test_data$label, logreg_probs, levels=c("TREG", "CD4+T"), direction = ">")

plot(roc_curve_log, col="blue", main="ROC Curve for Logistic Regression")

log_auc <- as.numeric(auc(roc_curve_log))
cat("The AUC of the ROC of Logistic Regression:", log_auc, "\n")
```

```{r logreg_Misclass}
log_misclas <- mean(glm_pred != test_data_log$label)
cat("The Misclassification rate of Logistic Regression:", log_misclas, "\n")
```

#### Quadratic Discriminant Analysis (QDA) (Chi)

The current dimensionality of the data is too high for a QDA classifier. During training, the covariance matrix estimation for each class, k, fails. This occurs because, when the data is partitioned into k classes, the number of observations, n, is much smaller than the number of features, p (i.e n << p). As a result, the covariance matrix estimate for class k becomes singular or nearly singular. Therefore, without dimension reduction or regularisation, training this classifier is not feasible.

```{r qda_Train}
# ## CAUTION: currently hard-coded numbers here. To be changed (Chi)
# options(expressions = 10000)
# qda_covariates = mean_table$genes[1:1100]
# ## CAUTION: currently hard-coded numbers here. To be changed (Chi)
# 
# qda_formula = as.formula(
#   paste(
#     "label ~",
#     paste(qda_covariates, collapse = " + ")
#     ))
# 
# qda_fit <- qda(qda_formula, data = train_data)
```

```{r qda_Confusion}
# qda_pred <- predict(qda_fit, test_data)$class
# 
# table(predicted = qda_pred, observed = test_data$label)
```

```{r qda_Misclass}
# qda_misclas = mean(qda_pred != test_data$label)
# 
# cat("Misclassification rate:", qda_misclas*100, "%")
```

```{r qda_ROC}
# qda_pred_numeric <- ifelse(qda_pred == "TREG", 1, 0)
#   
# qda_roc_curve <- roc(
#   response = test_data$label_as_numeric,
#   predictor = qda_pred_numeric, 
#   levels = c(0,1),
#   direction = "<"
#   )
# 
# plot(qda_roc_curve, col = "red", main = "ROC Curve for QDA")
# 
# qda_auc <- auc(qda_roc_curve)
# qda_auc
```

```{r QDA_TBD}
# qda_accuracy <- "n/a"
# qda_balanced_accuracy <- "n/a"
# qda_f1_score <- "n/a"
# qda_runtime <- "n/a"
```


#### Nearest Neighbor Classifier (k-NN) (Chi)

```{r kNN_train, eval = FALSE}
set.seed(seed)

knn_train_X <- train_data %>% 
  dplyr::select(-label, -label_as_numeric)

knn_train_Y <- train_data$label

knn_test_X <- test_data %>% 
  dplyr::select(-label, -label_as_numeric)

knn_start_time <- Sys.time()
knn_pred <- knn(train = knn_train_X, 
                test = knn_test_X, 
                cl = knn_train_Y,
                k = 8)
knn_end_time <- Sys.time()

knn_train_runtime <- knn_end_time - knn_start_time
print(knn_train_runtime) #50 mins for k=1 to k=10 

rm(knn_train_X, knn_train_Y, knn_test_X)
```

```{r kNN_save, include = FALSE}
write.csv(knn_pred, file = "knn_predictions.csv", row.names = TRUE)
```


```{r kNN_load, include = FALSE}
knn_pred <- read.csv("knn_predictions.csv") %>% 
  dplyr::select(-X)

knn_pred <- knn_pred[, 'x']
```

```{r knn_pca_confusion}
knn_confusion <- table(predicted = knn_pred,
                           observed = test_data$label)

print(knn_confusion)
```

```{r knn_diagnostics}
knn_accuracy <- mean(knn_pred == test_data_pca$label)

knn_balanced_accuracy <- fun_calc_balanced_accuracy(knn_confusion,
                                                        pos_label = 'TREG',
                                                        neg_label = 'CD4+T')

knn_f1_score <- F1_Score(y_true = test_data_pca$label, 
                             y_pred = knn_pred,
                             positive = "TREG")

knn_roc_curve <- roc(
  response = test_data_pca$label,
  predictor = as.numeric(knn_pred), 
  levels = c('CD4+T','TREG'),
  direction = "<"
  )

knn_auc <- auc(knn_roc_curve)

knn_misclas <- mean(knn_pred != test_data_pca$label)

plot(knn_roc_curve, col = "red", main = "ROC Curve for QDA")
```

```{r knn_evaluation}
# Create a table of the evaluation metrics
knn_metrics_table <- data.frame(
  Metric = c("Accuracy", "Balanced Accuracy", "F1 Score", "AUC (ROC)", "Misclassification Rate"),
  kNN = c(knn_accuracy, knn_balanced_accuracy, knn_f1_score, knn_auc, knn_misclas)
)

print(knn_metrics_table)
```

#### Gradient Boosting Decision Trees (GBDT) (Imar)

```{r gbmparams}
#Number of trees
num_trees = 1000
# Define grid of lambda (shrinkage) values to evaluate
lambda_grid <- c(0.001, 0.01,0.03,0.05)

# Initialize a vector to store test errors for each lambda
test_errors <- numeric(length(lambda_grid))
```

```{r dataprepgbm}
test_data_cleaned_gbm <- test_data[, !colnames(test_data) %in% c("label", "label")]
```

```{r gbmloop, eval=FALSE}
gbm_optimal_lambda_start <- Sys.time()
# Loop over each lambda value
for (i in seq_along(lambda_grid)) {
  lambda <- lambda_grid[i]
  
  # Train the gbm model with the current lambda (shrinkage) value
  gbm_model <- gbm(label_as_numeric ~. - label, data = train_data, 
                   distribution = "bernoulli", 
                   n.trees = num_trees, 
                   interaction.depth = 4, 
                   shrinkage = lambda, 
                   cv.folds = 5, 
                   verbose = FALSE)
  
  # Make predictions on the test set using the optimal number of trees
  predictions <- predict(gbm_model, newdata = test_data_cleaned_gbm, n.trees = num_trees)
  
  # Calculate the Mean Squared Error on the test set
  test_errors[i] <- mean((predictions - test_data_cleaned_gbm$label_as_numeric)^2)
}
gbm_optimal_lambda_end <- Sys.time()
gbm_optimal_lambda_total_time <- gbm_optimal_lambda_end - gbm_optimal_lambda_start
print(gbm_optimal_lambda_total_time)
```

```{r gbmtse, eval=FALSE}
# Combine lambda values and test errors into a data frame for plotting
error_df <- data.frame(lambda = lambda_grid, test_error = test_errors)

# Plot the test error over the range of lambda values
ggplot(error_df, aes(x = lambda, y = test_error)) +
  geom_line() +
  geom_point() +
  labs(title = "Test Error vs Lambda (Shrinkage)", 
       x = "Lambda (Shrinkage)", 
       y = "Test Mean Squared Error") +
  theme_minimal()

```

The results above show use that the GLM model with the lowest Test Mean Squared Error, will be the one with the lowest lambda value we assigned of 0.001. This does not necessarily mean that the best Gradient Descent Boosting model that will work best will be the best model for our data, as it may not account for over fitting. However, we will continue using the GBDT model with a shrinkage value of 0.001 to analyze the model and to compare it to our other models.

```{r gbm_model}
# Set a single shrinkage value
lambda <- 0.001

# Measure the time it takes to run 
gbm_start_time <- Sys.time()
# Train the gbm model
smallest_lambda_gbm_model <- gbm(label_as_numeric ~ . - label, 
                 data = train_data, 
                 distribution = "bernoulli", 
                 n.trees = num_trees, 
                 interaction.depth = 4, 
                 shrinkage = lambda, 
                 cv.folds = 5, 
                 verbose = FALSE)

# Get the best number of trees based on cross-validation
best_trees <- gbm.perf(smallest_lambda_gbm_model, method = "cv")

# Measure time after model training
gbm_end_time <- Sys.time()
gbm_model_training_time <- gbm_end_time - gbm_start_time
cat("GBM Model training time:", gbm_model_training_time, "\n")

# Measure time for predictions
gbm_start_time_pred <- Sys.time()
# Make predictions on the test set using the best number of trees
gbm_pred_probs <- predict(smallest_lambda_gbm_model, newdata = test_data_cleaned_gbm, n.trees = best_trees, type = "response")
gbm_pred_classes <- ifelse(gbm_pred_probs > 0.5, 1, 0)

# Measure time after predictions
gbm_end_time_pred <- Sys.time()
gbm_prediction_time <- gbm_end_time_pred - gbm_start_time_pred
cat("Prediction time:", gbm_prediction_time, "\n")
```

```{r gbm_save, include = FALSE}
write.csv(gbm_pred_classes, file = "predictions_1.2/gbm_predictions.csv", row.names = TRUE)
```

```{r gbm_load, include = FALSE}
gbm_pred_classes <- read.csv("predictions_1.2/gbm_predictions.csv") %>% 
  
  dplyr::select(x)

gbm_pred_classes <- gbm_pred_classes[,'x']
```

```{r gbm_Confusion}
gbm_confusion_matrix <- table(Predicted = gbm_pred_classes, Actual = test_data_cleaned_gbm$label_as_numeric)
print(gbm_confusion_matrix)

gbm_balanced_accuracy <- fun_calc_balanced_accuracy(gbm_confusion_matrix, 
                                  pos_label = "1", 
                                  neg_label = "0")

cat("The balanced accuracy is", gbm_balanced_accuracy, "\n")
```

```{r gbm_Misclass}
gbm_misclas <- mean(gbm_pred_classes != test_data_cleaned_gbm$label_as_numeric)
cat("Misclassification Rate:", gbm_misclas, "\n")
```

```{r gbm_Accuracy}
gbm_accuracy <- mean(gbm_pred_classes == test_data_cleaned_gbm$label_as_numeric)
cat("Accuracy:", gbm_accuracy, "\n")
```

```{r gbm_ROC_AUC}
gbm_roc_curve <- roc(test_data_cleaned_gbm$label_as_numeric, gbm_pred_probs)
gbm_auc <- auc(gbm_roc_curve)
cat("AUC:", gbm_auc, "\n")
plot(gbm_roc_curve, main = "ROC Curve")
```

```{r gbm_f1_score}
gbm_f1_score <- F1_Score(y_pred = gbm_pred_classes, y_true = test_data_cleaned_gbm$label_as_numeric)
cat("F1 Score:", gbm_f1_score, "\n")
```

The accuracy for the gradient descent decision tree model (where the shrinkage term was equal to 0.001) was 0.904. The AUC was equal to 0.972 and the Test mean squared error was equal to 0.109. Misclassification rate was equal to 0.096. And F1 score was equal to 0.928

#### Random Forest (Imar)

Above we use a tree search to find.... When using the tree search, we get that there are 11 features that are the most important to classifying whether a cell is a TREG cell or a CD4+T cell. However, we can see based on the tree that there are certain features that appear more than once. Based on the tree plot, we can determine that the most important regressor that helps determine if a cell is a TREG cell or a CD4+T cell is the IL7R. From the National Library of medicine ("<https://www.ncbi.nlm.nih.gov/gene/3575>"), IL7R is a protein encoded by a gene that plays a critical role in the development of lymph nodes. Based on the analysis above, we decided to create a Random Forest regression that will have 12 terminal nodes.

```{r rfmodel}
# RF model
start_time_rf <- Sys.time()

rf.rna_features <-randomForest(label~. - label_as_numeric, data=train_data, mtry=64, importance=TRUE, n.tree = 5000) # RF Model

end_time_rf <- Sys.time()
rf_training_time <- end_time_rf - start_time_rf
cat("Random Forest model training time:", rf_training_time, "\n")

# Predictions
start_time_rf_pred <- Sys.time()
yhat.rf <-predict(rf.rna_features, newdata=test_data)

end_time_rf_pred <- Sys.time()
rf_prediction_time <- end_time_rf_pred - start_time_rf_pred
cat("Random Forest prediction time:", rf_prediction_time, "\n")

```

```{r rf_save, include = FALSE}
write.csv(yhat.rf, file = "predictions_1.2/rf_predictions.csv", row.names = TRUE)
```

```{r rf_load, include = FALSE}
yhat.rf <- read.csv("predictions_1.2/rf_predictions.csv") %>% 
  
  dplyr::select(x)

yhat.rf <- yhat.rf[,'x']
```

```{r rf_confusion}
rf_confusion_matrix <- confusion_matrix <- table(Predicted = yhat.rf, Actual = test_data$label)
print(rf_confusion_matrix)
rf_balanced_accuracy <- fun_calc_balanced_accuracy(rf_confusion_matrix,
                                                   pos_label = "TREG",
                                                   neg_label = "CD4+T")

cat("The balanced accuracy is", rf_balanced_accuracy, "\n")
```

```{r rf_misclass}
rf_misclas <- mean(yhat.rf != test_data$label)
cat("Random Forest Misclassification Rate:", rf_misclas, "\n")
```

```{r rf_accuracy}
rf_accuracy <- mean(yhat.rf == test_data$label)
cat("Random Forest Accuracy:", rf_accuracy, "\n")
```

```{r rf_roc_auc}
yhat.rf_numeric <- ifelse(yhat.rf == "CD4+T", 0, ifelse(yhat.rf == "TREG", 1, NA))
roc_rf <- roc(test_data$label, yhat.rf_numeric)
rf_auc <- auc(roc_rf)
cat("Random Forest AUC:", rf_auc, "\n")
plot(roc_rf, main = "Random Forest ROC Curve")
```

```{r rf_f1_score}
rf_f1_score <- F1_Score(y_pred = yhat.rf, y_true = test_data$label)
cat("F1 Score:", rf_f1_score, "\n")
```

Using a random forest tree model, we see that the accuracy of the model was 0.936 Which is quite accurate! Misclassification Rate was equal to 0.064. The F1 score was equal to 0.9504

#### Support Vector Machine (SVM) (Chi)

##### Linear SVM Model
```{r svm_linear_model}
## Clean the test data for the SVM task
test_data_cleaned_for_svm <- test_data[, !colnames(test_data) %in% c("label_as_numeric", "label")]

## K-fold CV for hyperparameter tuning of 'cost' for the training data with a linear kernel
start_time_svm <- Sys.time()

tune.out = tune(svm, 
                label ~ . -label_as_numeric, 
                data = train_data, 
                kernel = "linear", 
                ranges = list(cost = c(0.01, 0.1, 1, 10)))

summary(tune.out)

end_time_svm <- Sys.time()
svm_training_time <- end_time_svm - start_time_svm # 1.18 hrs (~ 1hr & 10 min)
```

```{r svm_linear_tuned_model}
# Save the model with the cost that results in the lowest cross validation error rate
bestmod <-  tune.out$best.model
summary(bestmod)
```

```{r svm_predictions}
# Predictions
start_time_svm_pred <-  Sys.time()
yhat.svm = predict(bestmod, test_data)

end_time_svm_pred <- Sys.time()
svm_prediction_time <- end_time_svm_pred - start_time_svm_pred
cat("SVM prediction time:", svm_prediction_time, "\n")
```

```{r svm_save, include = FALSE}
write.csv(yhat.svm, 
          file = "predictions_1.2/svm_predictions.csv",
          row.names = TRUE)
```


```{r svm_load, include = FALSE}
yhat.svm <- read.csv("predictions_1.2/svm_predictions.csv") %>% 
  
  dplyr::select(x)

yhat.svm <- yhat.svm[,'x']
```

```{r svm_confusion}
svm_confusion_matrix <- table(predict = yhat.svm, 
                              truth = test_data$label)
print(svm_confusion_matrix)
```

```{r svm_diagnostics}
svm_balanced_accuracy <- fun_calc_balanced_accuracy(svm_confusion_matrix,
                                                    pos_label = "TREG",
                                                    neg_label = "CD4+T")

svm_misclass <- mean(yhat.svm != test_data$label)

svm_accuracy <- mean(yhat.svm == test_data$label)

svm_roc <- roc(test_data$label, 
               as.numeric(yhat.svm),
               levels = c("CD4+T", "TREG"),
               direction = "<")

svm_auc <- auc(svm_roc)

plot(svm_roc, main = "SVM ROC Curve") # Make the limits of the axes 0-1

svm_f1_score <- F1_Score(y_pred = yhat.svm, 
                         y_true = test_data$label,
                         positive = 'TREG')

```

```{r svm_evaluation}
# Create a table of the evaluation metrics
svm_metrics_table <- data.frame(
  Metric = c("Accuracy", "Balanced Accuracy", "F1 Score", "AUC (ROC)", "Misclassification Rate"),
  SVM_Linear = c(svm_accuracy, svm_balanced_accuracy, svm_f1_score, svm_auc, svm_misclass)
)

print(svm_metrics_table)
```

##### Radial SVM Model

```{r svm_radial_model}
# ## K-fold CV for hyperparameter tuning of 'cost' for the training data with a radial basis function (RBF) kernel
# start_time_svm_rad <- Sys.time()
# 
# tune.out_rad = tune(svm, label ~ . -label_as_numeric, data = train_data, kernel = "radial", ranges = list(cost = c(0.01, 0.1, 1, 10), gamma = c(0.00003, 0.0003, 0.003)))
# summary(tune.out_rad)
# 
# end_time_svm_rad <- Sys.time()
# svm_training_time_rad <- end_time_svm_rad - start_time_svm_rad
# cat("SVM model training time (radial kernel):", svm_training_time_rad, "\n")

# Note that a good rule of thumb is to start with a gamma that is (1/p)
# where p is the number of features in the model. This would result in a value of
# gamma approximately 0.0003

# # Save the model with the cost that results in the lowest cross validation error rate
# bestmod_rad <-  tune.out_rad$best.model
# summary(bestmod_rad)
```

```{r svm_predictions}
# Predictions
start_time_svm_pred <-  Sys.time()
svm_rad_pca_pred = predict(bestmod_rad, test_data)

end_time_svm_pred <- Sys.time()
svm_prediction_time <- end_time_svm_pred - start_time_svm_pred
cat("SVM prediction time:", svm_prediction_time, "\n")
```

```{r svm_confusion}
svm_confusion_matrix <- table(predict = svm_rad_pca_pred, truth = test_data$label)
print(svm_confusion_matrix)
print(svm_confusion_matrix[2,2])
```

```{r svm_balanced_accuracy}
svm_balanced_accuracy <- fun_calc_balanced_accuracy(svm_confusion_matrix, 
                                                    pos_label = "TREG",
                                                    neg_label = "CD4+T")

cat("The balanced accuracy is", svm_balanced_accuracy, "\n") # 0.9479
```

```{r svm_Misclass}
svm_misclassification_rate <- mean(svm_rad_pca_pred != test_data$label)
cat("SVM Misclassification Rate:", svm_misclassification_rate, "\n") # 0.0447
```

```{r svm_Accuracy}
accuracy_svm <- mean(svm_rad_pca_pred == test_data$label)
cat("SVM Accuracy:", accuracy_svm, "\n") # 0.9553
```

```{r svm_ROC_AUC}
roc_svm <- roc(test_data$label, as.numeric(svm_rad_pca_pred))
cat("SVM AUC:", auc(roc_svm), "\n")
plot(roc_svm, main = "SVM ROC Curve") # Make the limits of the axes 0-1
```

```{r rf_f1_score}
svm_f1_score <- F1_Score(y_pred = svm_rad_pca_pred, y_true = test_data$label)
cat("F1 Score:", svm_f1_score, "\n") # 0.9645
```

#### Summary without PCA

insert data frame with all the metrics of the classifiers
```{r}
# lda_accuracy
# lda_balanced_accuracy
# lda_f1_score
# lda_auc
# lda_misclas
# lda_runtime
# 
# log_accuracy
# log_balanced_accuracy
# log_f1_score
# log_auc
# log_misclas
# logreg_run_time
# 
# # n/a qda_accuracy
# # n/a qda_balanced_accuracy
# # n/a qda_f1_score
# qda_auc
# qda_misclas
# # n/a qda_runtime
# 
# # n/a knn_accuracy
# # n/a knn_balanced_accuracy
# # n/a knn_f1_score
# knn_auc
# knn_mislas
# knn_train_runtime
# 
# gbm_accuracy
# gbm_balanced_accuracy
# gbm_f1_score
# gbm_auc
# gbm_misclas
# gbm_model_training_time
# 
# svm_accuracy
# svm_balanced_accuracy
# svm_f1_score
# svm_auc
# svm_misclas
# svm_training_time


summary_without_PCA <- list(
    Model = c("LDA", "Logistic Regression", "QDA", "KNN", "GBM", "RF", "SVM"),
    accuracy = c(lda_accuracy, log_accuracy, qda_accuracy, knn_accuracy, gbm_accuracy, rf_accuracy, svm_accuracy),
    balanced_accuracy = c(lda_balanced_accuracy, log_balanced_accuracy, qda_balanced_accuracy, knn_balanced_accuracy, gbm_balanced_accuracy, rf_balanced_accuracy, svm_balanced_accuracy),
    f1_score = c(lda_f1_score, log_f1_score, qda_f1_score, knn_f1_score, gbm_f1_score, rf_f1_score, svm_f1_score),
    auc = c(lda_auc, log_auc, qda_auc, knn_auc$AUC[1], gbm_auc, rf_auc, svm_auc),
    misclas = c(lda_misclas, log_misclas, qda_misclas, knn_misclas[1], gbm_misclas, rf_misclas, svm_misclas),
    model_training_time = c(lda_runtime, logreg_run_time, qda_runtime, knn_train_runtime, gbm_model_training_time, rf_training_time, svm_training_time)
)
print(summary_without_PCA)
metrics_df <- data.frame(summary_without_PCA)
metrics_df
```


### With PCA

```{r pc_decomp, eval = FALSE}
# Perform PCA to get first 10 principal components
pca_start <- Sys.time()

task_one_df_pca <- task_one_df %>% 
  
  dplyr::select(-label, - label_as_numeric) %>% 
  
  prcomp(center = TRUE, scale. = TRUE)

pca_end <- Sys.time()

pca_runtime <- pca_end - pca_start # Time difference of 4.244169 mins
```

```{r pca_save, eval = FALSE}
write.csv(task_one_df_pca$x, file = "predictions_1.2/task_one_df_pca.csv", row.names = TRUE)
```

```{r logreg_load, include = FALSE}
task_one_df_pca <- read.csv("predictions_1.2/task_one_df_pca.csv") %>% 
  
  dplyr::select(-X)
```

```{r pca_split}
# Split into training and test data

if(is.data.frame(task_one_df_pca$x)){
  train_data_pca <- as.data.frame(task_one_df_pca$x[train_indices, 1:10])

  test_data_pca <- as.data.frame(task_one_df_pca$x[-train_indices, 1:10])
} else {
  train_data_pca <- as.data.frame(task_one_df_pca[train_indices, 1:10])

  test_data_pca <- as.data.frame(task_one_df_pca[-train_indices, 1:10])
}

train_data_pca$label <- train_data$label

train_data_pca$label_as_numeric <- train_data$label_as_numeric

test_data_pca$label <- test_data$label

test_data_pca$label_as_numeric <- test_data$label_as_numeric
```


#### Linear Discriminant Analysis (LDA) (Imar)

```{r LDA_with_PCA}
lda_pca_model <- lda(label ~ .-label_as_numeric, data = train_data_pca)
lda_pca_pred <- predict(lda_pca_model, test_data_pca)
lda_test_data_labels <- as.factor(test_data_pca$label)
```

```{r lda_pca_confusion}
lda_pca_confusion <- table(prediction = lda_pca_pred$class, truth = as.factor(test_data_pca$label))
lda_pca_confusion
```

```{r lda_pca_accuracy}
lda_pca_accuracy <- mean(lda_pca_pred$class == test_data_pca$label)
cat("Accuracy of LDA with PCA:", lda_pca_accuracy, "\n")
```

```{r lda_pca_balanced_accuracy}
lda_pca_balanced_accuracy <- fun_calc_balanced_accuracy(lda_pca_confusion,
                                                        pos_label = "TREG",
                                                        neg_label = "CD4+T")
cat("Balanced Accuracy of LDA with PCA:", lda_pca_balanced_accuracy, "\n")
```

```{r lda_pca_f1_score}
lda_pca_f1_score <- F1_Score(y_true = test_data_pca$label, 
                             y_pred = lda_pca_pred$class)
cat("F1 Score of LDA with PCA:", lda_pca_f1_score, "\n")
```

```{r lda_pca_roc_auc}
roc_curve_lda_pca <- roc(test_data_pca$label_as_numeric, lda_pca_pred$posterior[,2], levels = c(1, 0), direction = ">")
plot(roc_curve_lda_pca, col="blue", main="ROC Curve for LDA with PCA")
lda_pca_auc <- auc(roc_curve_lda_pca)
cat("AUC of LDA with PCA:", lda_pca_auc, "\n")
```

```{r lda_pca_misclas}
lda_pca_misclas <- mean(lda_pca_pred$class != test_data_pca$label)
cat("Misclassification rate of LDA with PCA:", lda_pca_misclas, "\n")
```

```{r lda_pca_summary}
lda_pca_metrics_table <- data.frame(
  Metric = c("Accuracy", "Balanced Accuracy", "F1 Score", "AUC (ROC)", "Misclassification Rate"),
  Value = c(lda_pca_accuracy, lda_pca_balanced_accuracy, lda_pca_f1_score, lda_pca_auc, lda_pca_misclas)
)
print(lda_pca_metrics_table)
```

#### Logistic classifier (Imar)

```{r log_pca_model}
log_pca_model <- glm(label ~ . -label_as_numeric, data=train_data_pca, family=binomial)
log_pca_probs <- predict(log_pca_model, newdata = test_data_pca, type="response")
log_pca_pred <- ifelse(log_pca_probs > 0.5, "TREG", "CD4+T")
```

```{r log_pca_confusion}
log_pca_confusion <- table(prediction = log_pca_pred, truth = test_data_pca$label)
print(log_pca_confusion)
```

```{r log_pca_accuracy}
log_pca_accuracy <- mean(log_pca_pred == test_data_pca$label)
cat("Accuracy of Logistic Regression with PCA:", log_pca_accuracy, "\n")
```

```{r log_pca_balanced_accuracy}
log_pca_balanced_accuracy <- fun_calc_balanced_accuracy(log_pca_confusion,
                                                        pos_label = "TREG",
                                                        neg_label = "CD4+T")
cat("Balanced Accuracy of Logistic Regression with PCA:", log_pca_balanced_accuracy, "\n")
```

```{r log_pca_f1_score}
log_pca_f1_score <- F1_Score(y_true = test_data_pca$label, y_pred = log_pca_pred)
cat("F1 Score of Logistic Regression with PCA:", log_pca_f1_score, "\n")
```

```{r log_pca_roc_auc}
log_pca_pred_num <- ifelse(log_pca_pred == 'TREG', 1, 0)

roc_curve_log_pca <- roc(test_data_pca$label_as_numeric,
                         log_pca_pred_num, 
                         levels = c(1, 0),
                         direction = ">")
log_pca_auc <- auc(roc_curve_log_pca)
plot(roc_curve_log_pca, col="blue", main="ROC Curve for Logistic Regression with PCA")
cat("AUC of Logistic Regression with PCA:", log_pca_auc, "\n")
```

```{r log_pca_misclas}
log_pca_misclas <- mean(log_pca_pred != test_data_pca$label)
cat("Misclassification rate of Logistic Regression with PCA:", log_pca_misclas, "\n")
```

```{r Log_PCA_evaluation}
# Create a table of the evaluation metrics
log_pca_metrics_table <- data.frame(
  Metric = c("Accuracy", "Balanced Accuracy", "F1 Score", "AUC (ROC)", "Misclassification Rate"),
  Value = c(log_pca_accuracy, log_pca_balanced_accuracy, log_pca_f1_score, log_pca_auc, log_pca_misclas)
)
print(log_pca_metrics_table)
```

#### Quadratic Discriminant Analysis (QDA) (Chi)

```{r qda_pca_train}
qda_start_time <- Sys.time()
qda_pca_fit <- qda(label ~ . - label_as_numeric, 
                   data = train_data_pca)
qda_runtime <- Sys.time() - qda_start_time # Time difference of 0.01315498 secs
```

```{r qda_pca_confusion}
qda_pca_pred <- predict(qda_pca_fit,
                        test_data_pca)$class

qda_pca_confusion <- table(predicted = qda_pca_pred,
                           observed = test_data$label)

print(qda_pca_confusion)
```

```{r qda_pca_diagnostics}
qda_pca_probs <- predict(qda_pca_fit, newdata = test_data_pca, type = "prob")

qda_pca_accuracy <- mean(qda_pca_pred == test_data_pca$label)

qda_pca_balanced_accuracy <- fun_calc_balanced_accuracy(qda_pca_confusion,
                                                        pos_label = 'TREG',
                                                        neg_label = 'CD4+T')

qda_pca_f1_score <- F1_Score(y_true = test_data_pca$label, 
                             y_pred = qda_pca_pred,
                             positive = "TREG")

qda_pca_roc_curve <- roc(
  response = test_data_pca$label,
  predictor = qda_pca_probs$posterior[,'TREG'], 
  levels = c('CD4+T','TREG'),
  direction = "<"
  )

qda_pca_auc <- auc(qda_pca_roc_curve)

qda_pca_misclas <- mean(qda_pca_pred != test_data_pca$label)

plot(qda_pca_roc_curve, col = "red", main = "ROC Curve for QDA")
```

```{r qda_PCA_evaluation}
# Create a table of the evaluation metrics
qda_pca_metrics_table <- data.frame(
  Metric = c("Accuracy", "Balanced Accuracy", "F1 Score", "AUC (ROC)", "Misclassification Rate"),
  QDA = c(qda_pca_accuracy, qda_pca_balanced_accuracy, qda_pca_f1_score, qda_pca_auc, qda_pca_misclas)
)

print(qda_pca_metrics_table)
```

#### Nearest Neighbor Classifier (k-NN) (Chi)

```{r kNN_train, eval = FALSE}
knn_train_X <- train_data_pca %>%
  dplyr::select(-label, -label_as_numeric)

knn_train_Y <- train_data_pca$label

knn_test_X <- test_data_pca %>%
  dplyr::select(-label, -label_as_numeric)

knn_start_time <- Sys.time()
knn_pca_pred <- knn(train = knn_train_X, 
                    test = knn_test_X, 
                    cl = knn_train_Y, 
                    k = 8)

knn_end_time <- Sys.time()

knn_train_runtime <- knn_end_time - knn_start_time # Time difference of 0.3525879 secs

rm(knn_train_X, knn_train_Y, knn_test_X)
```

```{r knn_pca_confusion}
knn_confusion <- table(predicted = knn_pca_pred,
                           observed = test_data$label)

print(knn_pca_confusion)
```

```{r knn_pca_diagnostics}
knn_pca_accuracy <- mean(knn_pca_pred == test_data_pca$label)

knn_pca_balanced_accuracy <- fun_calc_balanced_accuracy(knn_pca_confusion,
                                                        pos_label = 'TREG',
                                                        neg_label = 'CD4+T')

knn_pca_f1_score <- F1_Score(y_true = test_data_pca$label, 
                             y_pred = knn_pca_pred,
                             positive = "TREG")

knn_pca_roc_curve <- roc(
  response = test_data_pca$label,
  predictor = as.numeric(knn_pca_pred), 
  levels = c('CD4+T','TREG'),
  direction = "<"
  )

knn_pca_auc <- auc(knn_pca_roc_curve)

knn_pca_misclas <- mean(knn_pca_pred != test_data_pca$label)

plot(knn_pca_roc_curve, col = "red", main = "ROC Curve for QDA")
```

```{r knn_pca_evaluation}
# Create a table of the evaluation metrics
knn_pca_metrics_table <- data.frame(
  Metric = c("Accuracy", "Balanced Accuracy", "F1 Score", "AUC (ROC)", "Misclassification Rate"),
  kNN = c(knn_pca_accuracy, knn_pca_balanced_accuracy, knn_pca_f1_score, knn_pca_auc, knn_pca_misclas)
)

print(knn_pca_metrics_table)
```

#### Gradient Boosting Decision Trees (GBDT) (Imar)
```{r gbm_pca_training}
lambda <- 0.001
# Measure the time it takes to run 
gbm_start_time <- Sys.time()
# Train the gbm model with PCA data
pca_gbm_model <- gbm(label_as_numeric ~ . - label, 
                     data = train_data_pca, 
                     distribution = "bernoulli", 
                     n.trees = num_trees, 
                     interaction.depth = 4, 
                     shrinkage = lambda, 
                     cv.folds = 5, 
                     verbose = FALSE)

# Get the best number of trees based on cross-validation
best_trees_pca <- gbm.perf(pca_gbm_model, method = "cv")

# Measure time after model training
gbm_end_time <- Sys.time()
gbm_model_training_time <- gbm_end_time - gbm_start_time #Time difference of 8.224947 secs
cat("GBM Model training time with PCA:", gbm_model_training_time, "\n")

```

```{r gbm_pca_predict}
# Measure time for predictions
gbm_start_time_pred <- Sys.time()

# Make predictions on the test set using the best number of trees
gbm_pred_probs_pca <- predict(pca_gbm_model, newdata = test_data_pca, n.trees = best_trees_pca, type = "response")
gbm_pred_classes_pca <- ifelse(gbm_pred_probs_pca > 0.5, 1, 0)

# Measure time after predictions
gbm_end_time_pred <- Sys.time()
gbm_prediction_time <- gbm_end_time_pred - gbm_start_time_pred
cat("Prediction time with PCA:", gbm_prediction_time, "\n")

```

```{r gbm_pca_confusion}
# Confusion Matrix
gbm_confusion_matrix_pca <- table(Predicted = gbm_pred_classes_pca, Actual = test_data_pca$label_as_numeric)
print(gbm_confusion_matrix_pca)
```

```{r gbm_pca_balanced_accuracy}
# Balanced Accuracy
gbm_balanced_accuracy_pca <- fun_calc_balanced_accuracy(gbm_confusion_matrix_pca,
                                                        pos_label = '1',
                                                        neg_label = '0')
cat("The balanced accuracy with PCA is", gbm_balanced_accuracy_pca, "\n")
```

```{r gbm_pca_misclass}
# Misclassification Rate
gbm_misclassification_rate_pca <- mean(gbm_pred_classes_pca != test_data_pca$label_as_numeric)
cat("Misclassification Rate with PCA:", gbm_misclassification_rate_pca, "\n")
```

```{r gbm_pca_accuracy}
# Accuracy
gbm_accuracy_pca <- mean(gbm_pred_classes_pca == test_data_pca$label_as_numeric)
cat("Accuracy with PCA:", gbm_accuracy_pca, "\n")
```

```{r gbm_pca_ROC}
# ROC and AUC
gbm_roc_curve_pca <- roc(test_data_pca$label_as_numeric, gbm_pred_probs_pca)
cat("AUC with PCA:", auc(gbm_roc_curve_pca), "\n")
plot(gbm_roc_curve_pca, main = "ROC Curve with PCA")
```

```{r gbm_pca_F1}
# F1 Score
gbm_f1_score_pca <- F1_Score(y_pred = gbm_pred_classes_pca,
                             y_true = test_data_pca$label_as_numeric)
cat("F1 Score with PCA:", gbm_f1_score_pca, "\n")

```
#### Random Forest (Imar)
```{r rf_pca_train}
# RF model with PCA data
start_time_rf_pca <- Sys.time()

rf_pca_model <- randomForest(label ~ . - label_as_numeric, 
                             data = train_data_pca,
                             mtry = 3, 
                             importance = TRUE, 
                             n.tree = 5000)

end_time_rf_pca <- Sys.time()
rf_pca_training_time <- end_time_rf_pca - start_time_rf_pca #Time difference of 3.99099 secs
cat("Random Forest model training time with PCA:", rf_pca_training_time, "\n")
```

```{r rf_pca_prediction}
# Predictions with PCA data
start_time_rf_pred <- Sys.time()

rf_pca_pred <- predict(rf_pca_model, newdata = test_data_pca)

end_time_rf_pred <- Sys.time()
rf_prediction_time <- end_time_rf_pred - start_time_rf_pred
cat("Random Forest prediction time with PCA:", rf_prediction_time, "\n")

```

```{r rf_pca_Confusion}
rf_confusion_matrix_pca <- table(Predicted = rf_pca_pred, Actual = test_data_pca$label)
print(rf_confusion_matrix_pca)
```

```{r rf_pca_balanced_accuracy}
rf_balanced_accuracy_pca <- fun_calc_balanced_accuracy(rf_confusion_matrix_pca,
                                                       pos_label = 'TREG',
                                                       neg_label = 'CD4+T')

cat("The balanced accuracy with PCA is", rf_balanced_accuracy_pca, "\n")
```

```{r rf_pca_misclass}
rf_misclassification_rate_pca <- mean(rf_pca_pred != test_data_pca$label)
cat("Random Forest Misclassification Rate with PCA:", rf_misclassification_rate_pca, "\n")
```

```{r rf_pca_accuracy}
accuracy_rf_pca <- mean(rf_pca_pred == test_data_pca$label)
cat("Random Forest Accuracy with PCA:", accuracy_rf_pca, "\n")
```

```{r rf_pca_ROC}
roc_rf_pca <- roc(test_data_pca$label_as_numeric,
                  as.numeric(rf_pca_pred))

cat("Random Forest AUC with PCA:", auc(roc_rf_pca), "\n")
plot(roc_rf_pca, main = "Random Forest ROC Curve with PCA")
```

```{r rf_pca_F1}
# F1 Score
rf_f1_score_pca <- F1_Score(y_pred = rf_pca_pred, y_true = test_data_pca$label)
cat("F1 Score with PCA:", rf_f1_score_pca, "\n")
```

#### Support Vector Machine (SVM) (Chi)

##### Linear SVM Model
```{r svm_linear_model_pca}
## K-fold CV for hyperparameter tuning of 'cost' for the training data with a linear kernel
start_time_svm <- Sys.time()

tune.out = tune(svm,
                label ~ . -label_as_numeric,
                data = train_data_pca, kernel = "linear", 
                ranges = list(cost = c(0.01, 0.1, 1, 10)))

summary(tune.out)

end_time_svm <- Sys.time()
svm_training_time <- end_time_svm - start_time_svm # Time difference of 4.748707 secs
```


```{r svm_linear_tuned_model_pca}
# Save the model with the cost that results in the lowest cross validation error rate
bestmod <-  tune.out$best.model
summary(bestmod)
```

```{r svm_predictions_pca}
start_time_svm_pred <-  Sys.time()

svm_pca_pred = predict(bestmod, test_data_pca)

end_time_svm_pred <- Sys.time()
svm_prediction_time <- end_time_svm_pred - start_time_svm_pred 
# Time difference of 0.01050401 secs
```

```{r svm_pca_confusion}
svm_pca_confusion_matrix <- table(predict = svm_pca_pred,
                                  truth = test_data_pca$label)
print(svm_pca_confusion_matrix)
```

```{r svm_pca_diagnostics}
svm_pca_balanced_accuracy <- fun_calc_balanced_accuracy(svm_pca_confusion_matrix,
                                                    pos_label = "TREG",
                                                    neg_label = "CD4+T")

svm_pca_misclass <- mean(svm_pca_pred != test_data$label)

svm_pca_accuracy <- mean(svm_pca_pred == test_data$label)

svm_pca_roc <- roc(test_data$label_as_numeric,
               as.numeric(svm_pca_pred), 
               levels = c(0,1),
               direction = "<")

svm_pca_auc <- auc(svm_roc)

plot(svm_pca_roc, main = "SVM ROC Curve") # Make the limits of the axes 0-1

svm_pca_f1_score <- F1_Score(y_pred = svm_pca_pred, 
                             y_true = test_data$label,
                             positive = "TREG")

```

```{r svm_pca_evaluation}
# Create a table of the evaluation metrics
svm_pca_metrics_table <- data.frame(
  Metric = c("Accuracy", "Balanced Accuracy", "F1 Score", "AUC (ROC)", "Misclassification Rate"),
  SVM_Linear = c(svm_pca_accuracy, svm_pca_balanced_accuracy, svm_pca_f1_score, svm_pca_auc, svm_pca_misclass)
)

print(svm_pca_metrics_table)
```

##### Radial SVM Model
```{r svm_radial_pca_model}
## K-fold CV for hyperparameter tuning of 'cost' for the training data with a radial basis function (RBF) kernel
start_time_svm_rad <- Sys.time()

tune.out_rad = tune(svm,
                    label ~ . -label_as_numeric,
                    data = train_data_pca, 
                    kernel = "radial", 
                    ranges = list(cost = c(0.01, 0.1, 1, 10),
                                  gamma = c(0.00003, 0.0003, 0.003)))
summary(tune.out_rad)

end_time_svm_rad <- Sys.time()
svm_training_time_rad <- end_time_svm_rad - start_time_svm_rad
cat("SVM model training time (radial kernel):", svm_training_time_rad, "\n")

# Note that a good rule of thumb is to start with a gamma that is (1/p)
# where p is the number of features in the model. This would result in a value of
# gamma approximately 0.0003

# Save the model with the cost that results in the lowest cross validation error rate
bestmod_rad <-  tune.out_rad$best.model
summary(bestmod_rad)
```

```{r svm_radial_pca_predictions}
start_time_svm_pred <-  Sys.time()
svm_rad_pca_pred = predict(bestmod_rad, test_data_pca)

end_time_svm_pred <- Sys.time()
svm_prediction_time <- end_time_svm_pred - start_time_svm_pred #Time difference of 0.01661301 secs
```

```{r svm_radial_pca_confusion}
svm_rad_pca_confusion_matrix <- table(predict = svm_rad_pca_pred,
                                      truth = test_data_pca$label)
print(svm_rad_pca_confusion_matrix)
```

```{r svm_radial_pca_balanced_accuracy}
svm_rad_pca_balanced_accuracy <- fun_calc_balanced_accuracy(svm_rad_pca_confusion_matrix, 
                                                    pos_label = "TREG",
                                                    neg_label = "CD4+T")

svm_rad_pca_misclass <- mean(svm_rad_pca_pred != test_data$label)

svm_rad_pca_accuracy <- mean(svm_rad_pca_pred == test_data$label)

svm_rad_pca_roc <- roc(test_data$label, 
                       as.numeric(svm_rad_pca_pred),
                       levels = c("CD4+T", 'TREG'),
                       direction = "<")

svm_rad_pca_auc <- auc(svm_rad_pca_roc)

plot(svm_rad_pca_roc, main = "SVM ROC Curve") # Make the limits of the axes 0-1

svm_rad_pca_f1_score <- F1_Score(y_pred = svm_rad_pca_pred, 
                                 y_true = test_data$label,
                                 positive = 'TREG')

```

```{r svm_rad_pca_evaluation}
# Create a table of the evaluation metrics
svm_rad_pca_metrics_table <- data.frame(
  Metric = c("Accuracy", "Balanced Accuracy", "F1 Score", "AUC (ROC)", "Misclassification Rate"),
  SVM_Radial = c(svm_rad_pca_accuracy, svm_rad_pca_balanced_accuracy, svm_rad_pca_f1_score, svm_rad_pca_auc, svm_rad_pca_misclass)
)

print(svm_rad_pca_metrics_table)
```

#### Summary with PCA

insert data frame with all the metrics of the classifiers
```{r train_pca_summary}
summary_with_PCA <- data.frame(
    Model = c("LDA", "Logistic Regression", "QDA", "KNN", "GBM", "RF", "SVM"),
    
    accuracy = c(lda_pca_accuracy, log_pca_accuracy, qda_pca_accuracy, knn_pca_accuracy, gbm_pca_accuracy, rf_pca_accuracy, svm_pca_accuracy),
    
    balanced_accuracy = c(lda_pca_balanced_accuracy, log_pca_balanced_accuracy, qda_pca_balanced_accuracy, knn_pca_balanced_accuracy, gbm_pca_balanced_accuracy, rf_pca_balanced_accuracy, svm_pca_balanced_accuracy),
    
    f1_score = c(lda_pca_f1_score),
    
    auc = c(lda_pca_auc, log_pca_auc, qda_pca_auc, knn_pca_auc$AUC[1], gbm_pca_auc, rf_pca_auc, svm_pca_auc),
    
    misclas = c(lda_pca_misclas, log_pca_misclas, qda_pca_misclas, knn_pca_misclas[1], gbm_pca_misclas, rf_pca_misclas, svm_pca_misclas),
    
    model_training_time = c(lda_runtime, logreg_run_time, qda_runtime, knn_train_runtime, gbm_model_training_time, rf_training_time, svm_training_time)

    )

display(summary_with_PCA)
```

## T1.3: Improving F1 Score (???)

Train and evaluating three classifiers of our choice with the goal of improving the F1 score (can use methods like bagging, boosting, and regularization (cross-fold validation, ridge/lasso)).

#### Identifying the number of Principle Components to use 
```{r scree_plot}
variance_explained <- (task_one_df_pca$sdev)^2 / sum((task_one_df_pca$sdev)^2)

scree_data <- data.frame(
  Component = c(1:100),
  Variance = variance_explained[1:100]
)

ggplot(scree_data, aes(x = Component, y = Variance)) + 
   
  geom_line() +
  labs(title= 'Scree Plot', 
       x = 'Principal Component',
       y = 'Proportion of Variance Explained') +
  scale_x_continuous(limits = c(0, 100))
```

#### LDA (Chi)

LDA outperforms QDA. Does this imply a linear bayes decision boundary?   

##### Cross-validation (Finding optimal number of Principal Components and tuning threshold)

```{r lda_cv_train}
lda_cv_time <- Sys.time()
lda_cv_scores <- data.frame(no_pc = c(),
                            threshold = c(),
                            balanced_accuracy = c(),
                            f1 = c())

thresholds <- seq(0.1, 0.9, 0.1)

set.seed(seed)

fold_indices <- createFolds(train_data$label, k = 10, returnTrain = TRUE)

for (prin_comps in no_prin_comps) {
  
  lda_train <- task_one_df_pca[train_indices, 1:prin_comps] %>%
    
    data.frame(., label = train_data$label) # Use the first n_components
  
  lda_test_X <- task_one_df_pca[-train_indices, 1:prin_comps]
    
  for (fold in fold_indices){
    lda_cv_model <- lda(label ~ ., 
                        data = lda_train[fold,])
    
    lda_cv_probs <- predict(lda_cv_model, 
                            lda_train[-fold,], 
                            type = "prob")
    
    for (threshold in thresholds){
      lda_cv_preds <- as.factor(ifelse(lda_cv_probs$posterior[, "TREG"] > threshold,
                                       "TREG",
                                       "CD4+T"))
      
      cm <- table(lda_cv_preds, lda_train$label[-fold])
      
      balanced_accuracy <- fun_calc_balanced_accuracy(cm, 
                                                      pos_label = "TREG", 
                                                      neg_label = "CD4+T")
      
      f1 <- F1_Score(y_true = lda_train[-fold,]$label,
                     y_pred = lda_cv_preds,
                     positive = "TREG")
      
      return_df <- data.frame(no_pc = prin_comps,
                              threshold = threshold,
                              balanced_accuracy = balanced_accuracy,
                              f1 = f1)
      
      # Store the scores
      lda_cv_scores <- rbind(lda_cv_scores, return_df)
    }
  }
}
lda_cv_time <- Sys.time() - lda_cv_time # Time difference of 11.68833 secs

lda_avg_scores <- lda_cv_scores %>%
  
  group_by(no_pc, threshold) %>%
  
  summarise(Balanced_Accuracy = mean(balanced_accuracy),
            Balanced_Accuracy_sd = sd(balanced_accuracy),
            F1 = mean(f1), 
            F1_sd = sd(f1),
            .groups = "keep")

lda_improved_models <- lda_cv_scores[order(lda_cv_scores$F1,
                                                 decreasing = TRUE),]

lda_improved_models
```

```{r}
ggplot(lda_avg_scores, aes(x = threshold, y = F1, color = as.factor(no_pc))) +
  geom_line() +
  geom_point()
```

```{r}
lda_improved_models <- lda_avg_scores[order(lda_avg_scores$F1,
                                                 decreasing = TRUE),]


lda_tuned_training <- task_one_df_pca[train_indices, 1:lda_best_no_pc] %>%
  
  data.frame(., label = train_data$label)

test_lda_tuned <- task_one_df_pca[-train_indices, 1:lda_best_no_pc]

data_pca <- data.frame(X_pca, label = train_data_pca$label)

lda_tuned_model <- lda(label ~ ., 
                          lda_tuned_training)

lda_tuned_prob <- predict(lda_tuned_model, newdata = test_lda_tuned)$posterior[, "TREG"]

lda_tuned_pred <- ifelse(lda_tuned_prob > 0.2, "TREG", "CD4+T")
```

```{r lda_tuned_confusion}
lda_tuned_confusion <- table(predicted = lda_tuned_pred,
                           observed = test_data$label)

print(lda_tuned_confusion)
```

```{r lda_tuned_diagnostics}
lda_tuned_probs <- predict(lda_tuned_model, newdata = test_lda_tuned, type = "prob")

lda_tuned_accuracy <- mean(lda_tuned_pred == test_data_pca$label)

lda_tuned_balanced_accuracy <- fun_calc_balanced_accuracy(lda_tuned_confusion,
                                                        pos_label = 'TREG',
                                                        neg_label = 'CD4+T')

lda_tuned_f1_score <- F1_Score(y_true = test_data_pca$label, 
                             y_pred = lda_tuned_pred,
                             positive = "TREG")

lda_tuned_roc_curve <- roc(
  response = test_data_pca$label,
  predictor = lda_tuned_prob, 
  levels = c('CD4+T','TREG'),
  direction = "<"
  )

lda_tuned_auc <- auc(lda_tuned_roc_curve)

lda_tuned_misclas <- mean(lda_tuned_pred != test_data_pca$label)

plot(lda_tuned_roc_curve, col = "red", main = "ROC Curve for QDA")
```

```{r lda_tuned_evaluation}
# Create a table of the evaluation metrics
lda_tuned_metrics_table <- data.frame(
  Metric = c("Accuracy", "Balanced Accuracy", "F1 Score", "AUC (ROC)", "Misclassification Rate"),
  LDA_tuned = c(lda_tuned_accuracy, lda_tuned_balanced_accuracy, lda_tuned_f1_score, lda_tuned_auc, lda_tuned_misclas)
)

print(lda_tuned_metrics_table)
```
##### Boosting an LDA 

Not successful, the results of an LDA are too stable and the base classifier is too strong to have an effective improvement from boosting.

```{r lda_adaboost}
adaboost_lda <- function(train_data, train_labels, n_rounds = 50) {
  n <- nrow(train_data)
  weights <- rep(1/n, n) # Initialize weights equally
  alphas <- numeric(n_rounds)
  classifiers <- list()

  for (t in 1:n_rounds) {
    # Fit LDA model using weighted data
    lda_model <- lda(train_labels ~ ., data = train_data, weights = weights)
    classifiers[[t]] <- lda_model
    
    # Predict and calculate error
    predictions <- predict(lda_model, train_data)$class
    error <- sum(weights * (predictions != train_labels)) / sum(weights)
    
    # If error is high, stop boosting
    if (error > 0.5) break
    
    # Compute alpha (classifier weight)
    alpha <- 0.5 * log((1 - error) / error)
    alphas[t] <- alpha
    
    # Update weights
    weights <- weights * exp(-alpha * (predictions == train_labels) * 2 + 1)
    weights <- weights / sum(weights) # Normalize weights
  }
  
  return(list(models = classifiers, alphas = alphas))
}

lda_best_no_pc <- lda_improved_models[1, 'no_pc']

X_pca <- task_one_df_pca[train_indices, 1:5]

test_lda_improved <- task_one_df_pca[-train_indices, 1:lda_best_no_pc]

data_pca <- data.frame(X_pca, label = train_data_pca$label)

lda_boosted <- adaboost_lda(X_pca, train_data$label)
```

```{r}
lda_boosted_pred <- sapply(c(1:3),
                           function(ii){
                             vote = as.numeric(predict(lda_boosted$models[[ii]], 
                                     newdata = test_lda_improved)$class) -1.5
                             
                             alpha = lda_boosted$alphas[ii]
                             
                             weighted_vote = alpha * vote
                             
                             return(weighted_vote)
                           })
```


#### Logistic Regression (Chi - only using graphs, can be deleted??)

```{r}
logreg_cv_time <- Sys.time()
logreg_cv_scores <- data.frame(no_pc = c(),
                        alpha = c(),
                        lambda = c(),
                        accuracy = c(),
                        f1 = c())

for (prin_comps in no_prin_comps) {
  
  set.seed(seed)
  
  X_pca <- train_data_pca[, 1:prin_comps]  # Use the first n_components
  
  # Combine PCA-transformed data with the target
  data_pca <- data.frame(X_pca, label = task_one_df$label)
  
  # Perform LDA with 5-fold cross-validation
  train_control <- trainControl(method = "cv", 
                                number = no_folds,
                                summaryFunction = cv_f1_score)
  
  logistic_regression_model <- train(label ~ ., 
                                     data = data_pca, 
                                     method = "glmnet", 
                                     trControl = train_control,
                                     tuneGrid = expand.grid(
                                       alpha = c(0, 1),
                                       lambda = c(0.001,
                                                  0.01, 0.02,
                                                  0.05, 0.1)
                                     ))
  
  model_result <- logistic_regression_model$results
    
  return_df <- data.frame(no_pc = c(rep(prin_comps, length(model_result$alpha))),
                       alpha = c(model_result$alpha),
                       lambda = c(model_result$lambda),
                       accuracy = c(model_result$Accuracy),
                       f1 = c(model_result$F1))  
  
  # Store the cross-validation accuracy
  logreg_cv_scores <- rbind(logreg_cv_scores, scores)
}

logreg_cv_time <- Sys.time() - logreg_cv_time # Time difference of 3.394877 mins
```

```{r}
logreg_improved_models <- logreg_cv_scores[order(logreg_cv_scores$f1, 
                                                 decreasing = TRUE),]

logreg_improved_models
```

```{r}
graph_cv <- logreg_cv_scores %>% filter(no_pc == 37)

ggplot(graph_cv, aes(x = alpha, y = lambda, fill = f1, labels = f1)) +

  geom_tile() + 
  
  scale_fill_gradient(low = 'white', high = 'blue', limits = c(0.94,0.95)) + 
  
  geom_text(aes(label = round(f1, 5)), color = 'black', size = 3)
  
# alpha=1 for Lasso, alpha=0 for Ridge, or between 0 and 1 for Elastic Net
```

```{r}
graph_cv <- logreg_cv_scores %>% filter(alpha == 1, f1 > 0)

ggplot(graph_cv, aes(x = lambda, y = f1, colour = as.factor(no_pc))) +

  geom_line() + 
  
  geom_text(aes(label = round(f1, 5)), color = 'black', size = 3)
  
# alpha=1 for Lasso, alpha=0 for Ridge, or between 0 and 1 for Elastic Net
```


#### Logistic Regression with Ridge Regularisation (Imar)

```{r}
logreg_start_time <- Sys.time()

# Prepare the input data
x_train_log_regularized <- as.matrix(train_data[, setdiff(names(train_data), "label")])
y_train_log_regularized <- train_data$label_as_numeric

# Fit a regularized logistic regression model
logreg_fit_regularized <- glmnet(x_train_log_regularized, y_train_log_regularized, family = "binomial", alpha = 0)  
# alpha=1 for Lasso, alpha=0 for Ridge, or between 0 and 1 for Elastic Net

logreg_run_time <- Sys.time() - logreg_start_time 

# Make predictions on the test data
x_test_log_regularized <- as.matrix(test_data[, setdiff(names(test_data), "label")])
logreg_probs_regularized <- predict(logreg_fit_regularized, newx = x_test_log_regularized, type = "response", s = 0.1)  # s = 0.1 is the regularization parameter

logreg_pred_ridge <- ifelse(logreg_probs_regularized > 0.5, 'TREG', 'CD4+T')
```

```{r logreg_ridge_save, include = FALSE}
write.csv(logreg_pred_ridge, file = "predictions_1.3/logreg_ridge.csv", row.names = TRUE)
```

```{r log_ridge_load, include = FALSE}
logreg_pred_ridge <- read.csv("predictions_1.3/logreg_ridge.csv") %>% 
  
  dplyr::select(s1)

logreg_pred_ridge <- logreg_pred_ridge[,'s1']
```

```{r}
log_ridge_confusion <- table(prediction = logreg_pred_ridge, truth = test_data$label)
log_ridge_confusion
```

```{r log_pca_f1_score}
log_ridge_f1_score <- F1_Score(y_true = test_data$label, y_pred = logreg_pred_ridge)
cat("F1 Score of Logistic Regression with Ridge Regularization:", log_ridge_f1_score, "\n")
```

#### Logistic Regression with Ridge Regularisation and Cross Validation (Chi)

```{r logreg_cv_train}
logreg_cv_time <- Sys.time()

logreg_cv_scores <- data.frame(no_pc = c(),
                            threshold = c(),
                            lambda = c(),
                            #balanced_accuracy = c(),
                            f1 = c())

thresholds <- seq(0.1, 0.9, 0.1)

set.seed(seed)

fold_indices <- createFolds(train_data$label, k = 10, returnTrain = TRUE)

for (prin_comps in no_prin_comps) {
  
  logreg_train_X <- task_one_df_pca[train_indices, 1:prin_comps] %>%
    
    as.matrix() # Use the first n_components
    
  for (fold in fold_indices){
    logreg_cv_model <- glmnet(logreg_train_X[fold,],
                              as.factor(train_data[fold, 'label']),
                              family = "binomial",
                              alpha = 0)
    
    for (lambda in lambda_tuning){
      logreg_cv_probs <- predict(logreg_cv_model,
                                 logreg_train_X[-fold,],
                                 type = "response",
                                 s = lambda)
      
      for (threshold in thresholds){
        logreg_cv_preds <- as.factor(ifelse(logreg_cv_probs > threshold,
                                         "TREG",
                                         "CD4+T"))
        
        # cm <- table(logreg_cv_preds, train_data[-fold, "label"])
        # 
        # balanced_accuracy <- fun_calc_balanced_accuracy(cm, 
        #                                                 pos_label = "TREG", 
        #                                                 neg_label = "CD4+T")
        
        f1 <- F1_Score(y_true = train_data[-fold,"label"],
                       y_pred = logreg_cv_preds,
                       positive = "TREG")
        
        return_df <- data.frame(no_pc = prin_comps,
                                threshold = threshold,
                                lambda = lambda,
                                # balanced_accuracy = balanced_accuracy,
                                f1 = f1)
        
        # Store the scores
        logreg_cv_scores <- rbind(logreg_cv_scores, return_df)
      }
    }
  }
}
logreg_cv_time <- Sys.time() - logreg_cv_time # Time difference of 11.68833 secs

logreg_avg_scores <- logreg_cv_scores %>%
  
  group_by(no_pc, threshold, lambda) %>%
  
  summarise(#Balanced_Accuracy = mean(balanced_accuracy),
            #Balanced_Accuracy_sd = sd(balanced_accuracy),
            F1 = mean(f1), 
            F1_sd = sd(f1),
            .groups = "keep")

```
#### Random Forest with AdaBoost (Imar)

```{r}
# Train the AdaBoost model with Random Forest as the base learner
start_time_rf_ab <- Sys.time()

# Train AdaBoost using Random Forest as the base model
rf_ab_model <- boosting(
  label ~ . - label_as_numeric,       # Formula excluding label_as_numeric
  data = train_data,                  # Training data
  boos = TRUE,                        # Enable boosting
  mfinal = 50,                        # Number of boosting iterations (trees)
  baselearner = randomForest          # Use Random Forest as the base model
)

end_time_rf_ab <- Sys.time()
rf_ab_training_time <- end_time_rf_ab - start_time_rf_ab
cat("AdaBoost with Random Forest training time:", rf_ab_training_time, "\n")

# Predictions
start_time_rf_ab_pred <- Sys.time()
yhat.rf_ab <- predict(rf_ab_model, newdata = test_data)$class  # Predictions using the AdaBoost model
end_time_rf_ab_pred <- Sys.time()
rf_ab_prediction_time <- end_time_rf_ab_pred - start_time_rf_ab_pred
cat("AdaBoost with Random Forest prediction time:", rf_ab_prediction_time, "\n")
```

```{r rf_ab_save, include = FALSE}
write.csv(yhat.rf_ab, file = "predictions_1.3/rf_ab.csv", row.names = TRUE)
```

```{r logreg_load, include = FALSE}
yhat.rf_ab <- read.csv("predictions_1.3/rf_ab.csv") %>% 
  
  dplyr::select(x)

yhat.rf_ab <- yhat.rf_ab[,'x']
```

```{r}
# Evaluate F1 Score and other metrics
cm_rf_ab <- table(prediction = yhat.rf_ab, truth = test_data$label)
cm_rf_ab
```


```{r}
rf_cv_f1_score <- F1_Score(y_true = test_data$label, y_pred = yhat.rf_ab)
cat("F1 Score of Random Forest with AdaBoost:", rf_cv_f1_score, "\n")
```

## T1.4: Implement predictor (???)

Choose the best approach from those above tested and implement our predictor as a function of our code.

```{r}

```
