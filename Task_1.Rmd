---
title: "Task 1 RmD"
output:
  pdf_document: default
  html_document: default
date: "2024-11-12"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn=-1)
```

```{r parameters}
seed = 443
csv_file_one = "data1.csv"
training_size = 0.8 # Proportion of dataset that is to be split for training
packages = list("class", 
                "randomForest",
                "tree",
                "dplyr",
                "tidyr",
                "gbm", 
                "MASS",
                "ggplot2",
                "pROC",
                "MLmetrics",
                "caret",
                "e1071",
                "glmnet"
)

# For knn-training
kk_vector = c(1:10) 

# GBM Parameters
num_trees = 1000

## Define grid of lambda (shrinkage) values to evaluate
lambda_grid <- c(0.001, 0.01,0.03,0.05)

## Initialize a vector to store test errors for each lambda
test_errors <- numeric(length(lambda_grid))

## Number of Principal Components to train with
no_prin_comps <- c(2:50)

```

```{r, include=FALSE}
for (package in packages){
  if(!require(package, character.only = TRUE)){
    install.packages(package, character.only = TRUE)
  } else {
    library(package, character.only = TRUE)
  }
}
```

# Task 1: binary classification

```{r train_test_split}
set.seed(seed)

task_one_df <- read.csv(csv_file_one)

# str(task_one_df)
task_one_df$label <- as.factor(task_one_df$label)
task_one_df$label_as_numeric <- as.numeric(task_one_df$label) -1 #Labels cells that are T-Reg cells as 1 and CD4 cells as 0
# Split Data into Test and Train (We should all use the same data)

train_split = as.integer(nrow(task_one_df)*training_size)

train_indices <- sample(1:nrow(task_one_df), train_split)

train_data <- task_one_df[train_indices,]
test_data <- task_one_df[-train_indices,]
```

```{r functions}
fun_calc_balanced_accuracy <- function(confusion_matrix, pos_label, neg_label){
  tp <- confusion_matrix[pos_label, pos_label]
  tn <- confusion_matrix[neg_label, neg_label]
  fp <- confusion_matrix[pos_label, neg_label]
  fn <- confusion_matrix[neg_label, pos_label]
  
  balanced_accuracy <- 0.5 * (tp / (tp + fn) + tn / (tn + fp)) 
  
  return(balanced_accuracy)
}

```


## T1.1: exploratory data analysis and summary statistics (Peter?)

```{r EDA}
dim(task_one_df)  # The data has 5471 observations and 4125 variables-- 4124 originally, 1 added (label)
table(task_one_df$label) # 3356 observations are labelled as CD4+T and 2115 are labelled as TREG

mean_table <- task_one_df %>%

  dplyr::select(-label_as_numeric) %>%

  group_by(label) %>%

  summarise(across(everything(), mean, na.rm = TRUE)) %>%

  pivot_longer(cols = -label, names_to = 'genes', values_to = 'mean_value') %>%

  pivot_wider(names_from = label, values_from = mean_value) %>%

  mutate(diff = abs(`CD4+T` - TREG))

  # mutate(pct_diff = 100 * (diff / ((`CD4+T` + TREG)/2) )) ## This doesn't result in very nice graphs, perhaps stick to the absolute diff??


top_3_abs_diff <- mean_table %>% top_n(3, diff) %>% arrange(desc(diff))

# top_3_pct_diff <- mean_table %>% top_n(3, pct_diff) %>% arrange(desc(pct_diff)) # Doesn't result in very good graphs... perhaps remove

par(mfrow=c(1,3))

### Add x-axis label and title
for (var in top_3_abs_diff$genes) {
  formula <- as.formula(paste(var, "~ label"))
  plot <- boxplot(formula, data = task_one_df, col=c("blue", "red"))
  print(plot)
}


# # Doesn't result in very good graphs.. perhaps remove
# for (var in top_3_pct_diff$genes) {
#   formula <- as.formula(paste(var, "~ label"))
#   plot <- boxplot(formula, data = task_one_df, col=c("blue", "red"))
#   print(plot)
#   quantile(task_one_df$var)
# }

## There are decent differences between the ranking of absolute difference in means of groups
## and the ranking of percentage difference in means of groups
## perhaps we create box plots of the top 3-5 of each metric??

## Perhaps do summary(XXX) of a few variables??
## If want to display multiple subgraphs --> par(mfrow=c(x,y))

### Next steps --> 
  ### (1) instead of absolute value difference, include also
  ### percentage change. then plot box plots for the top 5? 10? variables
  ### that appear to have the clearest distinction between the two labels
  ### these specific variables provide a good baseline for what we expect
  ### our subsequent models to choose for the most important fields
  
## Checking if there are any rows that have missing values... Keep this hashed unless you want to check, because it takes a while to run
# for (i in 1:nrow(task_one_df)) {
#   if (any(is.na(task_one_df[i, ]))) {
#     print(paste("Row", i, "contains NA values."))
#   }
# }

# twenty_random_columns <- sample(names(task_one_df)[-1], 20)
# 
# ## Create boxplots for the random sample of columns
# for (column in twenty_random_columns) {
#     p <- ggplot(task_one_df, aes_string(x = "label", y = column)) +
#         geom_jitter(width = 0.2, height = 0) +  # Use jitter to avoid overplotting
#         ggtitle(paste("Scatter Plot of", column, "by Label")) +
#         theme_minimal()
#     print(p)
# }

```

## T1.2: training and evaluating on various classifiers

### Without PCA

#### Linear Discriminant Analysis (LDA) (Imar)

```{r lda_train}
lda_start_time  <- Sys.time()
lda_fit <- lda(label ~ . -label_as_numeric, data=train_data)
lda_end_time  <- Sys.time()
lda_runtime <- lda_end_time - lda_start_time # Time difference of 4.937018 mins

lda_pred <- predict(lda_fit, test_data)$class
```

```{r lda_save, include = FALSE}
write.csv(lda_pred, file = "predictions_1.2/lda_predictions.csv", row.names = TRUE)
```


```{r lda_load, include = FALSE}
lda_pred <- read.csv("predictions_1.2/lda_predictions.csv") %>% 
  
  dplyr::select(x)

lda_pred <- lda_pred[,'x']
```

```{r lda_confusion}
lda_confusion <- table(predicted_label = lda_pred, true_label = test_data$label)
lda_confusion
```

```{r lda_accuracy}
lda_accuracy <- mean(lda_pred == test_data$label)

cat("Accuracy of LDA:", lda_accuracy, "\n")
```

```{r lda_balanced_accuracy}
lda_balanced_accuracy <- fun_calc_balanced_accuracy(lda_confusion, 
                                                    pos_label = 'TREG', 
                                                    neg_label = 'CD4+T')

cat("Balanced Accuracy of LDA:", lda_balanced_accuracy, "\n")
```

```{r lda_f1_score}
lda_f1_score <- F1_Score(y_true= test_data$label, y_pred = lda_pred)
cat("F1 Score of LDA:", lda_f1_score, "\n")
```

```{r lda_roc}
lda_pred_numeric <- ifelse(lda_pred == "TREG", 1, 0)
  
lda_roc_curve <- roc(
  response = test_data$label_as_numeric,
  predictor = lda_pred_numeric, 
  levels = c(0,1),
  direction = "<"
  )

plot(lda_roc_curve, col = "red", main = "ROC Curve for LDA")

lda_auc <- auc(lda_roc_curve)
lda_auc
```

```{r lda_misclass}
lda_misclas <- mean(lda_pred != test_data$label)
cat("Misclassification error rate of LDA:", lda_misclas, "\n")
```

#### Logistic classifier (Imar)

```{r logreg_train}
logreg_start_time <- Sys.time()

logreg_fit <- glm(label_as_numeric ~ . -label,
                  data = train_data,
                  family = binomial)

logreg_run_time <- Sys.time() - logreg_start_time # Time difference of 17.64516 mins

logreg_probs <- predict(logreg_fit, newdata = test_data, type = "response")
logreg_pred <- ifelse(logreg_probs > 0.5, 'TREG', 'CD4+T')
```

```{r logreg_save, include = FALSE}
write.csv(logreg_pred, file = "predictions_1.2/logreg_predictions.csv", row.names = TRUE)
```


```{r logreg_load, include = FALSE}
logreg_pred <- read.csv("predictions_1.2/logreg_predictions.csv") %>% 
  
  dplyr::select(x)

logreg_pred <- logreg_pred[,'x']
```

```{r logreg_confusion}
log_confusion <- table(prediction = logreg_pred, truth = test_data$label)
log_confusion
```

```{r logreg_accuracy}
log_accuracy <- mean(logreg_pred == test_data$label)
cat("Accuracy of Logistic Regression:", log_accuracy, "\n")
```

```{r logreg_balanced_accuracy}
log_balanced_accuracy <- fun_calc_balanced_accuracy(log_confusion,
                                                    pos_label = 'TREG',
                                                    neg_label = 'CD4+T')

cat("Balanced Accuracy of Logistic Regression:", log_balanced_accuracy, "\n")
```

```{r logreg_f1_score}
log_f1_score <- F1_Score(y_true= test_data$label, y_pred = logreg_pred)
cat("F1 Score for Logistic Regression:", log_f1_score, "\n")
```

```{r logreg_ROC_AUC}
roc_curve_log <- roc(test_data$label, logreg_probs, levels=c("TREG", "CD4+T"), direction = ">")

plot(roc_curve_log, col="blue", main="ROC Curve for Logistic Regression")

log_auc <- as.numeric(auc(roc_curve_log))
cat("The AUC of the ROC of Logistic Regression:", log_auc, "\n")
```

```{r logreg_Misclass}
log_misclas <- mean(glm_pred != test_data_log$label)
cat("The Misclassification rate of Logistic Regression:", log_misclas, "\n")
```

#### Quadratic Discriminant Analysis (QDA) (Chi)

Estimation of a QDA classifier fails when training on this training data.

Due to the high-dimensionality of the data relative to number of data points, the covariance matrices for each class k are likely to be estimated as singular or near-singular. This leads to numerical instability and no solution can be found for the inverse of the covariance matrix. 

As a result no classifier can be found without dimension reduction or regularisation.
```{r qda_Train}
## CAUTION: currently hard-coded numbers here. To be changed (Chi)
options(expressions = 10000)
qda_covariates = mean_table$genes[1:1100]
## CAUTION: currently hard-coded numbers here. To be changed (Chi)

qda_formula = as.formula(
  paste(
    "label ~",
    paste(qda_covariates, collapse = " + ")
    ))

qda_fit <- qda(qda_formula, data = train_data)
```

```{r qda_Confusion}
qda_pred <- predict(qda_fit, test_data)$class

table(predicted = qda_pred, observed = test_data$label)
```

```{r qda_Misclass}
qda_misclas = mean(qda_pred != test_data$label)

cat("Misclassification rate:", qda_misclas*100, "%")
```

```{r qda_ROC}
qda_pred_numeric <- ifelse(qda_pred == "TREG", 1, 0)
  
qda_roc_curve <- roc(
  response = test_data$label_as_numeric,
  predictor = qda_pred_numeric, 
  levels = c(0,1),
  direction = "<"
  )

plot(qda_roc_curve, col = "red", main = "ROC Curve for QDA")

qda_auc <- auc(qda_roc_curve)
qda_auc
```

```{r QDA_TBD}
qda_accuracy <- "n/a"
qda_balanced_accuracy <- "n/a"
qda_f1_score <- "n/a"
qda_runtime <- "n/a"
```


#### Nearest Neighbor Classifier (k-NN) (Chi)

```{r kNN_train, eval = FALSE}
knn_train_X <- train_data %>% 
  dplyr::select(-label, -label_as_numeric)

knn_train_Y <- train_data$label

knn_test_X <- test_data %>% 
  dplyr::select(-label, -label_as_numeric)

knn_start_time <- Sys.time()
knn_pred <- sapply(kk_vector,
                   function(k)
                     {knn(train = knn_train_X, test = knn_test_X, cl = knn_train_Y, k = k)}
                   )
knn_end_time <- Sys.time()

knn_train_runtime <- knn_end_time - knn_start_time
print(knn_train_runtime) #50 mins for k=1 to k=10 

rm(knn_train_X, knn_train_Y, knn_test_X)
```

```{r kNN_save, include = FALSE}
write.csv(knn_pred, file = "knn_predictions.csv", row.names = TRUE)
```


```{r kNN_load, include = FALSE}
knn_pred <- read.csv("knn_predictions.csv") %>% 
  dplyr::select(-X)

## CAUTION: currently hard-coded numbers here. To be changed (Chi)
colnames(knn_pred) <- paste0("k=", 1:10)
## CAUTION: currently hard-coded numbers here. To be changed (Chi)
```

```{r kNN_Confusion}
for (i in kk_vector){
  print(paste("k =", i))
  print(table(predicted = knn_pred[,i], 
              observed = test_data$label))
}
```

```{r kNN_Misclass}
knn_misclas = c(rep(0,length(kk_vector)))

for (i in kk_vector){
  knn_misclas[i] = mean(knn_pred[,i] != test_data$label)
}
plot(knn_misclas, type = "l")
```

```{r kNN_ROC}
knn_pred_numeric <- ifelse(knn_pred == "TREG", 1, 0)

knn_roc_curves <- data.frame()

knn_auc <- data.frame()

for (kk in 1:ncol(knn_pred)) {
  roc_curve <- roc(
    response = test_data$label_as_numeric, 
    predictor = knn_pred_numeric[,kk],
    levels = c(0,1),
    direction = "<"
    )
  
  roc_curve_data <- data.frame(
    k = kk,
    FPR = 1 - roc_curve$specificities,
    TPR = roc_curve$sensitivities
    )
  
  auc_data <- data.frame(
    k = kk,
    AUC = roc_curve$auc
    )
  
  knn_roc_curves <- rbind(knn_roc_curves, roc_curve_data)
  
  knn_auc <- rbind(knn_auc, auc_data)
}

knn_roc_curves$k <- as.factor(knn_roc_curves$k)

ggplot(knn_roc_curves, aes(x = FPR, y = TPR, colour = k)) + 
  geom_line(linewidth = 1) +
  labs(
    title = "ROC Curves for k-NN (k = 1 to 10)",
    x = "False Positive Rate (FPR)",
    y = "True Positive Rate (TPR)",
    color = "k"
  )

ggplot(knn_auc, aes(x = k, y = AUC)) + 
  geom_line(linewidth = 1) +
  labs(
    title = "ROC AUCs for k-NN (k = 1 to 10)",
    x = "k",
    y = "ROC AUC"
  )
```

```{r knn_TBD}
knn_accuracy <- "n/a"
knn_balanced_accuracy <- "n/a"
knn_f1_score <- "n/a"
```


#### Gradient Boosting Decision Trees (GBDT) (Jonathan)

```{r gbmparams}
#Number of trees
num_trees = 1000
# Define grid of lambda (shrinkage) values to evaluate
lambda_grid <- c(0.001, 0.01,0.03,0.05)

# Initialize a vector to store test errors for each lambda
test_errors <- numeric(length(lambda_grid))
```

```{r dataprepgbm}
test_data_cleaned_gbm <- test_data[, !colnames(test_data) %in% c("label", "label")]
```

```{r gbmloop, eval=FALSE}
gbm_optimal_lambda_start <- Sys.time()
# Loop over each lambda value
for (i in seq_along(lambda_grid)) {
  lambda <- lambda_grid[i]
  
  # Train the gbm model with the current lambda (shrinkage) value
  gbm_model <- gbm(label_as_numeric ~. - label, data = train_data, 
                   distribution = "bernoulli", 
                   n.trees = num_trees, 
                   interaction.depth = 4, 
                   shrinkage = lambda, 
                   cv.folds = 5, 
                   verbose = FALSE)
  
  # Make predictions on the test set using the optimal number of trees
  predictions <- predict(gbm_model, newdata = test_data_cleaned_gbm, n.trees = num_trees)
  
  # Calculate the Mean Squared Error on the test set
  test_errors[i] <- mean((predictions - test_data_cleaned_gbm$label_as_numeric)^2)
}
gbm_optimal_lambda_end <- Sys.time()
gbm_optimal_lambda_total_time <- gbm_optimal_lambda_end - gbm_optimal_lambda_start
print(gbm_optimal_lambda_total_time)
```

```{r gbmtse, eval=FALSE}
# Combine lambda values and test errors into a data frame for plotting
error_df <- data.frame(lambda = lambda_grid, test_error = test_errors)

# Plot the test error over the range of lambda values
ggplot(error_df, aes(x = lambda, y = test_error)) +
  geom_line() +
  geom_point() +
  labs(title = "Test Error vs Lambda (Shrinkage)", 
       x = "Lambda (Shrinkage)", 
       y = "Test Mean Squared Error") +
  theme_minimal()

```

The results above show use that the GLM model with the lowest Test Mean Squared Error, will be the one with the lowest lambda value we assigned of 0.001. This does not necessarily mean that the best Gradient Descent Boosting model that will work best will be the best model for our data, as it may not account for over fitting. However, we will continue using the GBDT model with a shrinkage value of 0.001 to analyze the model and to compare it to our other models.

```{r gbm_model}
# Set a single shrinkage value
lambda <- 0.001

# Measure the time it takes to run 
gbm_start_time <- Sys.time()
# Train the gbm model
smallest_lambda_gbm_model <- gbm(label_as_numeric ~ . - label, 
                 data = train_data, 
                 distribution = "bernoulli", 
                 n.trees = num_trees, 
                 interaction.depth = 4, 
                 shrinkage = lambda, 
                 cv.folds = 5, 
                 verbose = FALSE)

# Get the best number of trees based on cross-validation
best_trees <- gbm.perf(smallest_lambda_gbm_model, method = "cv")

# Measure time after model training
gbm_end_time <- Sys.time()
gbm_model_training_time <- gbm_end_time - gbm_start_time
cat("GBM Model training time:", gbm_model_training_time, "\n")

# Measure time for predictions
gbm_start_time_pred <- Sys.time()
# Make predictions on the test set using the best number of trees
gbm_pred_probs <- predict(smallest_lambda_gbm_model, newdata = test_data_cleaned_gbm, n.trees = best_trees, type = "response")
gbm_pred_classes <- ifelse(gbm_pred_probs > 0.5, 1, 0)

# Measure time after predictions
gbm_end_time_pred <- Sys.time()
gbm_prediction_time <- gbm_end_time_pred - gbm_start_time_pred
cat("Prediction time:", gbm_prediction_time, "\n")
```

```{r gbm_save, include = FALSE}
write.csv(gbm_pred_classes, 
          file = "predictions_1.2/gbm_predictions.csv",
          row.names = TRUE)
```


```{r gbm_load, include = FALSE}
gbm_pred_classes <- read.csv("predictions_1.2/gbm_predictions.csv") %>% 
  
  dplyr::select(x)

gbm_pred_classes <- gbm_pred_classes[,'x']
```

```{r gbm_Confusion}
gbm_confusion_matrix <- table(Predicted = gbm_pred_classes, Actual = test_data_cleaned_gbm$label_as_numeric)
print(gbm_confusion_matrix)

gbm_balanced_accuracy <- fun_calc_balanced_accuracy(gbm_confusion_matrix, 
                                  pos_label = "1", 
                                  neg_label = "0")

cat("The balanced accuracy is", gbm_balanced_accuracy, "\n")
```

```{r gbm_Misclass}
gbm_misclas <- mean(gbm_pred_classes != test_data_cleaned_gbm$label_as_numeric)
cat("Misclassification Rate:", gbm_misclas, "\n")
```

```{r gbm_Accuracy}
gbm_accuracy <- mean(gbm_pred_classes == test_data_cleaned_gbm$label_as_numeric)
cat("Accuracy:", gbm_accuracy, "\n")
```

```{r gbm_ROC_AUC}
gbm_roc_curve <- roc(test_data_cleaned_gbm$label_as_numeric, gbm_pred_probs)
gbm_auc <- auc(gbm_roc_curve)
cat("AUC:", gbm_auc, "\n")
plot(gbm_roc_curve, main = "ROC Curve")
```

```{r gbm_f1_score}
gbm_f1_score <- F1_Score(y_pred = gbm_pred_classes, y_true = test_data_cleaned_gbm$label_as_numeric)
cat("F1 Score:", gbm_f1_score, "\n")
```

The accuracy for the gradient descent decision tree model (where the shrinkage term was equal to 0.001) was 0.904. The AUC was equal to 0.972 and the Test mean squared error was equal to 0.109. Misclassification rate was equal to 0.096. And F1 score was equal to 0.928

#### Random Forest (Jonathan)

Above we use a tree search to find.... When using the tree search, we get that there are 11 features that are the most important to classifying whether a cell is a TREG cell or a CD4+T cell. However, we can see based on the tree that there are certain features that appear more than once. Based on the tree plot, we can determine that the most important regressor that helps determine if a cell is a TREG cell or a CD4+T cell is the IL7R. From the National Library of medicine ("<https://www.ncbi.nlm.nih.gov/gene/3575>"), IL7R is a protein encoded by a gene that plays a critical role in the development of lymph nodes. Based on the analysis above, we decided to create a Random Forest regression that will have 12 terminal nodes.

```{r rfmodel}
# RF model
start_time_rf <- Sys.time()

rf.rna_features <-randomForest(label~. - label_as_numeric, data=train_data, mtry=64, importance=TRUE, n.tree = 5000) # RF Model

end_time_rf <- Sys.time()
rf_training_time <- end_time_rf - start_time_rf
cat("Random Forest model training time:", rf_training_time, "\n")

# Predictions
start_time_rf_pred <- Sys.time()
yhat.rf <-predict(rf.rna_features, newdata=test_data)

end_time_rf_pred <- Sys.time()
rf_prediction_time <- end_time_rf_pred - start_time_rf_pred
cat("Random Forest prediction time:", rf_prediction_time, "\n")

```

```{r rf_save, include = FALSE}
write.csv(yhat.rf, 
          file = "predictions_1.2/rf_predictions.csv",
          row.names = TRUE)
```


```{r rf_load, include = FALSE}
yhat.rf <- read.csv("predictions_1.2/rf_predictions.csv") %>% 
  
  dplyr::select(x)

yhat.rf <- yhat.rf[,'x']
```

```{r rf_confusion}
rf_confusion_matrix <- confusion_matrix <- table(Predicted = yhat.rf, Actual = test_data$label)
print(rf_confusion_matrix)
rf_balanced_accuracy <- fun_calc_balanced_accuracy(rf_confusion_matrix,
                                                   pos_label = "TREG",
                                                   neg_label = "CD4+T")

cat("The balanced accuracy is", rf_balanced_accuracy, "\n")
```

```{r rf_misclass}
rf_misclas <- mean(yhat.rf != test_data$label)
cat("Random Forest Misclassification Rate:", rf_misclas, "\n")
```

```{r rf_accuracy}
rf_accuracy <- mean(yhat.rf == test_data$label)
cat("Random Forest Accuracy:", rf_accuracy, "\n")
```

```{r rf_roc_auc}
roc_rf <- roc(test_data$label, as.numeric(yhat.rf))
rf_auc <- auc(roc_rf)
cat("Random Forest AUC:", rf_auc, "\n")
plot(roc_rf, main = "Random Forest ROC Curve")
```

```{r rf_f1_score}
rf_f1_score <- F1_Score(y_pred = yhat.rf, y_true = test_data$label)
cat("F1 Score:", rf_f1_score, "\n")
```

Using a random forest tree model, we see that the accuracy of the model was 0.936 Which is quite accurate! Misclassification Rate was equal to 0.064. The F1 score was equal to 0.9504

#### Support Vector Machine (SVM) (Peter)

##### Linear SVM Model
```{r svm_linear_model}
## Clean the test data for the SVM task
test_data_cleaned_for_svm <- test_data[, !colnames(test_data) %in% c("label_as_numeric", "label")]

## K-fold CV for hyperparameter tuning of 'cost' for the training data with a linear kernel
start_time_svm <- Sys.time()

tune.out = tune(svm, label ~ . -label_as_numeric, data = train_data, kernel = "linear", ranges = list(cost = c(0.01, 0.1, 1, 10)))
summary(tune.out)

end_time_svm <- Sys.time()
svm_training_time <- end_time_svm - start_time_svm
cat("SVM model training time:", svm_training_time, "\n") # 1.18 hrs (~ 1hr & 10 min)
```

```{r svm_linear_tuned_model}
# Save the model with the cost that results in the lowest cross validation error rate
bestmod <-  tune.out$best.model
summary(bestmod)
```

```{r svm_predictions}
# Predictions
start_time_svm_pred <-  Sys.time()
yhat.svm = predict(bestmod, test_data)

end_time_svm_pred <- Sys.time()
svm_prediction_time <- end_time_svm_pred - start_time_svm_pred
cat("SVM prediction time:", svm_prediction_time, "\n")
```

```{r svm_save, include = FALSE}
write.csv(yhat.svm, 
          file = "predictions_1.2/svm_predictions.csv",
          row.names = TRUE)
```


```{r svm_load, include = FALSE}
yhat.svm <- read.csv("predictions_1.2/svm_predictions.csv") %>% 
  
  dplyr::select(x)

yhat.svm <- yhat.svm[,'x']
```

```{r svm_Confusion}
svm_confusion_matrix <- table(predict = yhat.svm, truth = test_data$label)
print(svm_confusion_matrix)
print(svm_confusion_matrix[2,2])
svm_balanced_accuracy <- fun_calc_balanced_accuracy(svm_confusion_matrix,
                                                    pos_label = "TREG",
                                                    neg_label = "CD4+T")

cat("The balanced accuracy is", svm_balanced_accuracy, "\n") # 0.9479
```

```{r svm_Misclass}
svm_misclass <- mean(yhat.svm != test_data$label)
cat("SVM Misclassification Rate:", svm_misclass, "\n") # 0.0447
```

```{r svm_Accuracy}
svm_accuracy <- mean(yhat.svm == test_data$label)
cat("SVM Accuracy:", svm_accuracy, "\n") # 0.9553
```

```{r svm_ROC_AUC}
svm_roc <- roc(test_data$label, as.numeric(yhat.svm))
svm_auc <- auc(svm_roc)
cat("SVM AUC:", svm_auc, "\n")
plot(svm_roc, main = "SVM ROC Curve") # Make the limits of the axes 0-1
```

```{r svm_f1_score}
svm_f1_score <- F1_Score(y_pred = yhat.svm, y_true = test_data$label)
cat("F1 Score:", svm_f1_score, "\n") # 0.9645
```

##### Radial SVM Model

```{r svm_radial_model}
# ## K-fold CV for hyperparameter tuning of 'cost' for the training data with a radial basis function (RBF) kernel
# start_time_svm_rad <- Sys.time()
# 
# tune.out_rad = tune(svm, label ~ . -label_as_numeric, data = train_data, kernel = "radial", ranges = list(cost = c(0.01, 0.1, 1, 10), gamma = c(0.00003, 0.0003, 0.003)))
# summary(tune.out_rad)
# 
# end_time_svm_rad <- Sys.time()
# svm_training_time_rad <- end_time_svm_rad - start_time_svm_rad
# cat("SVM model training time (radial kernel):", svm_training_time_rad, "\n")

# Note that a good rule of thumb is to start with a gamma that is (1/p)
# where p is the number of features in the model. This would result in a value of
# gamma approximately 0.0003

# # Save the model with the cost that results in the lowest cross validation error rate
# bestmod_rad <-  tune.out_rad$best.model
# summary(bestmod_rad)
```

```{r svm_predictions}
# Predictions
start_time_svm_pred <-  Sys.time()
yhat.svm_rad = predict(bestmod_rad, test_data)

end_time_svm_pred <- Sys.time()
svm_prediction_time <- end_time_svm_pred - start_time_svm_pred
cat("SVM prediction time:", svm_prediction_time, "\n")
```

```{r svm_confusion}
svm_confusion_matrix <- table(predict = yhat.svm_rad, truth = test_data$label)
print(svm_confusion_matrix)
print(svm_confusion_matrix[2,2])
```

```{r svm_balanced_accuracy}
svm_balanced_accuracy <- fun_calc_balanced_accuracy(svm_confusion_matrix, 
                                                    pos_label = "TREG",
                                                    neg_label = "CD4+T")

cat("The balanced accuracy is", svm_balanced_accuracy, "\n") # 0.9479
```

```{r svm_Misclass}
svm_misclassification_rate <- mean(yhat.svm_rad != test_data$label)
cat("SVM Misclassification Rate:", svm_misclassification_rate, "\n") # 0.0447
```

```{r svm_Accuracy}
accuracy_svm <- mean(yhat.svm_rad == test_data$label)
cat("SVM Accuracy:", accuracy_svm, "\n") # 0.9553
```

```{r svm_ROC_AUC}
roc_svm <- roc(test_data$label, as.numeric(yhat.svm_rad))
cat("SVM AUC:", auc(roc_svm), "\n")
plot(roc_svm, main = "SVM ROC Curve") # Make the limits of the axes 0-1
```

```{r rf_f1_score}
svm_f1_score <- F1_Score(y_pred = yhat.svm_rad, y_true = test_data$label)
cat("F1 Score:", svm_f1_score, "\n") # 0.9645
```

#### Summary without PCA

insert data frame with all the metrics of the classifiers
```{r}
# lda_accuracy
# lda_balanced_accuracy
# lda_f1_score
# lda_auc
# lda_misclas
# lda_runtime
# 
# log_accuracy
# log_balanced_accuracy
# log_f1_score
# log_auc
# log_misclas
# logreg_run_time
# 
# # n/a qda_accuracy
# # n/a qda_balanced_accuracy
# # n/a qda_f1_score
# qda_auc
# qda_misclas
# # n/a qda_runtime
# 
# # n/a knn_accuracy
# # n/a knn_balanced_accuracy
# # n/a knn_f1_score
# knn_auc
# knn_mislas
# knn_train_runtime
# 
# gbm_accuracy
# gbm_balanced_accuracy
# gbm_f1_score
# gbm_auc
# gbm_misclas
# gbm_model_training_time
# 
# svm_accuracy
# svm_balanced_accuracy
# svm_f1_score
# svm_auc
# svm_misclas
# svm_training_time


summary_without_PCA <- list(
    Model = c("LDA", "Logistic Regression", "QDA", "KNN", "GBM", "RF", "SVM"),
    accuracy = c(lda_accuracy, log_accuracy, qda_accuracy, knn_accuracy, gbm_accuracy, rf_accuracy, svm_accuracy),
    balanced_accuracy = c(lda_balanced_accuracy, log_balanced_accuracy, qda_balanced_accuracy, knn_balanced_accuracy, gbm_balanced_accuracy, rf_balanced_accuracy, svm_balanced_accuracy),
    f1_score = c(lda_f1_score, log_f1_score, qda_f1_score, knn_f1_score, gbm_f1_score, rf_f1_score, svm_f1_score),
    auc = c(lda_auc, log_auc, qda_auc, knn_auc$AUC[1], gbm_auc, rf_auc, svm_auc),
    misclas = c(lda_misclas, log_misclas, qda_misclas, knn_misclas[1], gbm_misclas, rf_misclas, svm_misclas),
    model_training_time = c(lda_runtime, logreg_run_time, qda_runtime, knn_train_runtime, gbm_model_training_time, rf_training_time, svm_training_time)
)
print(summary_without_PCA)
metrics_df <- data.frame(summary_without_PCA)
metrics_df
```


### With PCA

```{r pc_decomp, eval = FALSE}
# Perform PCA to get first 10 principal components
pca_start <- Sys.time()

task_one_df_pca <- task_one_df %>% 
  
  dplyr::select(-label, - label_as_numeric) %>% 
  
  prcomp(center = TRUE, scale. = TRUE)

pca_end <- Sys.time()

pca_runtime <- pca_end - pca_start # Time difference of 4.244169 mins
```

```{r pca_save, eval = FALSE}
write.csv(task_one_df_pca$x, file = "predictions_1.2/task_one_df_pca.csv", row.names = TRUE)
```


```{r logreg_load, include = FALSE}
task_one_df_pca <- read.csv("predictions_1.2/task_one_df_pca.csv") %>% 
  
  dplyr::select(-X)
```

```{r pca_split}
# Split into training and test data

if(is.data.frame(task_one_df_pca$x)){
  train_data_pca <- as.data.frame(task_one_df_pca$x[train_indices, 1:10])

  test_data_pca <- as.data.frame(task_one_df_pca$x[-train_indices, 1:10])
} else {
  train_data_pca <- as.data.frame(task_one_df_pca[train_indices, 1:10])

  test_data_pca <- as.data.frame(task_one_df_pca[-train_indices, 1:10])
}

train_data_pca$label <- train_data$label

train_data_pca$label_as_numeric <- train_data$label_as_numeric

test_data_pca$label <- test_data$label

test_data_pca$label_as_numeric <- test_data$label_as_numeric
```


#### Linear Discriminant Analysis (LDA) (Imar)

```{r LDA_with_PCA}
lda_pca_model <- lda(label ~ .-label_as_numeric, data = train_data_pca)
lda_pca_pred <- predict(lda_pca_model, test_data_pca)
lda_test_data_labels <- as.factor(test_data_pca$label)
```

```{r lda_pca_confusion}
lda_pca_confusion <- table(prediction = lda_pca_pred$class, truth = as.factor(test_data_pca$label))
lda_pca_confusion
```

```{r lda_pca_accuracy}
lda_pca_accuracy <- mean(lda_pca_pred$class == test_data_pca$label)
cat("Accuracy of LDA with PCA:", lda_pca_accuracy, "\n")
```

```{r lda_pca_balanced_accuracy}
lda_pca_balanced_accuracy <- fun_calc_balanced_accuracy(lda_pca_confusion,
                                                        pos_label = "TREG",
                                                        neg_label = "CD4+T")
cat("Balanced Accuracy of LDA with PCA:", lda_pca_balanced_accuracy, "\n")
```

```{r lda_pca_f1_score}
lda_pca_f1_score <- F1_Score(y_true = test_data_pca$label, 
                             y_pred = lda_pca_pred$class)
cat("F1 Score of LDA with PCA:", lda_pca_f1_score, "\n")
```

```{r lda_pca_roc_auc}
roc_curve_lda_pca <- roc(test_data_pca$label_as_numeric, lda_pca_pred$posterior[,2], levels = c(1, 0), direction = ">")
plot(roc_curve_lda_pca, col="blue", main="ROC Curve for LDA with PCA")
lda_pca_auc <- auc(roc_curve_lda_pca)
cat("AUC of LDA with PCA:", lda_pca_auc, "\n")
```

```{r lda_pca_misclas}
lda_pca_misclas <- mean(lda_pca_pred$class != test_data_pca$label)
cat("Misclassification rate of LDA with PCA:", lda_pca_misclas, "\n")
```

```{r lda_pca_summary}
lda_pca_metrics_table <- data.frame(
  Metric = c("Accuracy", "Balanced Accuracy", "F1 Score", "AUC (ROC)", "Misclassification Rate"),
  Value = c(lda_pca_accuracy, lda_pca_balanced_accuracy, lda_pca_f1_score, lda_pca_auc, lda_pca_misclas)
)
print(lda_pca_metrics_table)
```

#### Logistic classifier (Imar)

```{r log_pca_model}
log_pca_model <- glm(label ~ . -label_as_numeric, data=train_data_pca, family=binomial)
log_pca_probs <- predict(log_pca_model, newdata = test_data_pca, type="response")
log_pca_pred <- ifelse(log_pca_probs > 0.5, "TREG", "CD4+T")
```

```{r log_pca_confusion}
log_pca_confusion <- table(prediction = log_pca_pred, truth = test_data_pca$label)
print(log_pca_confusion)
```

```{r log_pca_accuracy}
log_pca_accuracy <- mean(log_pca_pred == test_data_pca$label)
cat("Accuracy of Logistic Regression with PCA:", log_pca_accuracy, "\n")
```

```{r log_pca_balanced_accuracy}
log_pca_balanced_accuracy <- fun_calc_balanced_accuracy(log_pca_confusion,
                                                        pos_label = "TREG",
                                                        neg_label = "CD4+T")
cat("Balanced Accuracy of Logistic Regression with PCA:", log_pca_balanced_accuracy, "\n")
```

```{r log_pca_f1_score}
log_pca_f1_score <- F1_Score(y_true = test_data_pca$label, y_pred = log_pca_pred)
cat("F1 Score of Logistic Regression with PCA:", log_pca_f1_score, "\n")
```

```{r log_pca_roc_auc}
log_pca_pred_num <- ifelse(log_pca_pred == 'TREG', 1, 0)

roc_curve_log_pca <- roc(test_data_pca$label_as_numeric,
                         log_pca_pred_num, 
                         levels = c(1, 0),
                         direction = ">")
log_pca_auc <- auc(roc_curve_log_pca)
plot(roc_curve_log_pca, col="blue", main="ROC Curve for Logistic Regression with PCA")
cat("AUC of Logistic Regression with PCA:", log_pca_auc, "\n")
```

```{r log_pca_misclas}
log_pca_misclas <- mean(log_pca_pred != test_data_pca$label)
cat("Misclassification rate of Logistic Regression with PCA:", log_pca_misclas, "\n")
```

```{r Log_PCA_evaluation}
# Create a table of the evaluation metrics
log_pca_metrics_table <- data.frame(
  Metric = c("Accuracy", "Balanced Accuracy", "F1 Score", "AUC (ROC)", "Misclassification Rate"),
  Value = c(log_pca_accuracy, log_pca_balanced_accuracy, log_pca_f1_score, log_pca_auc, log_pca_misclas)
)
print(log_pca_metrics_table)
```

#### Quadratic Discriminant Analysis (QDA) (Chi)

```{r qda_pca_train}
qda_start_time <- Sys.time()
qda_pca_fit <- qda(label ~ . - label_as_numeric, data = train_data_pca)
qda_runtime <- Sys.time() - qda_start_time # Time difference of 0.01315498 secs
```

```{r qda_pca_confusion}
qda_pca_pred <- predict(qda_pca_fit, test_data_pca)$class

qda_pca_confusion <- table(predicted = qda_pca_pred,
                           observed = test_data$label)

print(qda_pca_confusion)
```

```{r qda_pca_accuracy}
qda_pca_accuracy <- mean(qda_pca_pred == test_data_pca$label)

cat("Accuracy of QDA with PCA:", log_pca_accuracy, "\n")
```

```{r qda_pca_balanced_accuracy}
qda_pca_balanced_accuracy <- fun_calc_balanced_accuracy(qda_pca_confusion,
                                                        pos_label = 'TREG',
                                                        neg_label = 'CD4+T')

cat("Balanced Accuracy of QDA with PCA:", log_pca_balanced_accuracy, "\n")
```

```{r qda_pca_f1_score}
qda_pca_f1_score <- F1_Score(y_true = test_data_pca$label, 
                             y_pred = qda_pca_pred,
                             positive = "TREG")

cat("F1 Score of QDA with PCA:", qda_pca_f1_score, "\n")
```

```{r qda_pca_roc_auc}
qda_pca_pred_numeric <- ifelse(qda_pca_pred == "TREG", 1, 0)
  
qda_pca_roc_curve <- roc(
  response = test_data_pca$label_as_numeric,
  predictor = qda_pca_pred_numeric, 
  levels = c(0,1),
  direction = "<"
  )

plot(qda_pca_roc_curve, col = "red", main = "ROC Curve for QDA")

qda_pca_auc <- auc(qda_pca_roc_curve)
qda_pca_auc
```

```{r qda_pca_misclas}
qda_pca_misclas <- mean(qda_pca_pred != test_data_pca$label)

cat("Misclassification rate of QDA with PCA:", qda_pca_misclas, "\n")
```

```{r qda_PCA_evaluation}
# Create a table of the evaluation metrics
qda_pca_metrics_table <- data.frame(
  Metric = c("Accuracy", "Balanced Accuracy", "F1 Score", "AUC (ROC)", "Misclassification Rate"),
  Value = c(qda_pca_accuracy, qda_pca_balanced_accuracy, qda_pca_f1_score, qda_pca_auc, qda_pca_misclas)
)

print(qda_pca_metrics_table)
```

#### Nearest Neighbor Classifier (k-NN) (Chi)

```{r kNN_train, eval = FALSE}
knn_train_X <- train_data_pca %>% 
  dplyr::select(-label, -label_as_numeric)

knn_train_Y <- train_data_pca$label

knn_test_X <- test_data_pca %>% 
  dplyr::select(-label, -label_as_numeric)

knn_start_time <- Sys.time()
knn_pca_pred <- sapply(kk_vector,
                   function(k)
                     {knn(train = knn_train_X, test = knn_test_X, cl = knn_train_Y, k = k)}
                   )
knn_end_time <- Sys.time()

knn_train_runtime <- knn_end_time - knn_start_time # Time difference of 0.3525879 secs

rm(knn_train_X, knn_train_Y, knn_test_X)
```

```{r kNN_Confusion}
for (i in kk_vector){
  print(paste("k =", i))
  print(table(predicted = knn_pca_pred[,i], observed = test_data$label))
}
```

```{r kNN_Misclass}
knn_misclass = c(rep(0,length(kk_vector)))

for (i in kk_vector){
  knn_misclass[i] = mean(knn_pca_pred[,i] != test_data$label)
}
plot(knn_misclass, type = "l")
```

```{r kNN_ROC}
knn_pred_numeric <- ifelse(knn_pca_pred == "TREG", 1, 0)

knn_roc_curves <- data.frame()

knn_auc <- data.frame()

for (kk in 1:ncol(knn_pred_numeric)) {
  roc_curve <- roc(
    response = test_data$label_as_numeric, 
    predictor = knn_pred_numeric[,kk],
    levels = c(1, 0),
    direction = ">"
    )
  
  roc_curve_data <- data.frame(
    k = kk,
    FPR = 1 - roc_curve$specificities,
    TPR = roc_curve$sensitivities
    )
  
  auc_data <- data.frame(
    k = kk,
    AUC = roc_curve$auc
    )
  
  knn_roc_curves <- rbind(knn_roc_curves, roc_curve_data)
  
  knn_auc <- rbind(knn_auc, auc_data)
}

knn_roc_curves$k <- as.factor(knn_roc_curves$k)

ggplot(knn_roc_curves, aes(x = FPR, y = TPR, colour = k)) + 
  geom_line(linewidth = 1) +
  labs(
    title = "ROC Curves for k-NN (k = 1 to 10)",
    x = "False Positive Rate (FPR)",
    y = "True Positive Rate (TPR)",
    color = "k"
  )

ggplot(knn_auc, aes(x = k, y = AUC)) + 
  geom_line(linewidth = 1) +
  labs(
    title = "ROC AUCs for k-NN (k = 1 to 10)",
    x = "k",
    y = "ROC AUC"
  )
```

#### Gradient Boosting Decision Trees (GBDT) (Jonathan)
```{r gbm_pca_training}
lambda <- 0.001
# Measure the time it takes to run 
gbm_start_time <- Sys.time()
# Train the gbm model with PCA data
pca_gbm_model <- gbm(label_as_numeric ~ . - label, 
                     data = train_data_pca, 
                     distribution = "bernoulli", 
                     n.trees = num_trees, 
                     interaction.depth = 4, 
                     shrinkage = lambda, 
                     cv.folds = 5, 
                     verbose = FALSE)

# Get the best number of trees based on cross-validation
best_trees_pca <- gbm.perf(pca_gbm_model, method = "cv")

# Measure time after model training
gbm_end_time <- Sys.time()
gbm_model_training_time <- gbm_end_time - gbm_start_time #Time difference of 8.224947 secs
cat("GBM Model training time with PCA:", gbm_model_training_time, "\n")

```

```{r gbm_pca_predict}
# Measure time for predictions
gbm_start_time_pred <- Sys.time()

# Make predictions on the test set using the best number of trees
gbm_pred_probs_pca <- predict(pca_gbm_model, newdata = test_data_pca, n.trees = best_trees_pca, type = "response")
gbm_pred_classes_pca <- ifelse(gbm_pred_probs_pca > 0.5, 1, 0)

# Measure time after predictions
gbm_end_time_pred <- Sys.time()
gbm_prediction_time <- gbm_end_time_pred - gbm_start_time_pred
cat("Prediction time with PCA:", gbm_prediction_time, "\n")

```

```{r gbm_pca_confusion}
# Confusion Matrix
gbm_confusion_matrix_pca <- table(Predicted = gbm_pred_classes_pca, Actual = test_data_pca$label_as_numeric)
print(gbm_confusion_matrix_pca)
```

```{r gbm_pca_balanced_accuracy}
# Balanced Accuracy
gbm_balanced_accuracy_pca <- fun_calc_balanced_accuracy(gbm_confusion_matrix_pca,
                                                        pos_label = '1',
                                                        neg_label = '0')
cat("The balanced accuracy with PCA is", gbm_balanced_accuracy_pca, "\n")
```

```{r gbm_pca_misclass}
# Misclassification Rate
gbm_misclassification_rate_pca <- mean(gbm_pred_classes_pca != test_data_pca$label_as_numeric)
cat("Misclassification Rate with PCA:", gbm_misclassification_rate_pca, "\n")
```

```{r gbm_pca_accuracy}
# Accuracy
gbm_accuracy_pca <- mean(gbm_pred_classes_pca == gbm_test_data_pca$label_as_numeric)
cat("Accuracy with PCA:", gbm_accuracy_pca, "\n")
```

```{r gbm_pca_ROC}
# ROC and AUC
gbm_roc_curve_pca <- roc(test_data_pca$label_as_numeric, gbm_pred_probs_pca)
cat("AUC with PCA:", auc(gbm_roc_curve_pca), "\n")
plot(gbm_roc_curve_pca, main = "ROC Curve with PCA")
```

```{r gbm_pca_F1}
# F1 Score
gbm_f1_score_pca <- F1_Score(y_pred = gbm_pred_classes_pca,
                             y_true = test_data_pca$label_as_numeric)
cat("F1 Score with PCA:", gbm_f1_score_pca, "\n")

```
#### Random Forest (Jonathan)
```{r rf_pca_train}
# RF model with PCA data
start_time_rf_pca <- Sys.time()

rf_pca_model <- randomForest(label ~ . - label_as_numeric, 
                             data = train_data_pca,
                             mtry = 3, 
                             importance = TRUE, 
                             n.tree = 5000)

end_time_rf_pca <- Sys.time()
rf_pca_training_time <- end_time_rf_pca - start_time_rf_pca #Time difference of 3.99099 secs
cat("Random Forest model training time with PCA:", rf_pca_training_time, "\n")
```

```{r rf_pca_prediction}
# Predictions with PCA data
start_time_rf_pred <- Sys.time()

rf_pca_pred <- predict(rf_pca_model, newdata = test_data_pca)

end_time_rf_pred <- Sys.time()
rf_prediction_time <- end_time_rf_pred - start_time_rf_pred
cat("Random Forest prediction time with PCA:", rf_prediction_time, "\n")

```

```{r rf_pca_Confusion}
rf_confusion_matrix_pca <- table(Predicted = rf_pca_pred, Actual = test_data_pca$label)
print(rf_confusion_matrix_pca)
```

```{r rf_pca_balanced_accuracy}
rf_balanced_accuracy_pca <- fun_calc_balanced_accuracy(rf_confusion_matrix_pca,
                                                       pos_label = 'TREG',
                                                       neg_label = 'CD4+T')

cat("The balanced accuracy with PCA is", rf_balanced_accuracy_pca, "\n")
```

```{r rf_pca_misclass}
rf_misclassification_rate_pca <- mean(rf_pca_pred != test_data_pca$label)
cat("Random Forest Misclassification Rate with PCA:", rf_misclassification_rate_pca, "\n")
```

```{r rf_pca_accuracy}
accuracy_rf_pca <- mean(rf_pca_pred == test_data_pca$label)
cat("Random Forest Accuracy with PCA:", accuracy_rf_pca, "\n")
```

```{r rf_pca_ROC}
roc_rf_pca <- roc(test_data_pca$label_as_numeric,
                  as.numeric(rf_pca_pred))

cat("Random Forest AUC with PCA:", auc(roc_rf_pca), "\n")
plot(roc_rf_pca, main = "Random Forest ROC Curve with PCA")
```

```{r rf_pca_F1}
# F1 Score
rf_f1_score_pca <- F1_Score(y_pred = rf_pca_pred, y_true = test_data_pca$label)
cat("F1 Score with PCA:", rf_f1_score_pca, "\n")
```

#### Support Vector Machine (SVM) (Peter)

##### Linear SVM Model
```{r svm_linear_model_pca}
## Clean the test data for the SVM task
test_data_cleaned_for_svm <- test_data_pca[, !colnames(test_data_pca) %in% c("label_as_numeric", "label")]

## K-fold CV for hyperparameter tuning of 'cost' for the training data with a linear kernel
start_time_svm <- Sys.time()

tune.out = tune(svm,
                label ~ . -label_as_numeric,
                data = train_data_pca, kernel = "linear", 
                ranges = list(cost = c(0.01, 0.1, 1, 10)))

summary(tune.out)

end_time_svm <- Sys.time()
svm_training_time <- end_time_svm - start_time_svm # Time difference of 4.748707 secs
```


```{r svm_linear_tuned_model_pca}
# Save the model with the cost that results in the lowest cross validation error rate
bestmod <-  tune.out$best.model
summary(bestmod)
```

```{r svm_predictions_pca}
start_time_svm_pred <-  Sys.time()

svm_pca_pred = predict(bestmod, test_data_pca)

end_time_svm_pred <- Sys.time()
svm_prediction_time <- end_time_svm_pred - start_time_svm_pred 
# Time difference of 0.01050401 secs
```

```{r svm_pca_confusion}
svm_pca_confusion_matrix <- table(predict = svm_pca_pred, truth = test_data_pca$label)
print(svm_pca_confusion_matrix)
```

```{r svm_pca_balanced_accuracy}
svm_pca_balanced_accuracy <- fun_calc_balanced_accuracy(svm_pca_confusion_matrix,
                                                    pos_label = "TREG",
                                                    neg_label = "CD4+T")

cat("The balanced accuracy is", svm_pca_balanced_accuracy, "\n") # 0.9418
```


```{r svm_pca_misclass}
svm_misclas <- mean(svm_pca_pred != test_data$label)
cat("SVM Misclassification Rate:", svm_misclas, "\n") # 0.0521
```

```{r svm_Accuracy}
svm_accuracy <- mean(svm_pca_pred == test_data$label)
cat("SVM Accuracy:", svm_accuracy, "\n") # 0.9479
```

```{r svm_ROC_AUC}
svm_roc <- roc(test_data$label_as_numeric,
               as.numeric(svm_pca_pred), 
               levels = c(0,1),
               direction = "<")
svm_auc <- auc(svm_roc)
svm_auc
plot(svm_roc, main = "SVM ROC Curve") # Make the limits of the axes 0-1
```

```{r rf_f1_score}
svm_f1_score <- F1_Score(y_pred = svm_pca_pred, 
                         y_true = test_data$label,
                         positive = "TREG")
cat("F1 Score:", svm_f1_score, "\n") # 0.9645
```

##### Radial SVM Model
```{r svm_radial_model}
## K-fold CV for hyperparameter tuning of 'cost' for the training data with a radial basis function (RBF) kernel
start_time_svm_rad <- Sys.time()

tune.out_rad = tune(svm,
                    label ~ . -label_as_numeric,
                    data = train_data_pca, 
                    kernel = "radial", 
                    ranges = list(cost = c(0.01, 0.1, 1, 10),
                                  gamma = c(0.00003, 0.0003, 0.003)))
summary(tune.out_rad)

end_time_svm_rad <- Sys.time()
svm_training_time_rad <- end_time_svm_rad - start_time_svm_rad
cat("SVM model training time (radial kernel):", svm_training_time_rad, "\n")

# Note that a good rule of thumb is to start with a gamma that is (1/p)
# where p is the number of features in the model. This would result in a value of
# gamma approximately 0.0003

# Save the model with the cost that results in the lowest cross validation error rate
bestmod_rad <-  tune.out_rad$best.model
summary(bestmod_rad)
```

```{r svm_predictions}
start_time_svm_pred <-  Sys.time()
svm_rad_pca_pred = predict(bestmod_rad, test_data_pca)

end_time_svm_pred <- Sys.time()
svm_prediction_time <- end_time_svm_pred - start_time_svm_pred #Time difference of 0.01661301 secs
```

```{r svm_confusion}
svm_rad_pca_confusion_matrix <- table(predict = svm_rad_pca_pred, truth = test_data_pca$label)
print(svm_confusion_matrix)
```

```{r svm_balanced_accuracy}
svm_rad_pca_balanced_accuracy <- fun_calc_balanced_accuracy(svm_rad_pca_confusion_matrix, 
                                                    pos_label = "TREG",
                                                    neg_label = "CD4+T")

cat("The balanced accuracy is", svm_balanced_accuracy, "\n") # 0.9411
```

```{r svm_Misclass}
svm_misclassification_rate <- mean(yhat.svm_rad != test_data$label)
cat("SVM Misclassification Rate:", svm_misclassification_rate, "\n") # 0.0447
```

```{r svm_Accuracy}
accuracy_svm <- mean(yhat.svm_rad == test_data$label)
cat("SVM Accuracy:", accuracy_svm, "\n") # 0.9553
```

```{r svm_ROC_AUC}
roc_svm <- roc(test_data$label, as.numeric(yhat.svm_rad))
cat("SVM AUC:", auc(roc_svm), "\n")
plot(roc_svm, main = "SVM ROC Curve") # Make the limits of the axes 0-1
```

```{r rf_f1_score}
svm_f1_score <- F1_Score(y_pred = yhat.svm_rad, 
                         y_true = test_data$label)
cat("F1 Score:", svm_f1_score, "\n") # 0.9645
```

#### Summary with PCA

insert data frame with all the metrics of the classifiers
```{r train_pca_summary}
# lda_accuracy
# lda_balanced_accuracy
# lda_f1_score
# lda_auc
# lda_misclas
# lda_runtime
# 
# log_accuracy
# log_balanced_accuracy
# log_f1_score
# log_auc
# log_misclas
# logreg_run_time
# 
# # n/a qda_accuracy
# # n/a qda_balanced_accuracy
# # n/a qda_f1_score
# qda_auc
# qda_misclas
# # n/a qda_runtime
# 
# # n/a knn_accuracy
# # n/a knn_balanced_accuracy
# # n/a knn_f1_score
# knn_auc
# knn_mislas
# knn_train_runtime
# 
# gbm_accuracy
# gbm_balanced_accuracy
# gbm_f1_score
# gbm_auc
# gbm_misclas
# gbm_model_training_time
# 
# svm_accuracy
# svm_balanced_accuracy
# svm_f1_score
# svm_auc
# svm_misclas
# svm_training_time


summary_with_PCA <- data.frame(
    Model = c("LDA", "Logistic Regression", "QDA", "KNN", "GBM", "RF", "SVM"),
    
    accuracy = c(lda_pca_accuracy, log_pca_accuracy, qda_pca_accuracy, knn_pca_accuracy, gbm_pca_accuracy, rf_pca_accuracy, svm_pca_accuracy),
    
    balanced_accuracy = c(lda_pca_balanced_accuracy, log_pca_balanced_accuracy, qda_pca_balanced_accuracy, knn_pca_balanced_accuracy, gbm_pca_balanced_accuracy, rf_pca_balanced_accuracy, svm_pca_balanced_accuracy),
    
    f1_score = c(lda_pca_f1_score),
    
    auc = c(lda_pca_auc, log_pca_auc, qda_pca_auc, knn_pca_auc$AUC[1], gbm_pca_auc, rf_pca_auc, svm_pca_auc),
    
    misclas = c(lda_pca_misclas, log_pca_misclas, qda_pca_misclas, knn_pca_misclas[1], gbm_pca_misclas, rf_pca_misclas, svm_pca_misclas),
    
    model_training_time = c(lda_runtime, logreg_run_time, qda_runtime, knn_train_runtime, gbm_model_training_time, rf_training_time, svm_training_time)

    )

display(summary_with_PCA)
```

## T1.3: Improving F1 Score (???)

Train and evaluating three classifiers of our choice with the goal of improving the F1 score (can use methods like bagging, boosting, and regularization (cross-fold validation, ridge/lasso)).

### Identifying the number of Principle Components to use 
```{r scree_plot}
variance_explained <- (task_one_df_pca$sdev)^2 / sum((task_one_df_pca$sdev)^2)

scree_data <- data.frame(
  Component = c(1:100),
  Variance = variance_explained[1:100]
)

ggplot(scree_data, aes(x = Component, y = Variance)) + 
   
  geom_line() +
  labs(title= 'Scree Plot', 
       x = 'Principal Component',
       y = 'Proportion of Variance Explained') +
  scale_x_continuous(limits = c(0, 100))
```

#### LDA/QDA? - improve through PCA and CV (Chi)

```{r}
cv_f1_score <- function(data, lev = NULL, model = NULL) {
  # Calculate confusion matrix
  cm <- table(data$pred, data$obs)
  
  tp <- cm["TREG", "TREG"]
  fp <- cm["TREG", "CD4+T"]
  fn <- cm["CD4+T", "TREG"]
  
  precision <- tp / (tp + fp)
  
  recall <- tp / (tp + fn)
  
  # Extract precision, recall, and F1-score
  f1 <- 2 * (precision * recall) / (precision + recall)
  
  accuracy <- mean(data$pred == data$obs)
  
  # Return a named vector
  c(Accuracy = accuracy, F1 = f1)
}

cv_scores <- c()

for (prin_comps in no_prin_comps) {
  
  set.seed(seed)
  
  X_pca <- task_one_df_pca[, 1:prin_comps]  # Use the first n_components
  
  # Combine PCA-transformed data with the target
  data_pca <- data.frame(X_pca, label = task_one_df$label)
  
  # Perform LDA with 5-fold cross-validation
  train_control <- trainControl(method = "cv", 
                                number = 10,
                                summaryFunction = cv_f1_score)
  
  lda_model <- train(label ~ ., 
                     data = data_pca, 
                     method = "lda", 
                     trControl = train_control)
  
  # Store the cross-validation accuracy
  cv_scores <- c(cv_scores, lda_model$results$F1)
}
```


#### Logistic Regression 

##### with Ridge Regression (Regularisation)

```{r}
logreg_cv_time <- Sys.time()
logreg_cv_scores <- data.frame(no_pc = c(),
                        alpha = c(),
                        lambda = c(),
                        accuracy = c(),
                        f1 = c())

for (prin_comps in no_prin_comps) {
  
  set.seed(seed)
  
  X_pca <- task_one_df_pca[, 1:prin_comps]  # Use the first n_components
  
  # Combine PCA-transformed data with the target
  data_pca <- data.frame(X_pca, label = task_one_df$label)
  
  # Perform LDA with 5-fold cross-validation
  train_control <- trainControl(method = "cv", 
                                number = 10,
                                summaryFunction = cv_f1_score)
  
  logistic_regression_model <- train(label ~ ., 
                                     data = data_pca, 
                                     method = "glmnet", 
                                     trControl = train_control,
                                     tuneGrid = expand.grid(
                                       alpha = seq(0, 1, 0.25),
                                       lambda = lambda_grid
                                     ))
  
  result <- logistic_regression_model$results
    
  scores <- data.frame(no_pc = c(rep(prin_comps, length(result$alpha))),
                       alpha = c(result$alpha),
                       lambda = c(result$lambda),
                       accuracy = c(result$Accuracy),
                       f1 = c(result$F1))  
  
  # Store the cross-validation accuracy
  logreg_cv_scores <- rbind(logreg_cv_scores, scores)
}

logreg_cv_time <- Sys.time() - logreg_cv_time # Time difference of 3.394877 mins
```

```{r}
graph_cv <- logreg_cv_scores %>% filter(no_pc == 30)

ggplot(graph_cv, aes(x = alpha, y = lambda, fill = f1, labels = f1)) +

  geom_tile() + 
  
  scale_fill_gradient(low = 'white', high = 'blue', limits = c(0.94,0.95)) + 
  
  geom_text(aes(label = round(f1, 5)), color = 'black', size = 3)
  
# alpha=1 for Lasso, alpha=0 for Ridge, or between 0 and 1 for Elastic Net
```


#### Logistic Regression with Ridge Regression (Regularisation)

```{r}
logreg_start_time <- Sys.time()

# Prepare the input data
x_train_log_regularized <- as.matrix(train_data[, setdiff(names(train_data), "label")])
y_train_log_regularized <- train_data$label_as_numeric

# Fit a regularized logistic regression model
logreg_fit_regularized <- glmnet(x_train_log_regularized, y_train_log_regularized, family = "binomial", alpha = 0)  
# alpha=1 for Lasso, alpha=0 for Ridge, or between 0 and 1 for Elastic Net

logreg_run_time <- Sys.time() - logreg_start_time 

# Make predictions on the test data
x_test_log_regularized <- as.matrix(test_data[, setdiff(names(test_data), "label")])
logreg_probs_regularized <- predict(logreg_fit_regularized, newx = x_test_log_regularized, type = "response", s = 0.1)  # s = 0.1 is the regularization parameter

logreg_pred_ridge <- ifelse(logreg_probs_regularized > 0.5, 'TREG', 'CD4+T')
```

```{r}
log_ridge_confusion <- table(prediction = logreg_pred_ridge, truth = test_data$label)
log_ridge_confusion
```

```{r log_pca_f1_score}
log_ridge_f1_score <- F1_Score(y_true = test_data$label, y_pred = logreg_pred_ridge)
cat("F1 Score of Logistic Regression with Ridge Regularization:", log_ridge_f1_score, "\n")
```

graph for f1 score with changing the s parameter = regularization parameter

##### with Boosting??


#### Random Forest with CV (Imar)

Hyperparameter tuning (cross-validation): Number of trees (n_estimators): Increasing the number of trees generally improves accuracy, but can slow down the model. Maximum tree depth (max_depth): Controls how deep each tree can grow, preventing overfitting with a smaller depth. Minimum samples per leaf (min_samples_leaf): Sets the minimum number of samples required to split a node, preventing overfitting. Number of features considered at each split (max_features): Controls the randomness in the feature selection process.

```{r}
# Define a custom grid of hyperparameters for tuning
rf_cv_grid <- expand.grid(
  mtry = c(30, 50, 64, 80), # Number of features considered at each split
  splitrule = c("gini", "extratrees"), # Criterion for splitting nodes
  min.node.size = c(1, 5, 10),         # Minimum number of samples per leaf
  ntree = c(100, 500, 1000)            # Number of trees in the forest
)

# Set up 5-fold cross-validation
rf_cv_ctrl <- trainControl(
  method = "cv",                      # Cross-validation method
  number = 5,                         # Number of folds
  summaryFunction = twoClassSummary,  # Use F1 score, ROC, etc.
  classProbs = TRUE,                  # To compute probabilities
  savePredictions = "final"           # Save the final predictions
)

# Train the Random Forest model using cross-validation
start_time_rf_cv <- Sys.time()
rf_cv_model <- randomForest(label ~ . - label_as_numeric, data = train_data, method = "rf", trControl = rf_cv_ctrl, tuneGrid = rf_cv_grid, metric = "F1", importance = TRUE)
end_time_rf_cv <- Sys.time()
rf_cv_training_time <- end_time_rf_cv - start_time_rf_cv
cat("Random Forest model training time:", rf_cv_training_time, "\n")

# trControl = rf_cv_ctrl # Cross-validation control
# tuneGrid = rf_cv_grid # Hyperparameter grid
# metric = "F1" # Optimize for F1 score
# importance = TRUE # To calculate feature importance

# Predictions on the test set
start_time_rf_cv_pred <- Sys.time()
rf_cv_pred <- predict(rf_cv_model, newdata = test_data)
end_time_rf_cv_pred <- Sys.time()
rf_cv_prediction_time <- end_time_rf_cv_pred - start_time_rf_cv_pred
cat("Random Forest prediction time:", rf_cv_prediction_time, "\n")
```

```{r}
# Check the performance on the test data
confusionMatrix(rf_cv_pred, test_data$label)
```

```{r}
rf_cv_f1_score <- F1_Score(y_true = test_data$label, y_pred = rf_cv_pred)
cat("F1 Score of Random Forest with CV:", rf_cv_f1_score, "\n")
```

## T1.4: Implement predictor (???)

Choose the best approach from those above tested and implement our predictor as a function of our code.

```{r}

```
