---
title: "Task 1 RmD"
output: html_document
date: "2024-11-12"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn=-1)
```

```{r parameters}
seed = 443
csv_file_one = "data1.csv"
training_size = 0.7 # Proportion of dataset that is to be split for training
packages = list("class", 
                "randomForest",
                "tree",
                "dplyr",
                "tidyr",
                "gbm", 
                "MASS",
                "ggplot2",
                "pROC",
                "MLmetrics",
                "caret"
)

```

```{r, include=FALSE}
for (package in packages){
  if(!require(package, character.only = TRUE)){
    install.packages(package, character.only = TRUE)
  } else {
    library(package, character.only = TRUE)
  }
}
```

# Task 1: binary classification

```{r train_test_split}
set.seed(seed)

task_one_df <- read.csv(csv_file_one)

# str(task_one_df)
task_one_df$label_as_factor <- as.factor(task_one_df$label)
task_one_df$label_as_numeric <- as.numeric(task_one_df$label_as_factor) -1 
# Split Data into Test and Train (We should all use the same data)

train_split = as.integer(nrow(task_one_df)*training_size)

train_indices <- sample(1:nrow(task_one_df), train_split)
train_data <- task_one_df[train_indices,]
test_data <- task_one_df[-train_indices,]
```

## T1.1: exploratory data analysis and summary statistics (Peter?)

```{r}
nrow(task_one_df) # The data has 5471 observations
ncol(task_one_df) # The data contains 4125 variables-- 4124 originally, 1 added (label_as_factor)
table(task_one_df$label) # 3356 observations are labelled as CD4+T and 2115 are labelled as TREG

mean_table <- task_one_df %>%

  dplyr::select(-label, -label_as_numeric) %>%

  group_by(label_as_factor) %>%

  summarise(across(everything(), mean, na.rm = TRUE)) %>%

  pivot_longer(cols = -label_as_factor, names_to = 'genes', values_to = 'mean_value') %>%

  pivot_wider(names_from = label_as_factor, values_from = mean_value) %>%

  mutate(diff = abs(`CD4+T` - TREG)) %>%

  mutate(pct_diff = 100 * (diff / ((`CD4+T` + TREG)/2) )) %>%

  arrange(desc(diff))

## There are decent differences between the ranking of absolute difference in means of groups
## and the ranking of percentage difference in means of groups
## perhaps we create box plots of the top 5 of each metric??

### Next steps --> 
  ### (1) instead of absolute value difference, include also
  ### percentage change. then plot box plots for the top 5? 10? variables
  ### that appear to have the clearest distinction between the two labels
  ### these specific variables provide a good baseline for what we expect
  ### our subsequent models to choose for the most important fields
  
## Checking if there are any rows that have missing values... Keep this hashed unless you want to check, because it takes a while to run
# for (i in 1:nrow(task_one_df)) {
#   if (any(is.na(task_one_df[i, ]))) {
#     print(paste("Row", i, "contains NA values."))
#   }
# }

twenty_random_columns <- sample(names(task_one_df)[-1], 20)

## Create boxplots for the random sample of columns
for (column in twenty_random_columns) {
    p <- ggplot(task_one_df, aes_string(x = "label", y = column)) +
        geom_jitter(width = 0.2, height = 0) +  # Use jitter to avoid overplotting
        ggtitle(paste("Scatter Plot of", column, "by Label")) +
        theme_minimal()
    print(p)
}

```

## T1.2: training and evaluating on various classifiers

### Without PCA

#### Linear Discriminant Analysis (LDA) (Imar)

```{r}

```

#### Logistic classifier (Imar)

```{r log regression with 300 predictors}
# 1.) setting up subset dataframe
train_data_subset1 <- train_data[, c("label", names(train_data)[2:301])]
train_data_subset1$label_as_factor <- as.factor(train_data_subset1$label)
train_data_subset1$label_as_numeric <- as.numeric(train_data_subset1$label_as_factor)-1
train_data_subset1$label <- train_data_subset1$label_as_numeric
train_data_subset1 <- train_data_subset1 %>%
  dplyr::select(-label_as_factor, -label_as_numeric)

# 2.) Log Regression on data subset
log_formula <- as.formula(
  paste("label_as_numeric ~", paste(names(train_data_subset1)[-1], collapse = " + ")))
glm_fit_log <- glm(log_formula, data=train_data, family = binomial)
summary(glm_fit_log)
```

```{r log regression confusion}

```

```{r log regression Missclassification error rate}

```

```{r log regression ROC}

```

```{r log regression with ALL predictors}

log_regression <- glm(label_as_numeric ~ . -label -label_as_factor, data = train_data, family = binomial)
```

#### Quadratic Discriminant Analysis (QDA) (Chi)

```{r qda_Train}
options(expressions = 10000)
qda_covariates = mean_table$genes[1:1100]

qda_formula = as.formula(
  paste(
    "label_as_factor ~",
    paste(qda_covariates, collapse = " + ")
    ))

qda_fit <- qda(qda_formula, data = train_data)
```

```{r qda_Confusion}
qda_pred <- predict(qda_fit, test_data)$class

table(predicted = qda_pred, observed = test_data$label)
```

```{r qda_Misclass}
qda_misclass_rate = mean(qda_pred != test_data$label)

cat("Misclassification rate:", qda_misclass_rate*100, "%")
```

```{r qda_ROC}
qda_pred_numeric <- ifelse(qda_pred == "TREG", 1, 0)
  
qda_roc_curve <- roc(
  response = test_data$label_as_numeric,
  predictor = qda_pred_numeric, 
  levels = c(0,1),
  direction = "<"
  )

plot(qda_roc_curve, col = "red", main = "ROC Curve for QDA")

qda_auc <- auc(qda_roc_curve)
qda_auc
```

#### Nearest Neighbor Classifier (k-NN) (Chi)

```{r kNN_train}
kk_vector = c(3:7) 

knn_train_X <- train_data %>% 
  dplyr::select(-label, -label_as_factor, label_as_numeric)

knn_train_Y <- train_data$label_as_factor

knn_test_X <- test_data %>% 
  dplyr::select(-label, -label_as_factor, label_as_numeric)

knn_test_Y <- test_data$label_as_factor

knn_start_time <- Sys.time()
knn_pred <- sapply(kk_vector,
                   function(k)
                     {knn(train = knn_train_X, test = knn_test_X, cl = knn_train_Y, k = k)}
                   )
knn_end_time <- Sys.time()

knn_train_runtime <- knn_end_time - knn_start_time
print(knn_train_runtime)

write.csv(knn_pred, file = "knn_predictions.csv", row.names = TRUE)
```

```{r kNN_Confusion}
for (i in kk_vector){
  print(paste("k =", i))
  print(table(predicted = knn_pred[,i], observed = knn_test_Y))
}
```

```{r kNN_Misclass}
knn_misclass = c(rep(0,length(kk_vector)))
for (i in kk_vector){
  knn_misclass[i] = mean(knn_pred[,i] != knn_test_Y)
}
plot(knn_misclass, type = "l")
```

#### Gradient Boosting Decision Trees (GBDT) (Jonathan)

```{r gbmparams}
#Number of trees
num_trees = 1000
# Define grid of lambda (shrinkage) values to evaluate
lambda_grid <- c(0.001, 0.01,0.03,0.05)

# Initialize a vector to store test errors for each lambda
test_errors <- numeric(length(lambda_grid))
```

```{r dataprepgbm}
test_data_cleaned <- test_data[, !colnames(test_data) %in% c("label_as_factor", "label")]
```

```{r gbmloop, eval=FALSE}

# Loop over each lambda value
for (i in seq_along(lambda_grid)) {
  lambda <- lambda_grid[i]
  
  # Train the gbm model with the current lambda (shrinkage) value
  gbm_model <- gbm(label_as_numeric ~. -label - label_as_factor, data = train_data, 
                   distribution = "bernoulli", 
                   n.trees = num_trees, 
                   interaction.depth = 4, 
                   shrinkage = lambda, 
                   cv.folds = 5, 
                   verbose = FALSE)
  
  # Make predictions on the test set using the optimal number of trees
  predictions <- predict(gbm_model, newdata = test_data_cleaned, n.trees = num_trees)
  
  # Calculate the Mean Squared Error on the test set
  test_errors[i] <- mean((predictions - test_data_cleaned$label_as_numeric)^2)
}
```

```{r gbmtse, eval=FALSE}
# Combine lambda values and test errors into a data frame for plotting
error_df <- data.frame(lambda = lambda_grid, test_error = test_errors)

# Plot the test error over the range of lambda values
ggplot(error_df, aes(x = lambda, y = test_error)) +
  geom_line() +
  geom_point() +
  labs(title = "Test Error vs Lambda (Shrinkage)", 
       x = "Lambda (Shrinkage)", 
       y = "Test Mean Squared Error") +
  theme_minimal()

```

The results above show use that the GLM model with the lowest Test Mean Squared Error, will be the one with the lowest lambda value we assigned of 0.001. This does not necessarily mean that the best Gradient Descent Boosting model that will work best will be the best model for our data, as it may not account for over fitting. However, we will continue using the GBDT model with a shrinkage value of 0.001 to analyze the model and to compare it to our other models.

```{r gbm_model}
# Set a single shrinkage value
lambda <- 0.001

# Measure the time it takes to run 
gbm_start_time <- Sys.time()
# Train the gbm model
smallest_lambda_gbm_model <- gbm(label_as_numeric ~. -label - label_as_factor, 
                 data = train_data, 
                 distribution = "bernoulli", 
                 n.trees = num_trees, 
                 interaction.depth = 4, 
                 shrinkage = lambda, 
                 cv.folds = 5, 
                 verbose = FALSE)

# Get the best number of trees based on cross-validation
best_trees <- gbm.perf(smallest_lambda_gbm_model, method = "cv")

# Measure time after model training
gbm_end_time <- Sys.time()
gbm_model_training_time <- gbm_end_time - gbm_start_time
cat("GBM Model training time:", gbm_model_training_time, "\n")

# Measure time for predictions
gbm_start_time_pred <- Sys.time()
# Make predictions on the test set using the best number of trees
gbm_pred_probs <- predict(smallest_lambda_gbm_model, newdata = test_data_cleaned, n.trees = best_trees, type = "response")
gbm_pred_classes <- ifelse(gbm_pred_probs > 0.5, 1, 0)

# Measure time after predictions
gbm_end_time_pred <- Sys.time()
gbm_prediction_time <- gbm_end_time_pred - gbm_start_time_pred
cat("Prediction time:", gbm_prediction_time, "\n")
```

```{r gbm_Confusion}
gbm_confusion_matrix <- table(Predicted = gbm_pred_classes, Actual = test_data_cleaned$label_as_numeric)
print(gbm_confusion_matrix)
```

```{r gbm_Misclass}
gbm_misclassification_rate <- mean(gbm_pred_classes != test_data_cleaned$label_as_numeric)
cat("Misclassification Rate:", gbm_misclassification_rate, "\n")
```

```{r gbm_Accuracy}
gbm_accuracy <- mean(gbm_pred_classes == test_data_cleaned$label_as_numeric)
cat("Accuracy:", gbm_accuracy, "\n")
```

```{r gbm_ROC_AUC}
gbm_roc_curve <- roc(test_data_cleaned$label_as_numeric, gbm_pred_probs)
cat("AUC:", auc(gbm_roc_curve), "\n")
plot(gbm_roc_curve, main = "ROC Curve")
```

```{r gbm_f1_score}
gbm_f1_score <- F1_Score(y_pred = gbm_pred_classes, y_true = test_data_cleaned$label_as_numeric)
cat("F1 Score:", gbm_f1_score, "\n")
```

The accuracy for the gradient descent decision tree model (where the shrinkage term was equal to 0.001) was 0.904. The Area under the curve was equal to 0.972 and the Test mean squared error was equal to 0.109. Misclassification rate was equal to 0.096. And F1 score was equal to 0.93

#### Random Forest (Jonathan)

```{r tree_init}
## Clean the test data for the Random Forest Classifier task
test_data_cleaned_for_rf <- test_data[, !colnames(test_data) %in% c("label_as_numeric", "label")]

## Running Tree on all the variables on train data
tree.rna_features <- tree(label_as_factor ~ . -label_as_numeric -label, split="deviance", data = train_data)
summary(tree.rna_features)
plot(tree.rna_features)
text(tree.rna_features, pretty = 0, cex = 1)

## Predict on Test variables
tree.pred <-predict(tree.rna_features, test_data, type="class")

## Mis-classification Rate and Confusion Matrix
table(tree.pred,test_data_cleaned_for_rf$label_as_factor)
mean(tree.pred!=test_data_cleaned_for_rf$label_as_factor)

```

```{r treecv}
cv.rna_data <- cv.tree(tree.rna_features)
plot(cv.rna_data$size, cv.rna_data$dev, type ='b')
```

Above we use a tree search to find.... When using the GBDT, we get that there are 11 features that are the most important to classifying whether a cell is a T Regulatory Cell or a CD4-positive cell. However, we can see based on the tree that there are certain features that appear more than once. Based on the tree plot, we can determine that the most important regressor that helps determine if a cell is a Regulatory T cell or a CD4-positive cell is the IL7R. From the National Library of medicine ("<https://www.ncbi.nlm.nih.gov/gene/3575>"), IL7R is a protein encoded by a gene that plays a critical role in the development of lymph nodes. Based on the analysis above, we decided to create a Random Forest regression that will have 12 terminal nodes.

```{r rfmodel}

# RF model
start_time_rf <- Sys.time()

rf.rna_features <-randomForest(label_as_factor~. - label_as_numeric -label, data=train_data, mtry=12, importance=TRUE, n.tree = 5000) # RF Model

end_time_rf <- Sys.time()
rf_training_time <- end_time_rf - start_time_rf
cat("Random Forest model training time:", rf_training_time, "\n")

# Predictions
start_time_rf_pred <- Sys.time()
yhat.rf <-predict(rf.rna_features, newdata=test_data)

end_time_rf_pred <- Sys.time()
rf_prediction_time <- end_time_rf_pred - start_time_rf_pred
cat("Random Forest prediction time:", rf_prediction_time, "\n")



```

```{r rf_Confusion}
performance_metrics <- confusionMatrix(yhat.rf, test_data_cleaned_for_rf$label_as_factor)
print(performance_metrics)
```

```{r rf_Misclass}
rf_misclassification_rate <- mean(yhat.rf != test_data_cleaned_for_rf$label_as_factor)
cat("Random Forest Misclassification Rate:", rf_misclassification_rate, "\n")
```

```{r rf_Accuracy}
accuracy_rf <- mean(yhat.rf == test_data_cleaned_for_rf$label_as_factor)
cat("Random Forest Accuracy:", accuracy_rf, "\n")
```

```{r rf_ROC_AUC}
roc_rf <- roc(test_data_cleaned_for_rf$label_as_factor, as.numeric(yhat.rf))
cat("Random Forest AUC:", auc(roc_rf), "\n")
plot(roc_rf, main = "Random Forest ROC Curve")
```

```{r rf_f1_score}
rf_f1_score <- F1_Score(y_pred = yhat.rf, y_true = test_data_cleaned_for_rf$label_as_factor)
cat("F1 Score:", rf_f1_score, "\n")
```

Using a random forest tree model, we see that the accuracy of the model was 0.0.891 percent. Which is quite accurate! Misclassification Rate was equal to 0.898. The F1 score was equal to 0.924.

#### Support Vector Machine (SVM) (Peter)

```{r}

```

### With PCA

#### Linear Discriminant Analysis (LDA) (Imar)

```{r}

```

#### Logistic classifier (Imar)

```{r}

```

#### Quadratic Discriminant Analysis (QDA) (Chi)

```{r}

```

#### Nearest Neighbor Classifier (k-NN) (Chi)

```{r}

```

#### Gradient Boosting Decision Trees (GBDT) (Jonathan)

```{r}

```

#### Random Forest (Jonathan)

```{r}

```

#### Support Vector Machine (SVM) (Peter)

```{r}

```

## T1.3: Improving F1 Score (???)

Train and evaluating three classifiers of our choice with the goal of improving the F1 score (can use methods like bagging, boosting, and regularization).

```{r}

```

## T1.4: Implement predictor (???)

Choose the best approach from those above tested and implement our predictor as a function of our code.

```{r}

```
