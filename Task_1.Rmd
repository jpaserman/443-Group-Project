---
title: "Task 1 RmD"
output: html_document
date: "2024-11-12"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn=-1)
```

```{r parameters}
seed = 443
csv_file_one = "data1.csv"
packages = list("caret", 
                "randomForest",
                "tree",
                "dplyr",
                "tidyr", "caret"
)

```

```{r, include=FALSE}
for (package in packages){
  if(!require(package, character.only = TRUE)){
    install.packages(package, character.only = TRUE)
  } else {
    library(package, character.only = TRUE)
  }
}
```

# Task 1: binary classification

```{r}
task_one_df <- read.csv(csv_file_one)

str(task_one_df)
task_one_df$label_as_factor <- as.factor(task_one_df$label)
```

## T1.1: exploratory data analysis and summary statistics (Peter?)

```{r}
table(task_one_df$label)
library(ggplot2)
nrow(task_one_df)
ncol(task_one_df)

mean_table <- task_one_df %>% 
  
  select(!label) %>%
  
  group_by(label_as_factor) %>%
  
  summarise(across(everything(), mean, na.rm = TRUE)) %>% 
  
  pivot_longer(cols = -label_as_factor, names_to = 'genes', values_to = 'mean_value') %>%
  
  pivot_wider(names_from = label_as_factor, values_from = mean_value) %>%
  
  mutate(diff = abs(`CD4+T` - TREG)) %>% 
  
  arrange(desc(diff))
  
  
## Checking if there are any rows that have missing values... Keep this hashed unless you want to check, because it takes a while to run
# for (i in 1:nrow(task_one_df)) {
#   if (any(is.na(task_one_df[i, ]))) {
#     print(paste("Row", i, "contains NA values."))
#   }
# }

twenty_random_columns <- sample(names(task_one_df)[-1], 20)

## Create boxplots for the random sample of columns
for (column in twenty_random_columns) {
    p <- ggplot(task_one_df, aes_string(x = "label", y = column)) +
        geom_jitter(width = 0.2, height = 0) +  # Use jitter to avoid overplotting
        ggtitle(paste("Scatter Plot of", column, "by Label")) +
        theme_minimal()
    print(p)
}

# Split Data into Test and Train (We should all use the same data)
train_indeces <-sample(1:nrow(task_one_df), 3829) # 3829 is 70% of the data
train_data <- task_one_df[train_indeces,]
test_data <-task_one_df[-train_indeces,]
```

## T1.2: training and evaluating on various classifiers

### Without PCA

#### Linear Discriminant Analysis (LDA) (Imar)

```{r}

```

#### Logistic classifier (Imar)

```{r}

```

#### Quadratic Discriminant Analysis (QDA) (Chi)

```{r}

```

#### Nearest Neighbor Classifier (k-NN) (Chi)

```{r}

```

#### Gradient Boosting Decision Trees (GBDT) (Jonathan)

```{r}
## Running Tree on all the variables on train data
tree.rna_features <- tree(label_as_factor ~ ., split="deviance", data = train_data)
summary(tree.rna_features)
plot(tree.rna_features)
text(tree.rna_features, pretty = 0, cex = 1)

## Predict on Test variables
tree.pred <-predict(tree.rna_features, test_data, type="class")

## Mis-classification Rate and Confusion Matrix
table(tree.pred,test_data$label_as_factor)
mean(tree.pred!=test_data$label_as_factor)

```

```{r}
cv.rna_data <- cv.tree(tree.rna_features)
plot(cv.rna_data$size, cv.rna_data$dev, type ='b')
```

Above we use a  tree search to find....  When using the GBDT, we get that there are 11 features that are the most important to classifying whether a cell is a T Regulatory Cell or a CD4-positive cell. However, we can see based on the tree that there are certain features that appear more than once. Based on the tree plot, we can determine that the most important regressor that helps determine if a cell is a Regulatory T cell or a CD4-positive cell is the IL7R. From the National Library of medicine ("https://www.ncbi.nlm.nih.gov/gene/3575"), IL7R is a protein encoded by a gene that plays a critical role in the development of lymph nodes. 

```{r}
prune.rna_features <- prune.tree(tree.rna_features, best = 6) #best argument sets # terminal nodes desired.
plot(prune.rna_features)
text(prune.rna_features, pretty = 0)
```

#### Random Forest (Jonathan)

```{r}
rna_features.test <- test_data$label_as_factor
rf.rna_features <-randomForest(label_as_factor~., data=train_data, mtry=14, importance=TRUE, n.tree = 5000)
yhat.rf <-predict(rf.rna_features, newdata=test_data)

# Calculate precision, recall, F1-score (using `caret` package)
library(caret)
performance_metrics <- confusionMatrix(yhat.rf, test_data$label_as_factor)
print(performance_metrics)

```


#### Support Vector Machine (SVM) (Peter)

```{r}

```

### With PCA

#### Linear Discriminant Analysis (LDA) (Imar)

```{r}

```

#### Logistic classifier (Imar)

```{r}

```

#### Quadratic Discriminant Analysis (QDA) (Chi)

```{r}

```

#### Nearest Neighbor Classifier (k-NN) (Chi)

```{r}

```

#### Gradient Boosting Decision Trees (GBDT) (Jonathan)

```{r}

```

#### Random Forest (Jonathan)

```{r}

```

#### Support Vector Machine (SVM) (Peter)

```{r}

```

## T1.3: Improving F1 Score (???)

Train and evaluating three classifiers of our choice with the goal of improving
the F1 score (can use methods like bagging, boosting, and regularization).

```{r}

```


## T1.4: Implement predictor (???)

Choose the best approach from those above tested and implement our predictor
as a function of our code.

```{r}

```


