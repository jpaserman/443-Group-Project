---
title: "Task 1 RmD"
output:
  pdf_document: default
  html_document: default
date: "2024-11-12"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn=-1)
```

```{r parameters}
seed = 443
csv_file_one = "data1.csv"
training_size = 0.8 # Proportion of dataset that is to be split for training
packages = list("class", 
                "randomForest",
                "tree",
                "dplyr",
                "tidyr",
                "gbm", 
                "MASS",
                "ggplot2",
                "pROC",
                "MLmetrics",
                "caret"
)

# For knn-training
kk_vector = c(2:7) 
```

```{r, include=FALSE}
for (package in packages){
  if(!require(package, character.only = TRUE)){
    install.packages(package, character.only = TRUE)
  } else {
    library(package, character.only = TRUE)
  }
}
```

# Task 1: binary classification

```{r train_test_split}
set.seed(seed)

task_one_df <- read.csv(csv_file_one)

# str(task_one_df)
task_one_df$label_as_factor <- as.factor(task_one_df$label)
task_one_df$label_as_numeric <- as.numeric(task_one_df$label_as_factor) -1 #Labels cells that are T-Reg cells as 1 and CD4 cells as 0
# Split Data into Test and Train (We should all use the same data)

train_split = as.integer(nrow(task_one_df)*training_size)

train_indices <- sample(1:nrow(task_one_df), train_split)
train_data <- task_one_df[train_indices,]
test_data <- task_one_df[-train_indices,]
```

## T1.1: exploratory data analysis and summary statistics (Peter?)

```{r}
nrow(task_one_df) # The data has 5471 observations
ncol(task_one_df) # The data contains 4125 variables-- 4124 originally, 1 added (label_as_factor)
table(task_one_df$label) # 3356 observations are labelled as CD4+T and 2115 are labelled as TREG

mean_table <- task_one_df %>%

  dplyr::select(-label, -label_as_numeric) %>%

  group_by(label_as_factor) %>%

  summarise(across(everything(), mean, na.rm = TRUE)) %>%

  pivot_longer(cols = -label_as_factor, names_to = 'genes', values_to = 'mean_value') %>%

  pivot_wider(names_from = label_as_factor, values_from = mean_value) %>%

  mutate(diff = abs(`CD4+T` - TREG)) %>%

  mutate(pct_diff = 100 * (diff / ((`CD4+T` + TREG)/2) )) %>%

  arrange(desc(diff))

## There are decent differences between the ranking of absolute difference in means of groups
## and the ranking of percentage difference in means of groups
## perhaps we create box plots of the top 5 of each metric??

### Next steps --> 
  ### (1) instead of absolute value difference, include also
  ### percentage change. then plot box plots for the top 5? 10? variables
  ### that appear to have the clearest distinction between the two labels
  ### these specific variables provide a good baseline for what we expect
  ### our subsequent models to choose for the most important fields
  
## Checking if there are any rows that have missing values... Keep this hashed unless you want to check, because it takes a while to run
# for (i in 1:nrow(task_one_df)) {
#   if (any(is.na(task_one_df[i, ]))) {
#     print(paste("Row", i, "contains NA values."))
#   }
# }

twenty_random_columns <- sample(names(task_one_df)[-1], 20)

## Create boxplots for the random sample of columns
for (column in twenty_random_columns) {
    p <- ggplot(task_one_df, aes_string(x = "label", y = column)) +
        geom_jitter(width = 0.2, height = 0) +  # Use jitter to avoid overplotting
        ggtitle(paste("Scatter Plot of", column, "by Label")) +
        theme_minimal()
    print(p)
}

```

## T1.2: training and evaluating on various classifiers

### Without PCA

#### Linear Discriminant Analysis (LDA) (Imar)

```{r LDA}

lda_fit <- lda(label_as_factor ~ . -label -label_as_numeric, data=train_data)

lda_pred_posterior <- predict(lda_fit, test_data)
lda_pred <- predict(lda_fit, test_data)$class
head(predict(lda_fit, test_data)$posterior)
```

LDA Performance evaluation

```{r LDA Confusion matrix}
confusion_matrix_lda <- table(lda_pred, truth = test_data$label_as_factor)
confusion_matrix_lda
```

The Confusion matrix here shows how many cells were correctly classified as either CD4+T or TREG but also at the same time, how many cells were incorrectly classified. Hence we are able to see the four respective classification categories: True Positives, False Positives, True Negatives, False Negatives.

```{r LDA: Accuracy}
# TP_lda <- confusion_matrix_lda["TREG", "TREG"]  # True Positives
# FP_lda <- confusion_matrix_lda["CD4+T", "TREG"]  # False Positives
# TN_lda <- confusion_matrix_lda["CD4+T", "CD4+T"]  # True Negatives
# FN_lda <- confusion_matrix_lda["TREG", "CD4+T"]  # False Negatives

lda_accuracy <- mean(lda_pred == test_data$label)
cat("Accuracy of LDA:", lda_accuracy, "\n")
```

The accuracy of \~69% indicates that the LDA model accurately estimated 69% of all cells which were in the test data.

```{r LDA: Balanced Accuracy}
balanced_accuracy_lda <- 1/2*(TP_lda/(TP_lda+FN_lda))+1/2*(TN_lda/(TN_lda+FN_lda))
cat("Balenced Accuracy of LDA:", balanced_accuracy_lda, "\n")

# sum(test_data$label == "CD4+T"); sum(test_data$label == "TREG") # checking the amount of both classes present in the test_data set
```

The balanced accuracy of \~65% accounts furthermore for class imbalance by giving equal weight to the accuracy of both classes, shown by True Positives and True Negatives. Since there are 1017 CD4+T cells and 625 TREG cells, the balanced accuracy metric might be more accurate due to the fact that there are 1.6 times more CD4+T cells than TREG cells, implying a slightly imbalanced test data set.

```{r LDA: F1 Score}
lda_f1_score <- F1_Score(y_true= test_data$label, y_pred = lda_pred)
cat("F1 Score of LDA:", lda_f1_score, "\n")
```

```{r LDA: ROC curve & AUC}
roc_curve_lda <- roc(
  test_data$label_as_factor, predict(lda_fit, test_data)$posterior[,2], 
  levels= c("TREG", "CD4+T"), direction =">")
plot(roc_curve_lda, col="blue", main = "ROC Curve for LDA")

lda_auc <- auc(roc_curve_lda)
cat("The AUC of the ROC of LDA:", lda_auc, "\n")
```

A higher AUC value indicates a better performing model. AUC is particularly useful when the data is imbalanced, as it evaluates the model's performance across all classification thresholds. Intuition: It answers the question: "How good is the model at distinguishing between the positive and negative classes?"

```{r Misclassification error rate}
mean(lda_pred != test_data$label_as_factor)
```

#### Logistic classifier (Imar)

```{r log regression with 300 predictors}
### Imar Log
# 1.) setting up subset dataframe
# train_data_subset1 <- train_data[, c("label", names(train_data)[2:301])]
# train_data_subset1$label_as_factor <- as.factor(train_data_subset1$label)
# train_data_subset1$label_as_numeric <- as.numeric(train_data_subset1$label_as_factor)-1
# train_data_subset1$label <- train_data_subset1$label_as_numeric
# train_data_subset1 <- train_data_subset1 %>%
#   dplyr::select(-label_as_factor, -label_as_numeric)

# 2.) Log Regression on data subset
# log_formula <- as.formula(
#   paste("label_as_numeric ~", paste(names(train_data_subset1)[-1], collapse = " + "))


### Jonathan Log
# glm_train_data <- train_data[,!colnames(train_data) %in% c("label_as_factor", "label")]
# glm_start_time <- Sys.time()
# glm_fit_log <- glm(label_as_numeric ~., data=glm_train_data, family = binomial)
# 
# glm_end_time <- Sys.time()
# glm_time_to_run <- glm_end_time - glm_start_time
# cat("Logistic Model training time:", glm_time_to_run, "\n")
# summary(glm_fit_log)
# 
# 
# test_data_cleaned_log <- test_data[, !colnames(test_data) %in% c("label_as_factor", "label")]
# glm_probs <- predict(glm_fit_log, newdata = test_data_cleaned_log, type = "response")
# glm_pred <- ifelse(glm_probs > 0.5, "TREG", "CD4+T")

train_split_log = as.integer(nrow(task_one_df)*0.8)

train_indices_log <- sample(1:nrow(task_one_df), train_split)
train_data_log <- task_one_df[train_indices_log,]
test_data_log <- task_one_df[-train_indices_log,]

train_data_log$label <- ifelse(train_data_log$label == "TREG", 1, 0)
test_data_log$label <- ifelse(test_data_log$label == "TREG", 1, 0)

# Train a logistic regression model
logistic_model <- glm(label ~ ., data = train_data_log, family = binomial)

# Create training (80%) and testing (20%) sets
sample_index <- createDataPartition(RNAdata$label, p = 0.8, list = FALSE)
train_data1 <- RNAdata[sample_index, ]
test_data1 <- RNAdata[-sample_index, ]
```

```{r log regression with ALL predictors}

#not loading, error says: no convergence, some predictors take extreme values =0 or =1
train_data_log <- train_data[, !(names(test_data) %in% c("label_as_factor", "label"))]

glm_start_time <- Sys.time()
glm_fit_log <- glm(label_as_numeric ~ ., data= train_data_log, family = binomial)
glm_end_time <- Sys.time()
glm_time_to_run <- glm_end_time - glm_start_time
cat("Logistic Model training time:", glm_time_to_run, "\n")
summary(glm_fit_log)
#for 70% training data - Warning: glm.fit: algorithm did not converge
#for 80% training data - Warning: glm.fit: algorithm did not converge
glm_probs <- predict(glm_fit_log, newdata = test_data, type = "response")
#Warning: prediction from rank-deficient fit; attr(*, "non-estim") has doubtful cases
glm_pred <- ifelse(glm_probs > 0.5, "TREG", "CD4+T")
```

Logistic Regression Evaluation

```{r Log Reg: Confusion Matrix}
confusion_log <- table(prediction = glm_pred, truth = test_data$label_as_factor)
confusion_log
# cat("The Confusion Matrix of the Logistic Regression:", confusion_log, "\n")
```

```{r Log Reg: Accuracy}
# # TP_lda <- confusion_matrix_lda["TREG", "TREG"]  # True Positives
# # FP_lda <- confusion_matrix_lda["CD4+T", "TREG"]  # False Positives
# # TN_lda <- confusion_matrix_lda["CD4+T", "CD4+T"]  # True Negatives
# # FN_lda <- confusion_matrix_lda["TREG", "CD4+T"]  # False Negatives

log_accuracy <- mean(glm_pred == test_data$label)
cat("Accuracy of Logistic Regression:", log_accuracy, "\n")
```

```{r Log Reg: Balanced Accuracy}
# balanced_accuracy_lda <- 1/2*(TP_lda/(TP_lda+FN_lda))+1/2*(TN_lda/(TN_lda+FN_lda))
# cat("Balenced Accuracy of LDA:", balanced_accuracy_lda, "\n")
# 
# # sum(test_data$label == "CD4+T"); sum(test_data$label == "TREG") # checking the amount of both classes present in the test_data set
```

```{r Log Reg: F1 Score}
# lda_f1_score <- F1_Score(y_true= test_data$label, y_pred = lda_pred)
# cat("F1 Score of LDA:", lda_f1_score, "\n")
```

```{r log regression ROC and AUC}
roc_curve_log <- roc(test_data$label_as_factor, glm_probs, levels=c("TREG", "CD4+T"), direction = ">")
plot(roc_curve_log, col="blue", main="ROC Curve for Logistic Regression")

log_auc <- auc(roc_curve_log)
cat("The AUC of the ROC of Logistic Regression:", log_auc, "\n")
```

```{r log regression Missclassification error rate}
misclass_log <- mean(glm_pred != test_data$label_as_factor)
cat("The Misclassification rate of Logistic Regression:", misclass_log, "\n")
```

#### Quadratic Discriminant Analysis (QDA) (Chi)

```{r qda_Train}
## CAUTION: currently hard-coded numbers here. To be changed (Chi)
options(expressions = 10000)
qda_covariates = mean_table$genes[1:1100]
## CAUTION: currently hard-coded numbers here. To be changed (Chi)

qda_formula = as.formula(
  paste(
    "label_as_factor ~",
    paste(qda_covariates, collapse = " + ")
    ))

qda_fit <- qda(qda_formula, data = train_data)
```

```{r qda_Confusion}
qda_pred <- predict(qda_fit, test_data)$class

table(predicted = qda_pred, observed = test_data$label)
```

```{r qda_Misclass}
qda_misclass_rate = mean(qda_pred != test_data$label)

cat("Misclassification rate:", qda_misclass_rate*100, "%")
```

```{r qda_ROC}
qda_pred_numeric <- ifelse(qda_pred == "TREG", 1, 0)
  
qda_roc_curve <- roc(
  response = test_data$label_as_numeric,
  predictor = qda_pred_numeric, 
  levels = c(0,1),
  direction = "<"
  )

plot(qda_roc_curve, col = "red", main = "ROC Curve for QDA")

qda_auc <- auc(qda_roc_curve)
qda_auc
```

#### Nearest Neighbor Classifier (k-NN) (Chi)

```{r kNN_train, eval = FALSE}
knn_train_X <- train_data %>% 
  dplyr::select(-label, -label_as_factor, -label_as_numeric)

knn_train_Y <- train_data$label_as_factor

knn_test_X <- test_data %>% 
  dplyr::select(-label, -label_as_factor, -label_as_numeric)

knn_start_time <- Sys.time()
knn_pred <- sapply(kk_vector,
                   function(k)
                     {knn(train = knn_train_X, test = knn_test_X, cl = knn_train_Y, k = k)}
                   )
knn_end_time <- Sys.time()

knn_train_runtime <- knn_end_time - knn_start_time
print(knn_train_runtime) #50 mins for k=1 to k=10 

rm(knn_train_X, knn_train_Y, knn_test_X)
```

```{r kNN_save_load, include = FALSE}
write.csv(knn_pred, file = "knn_predictions.csv", row.names = TRUE)

knn_pred <- read.csv("knn_predictions.csv") %>% 
  dplyr::select(-X)

## CAUTION: currently hard-coded numbers here. To be changed (Chi)
colnames(knn_pred) <- paste0("k=", 1:10)
## CAUTION: currently hard-coded numbers here. To be changed (Chi)
```

```{r kNN_Confusion}
for (i in kk_vector){
  print(paste("k =", i))
  print(table(predicted = knn_pred[,i], observed = test_data$label_as_factor))
}

knn_test_Y <- 
```

```{r kNN_Misclass}
knn_misclass = c(rep(0,length(kk_vector)))

for (i in kk_vector){
  knn_misclass[i] = mean(knn_pred[,i] != test_data$label_as_factor)
}
plot(knn_misclass, type = "l")
```

```{r kNN_ROC}
knn_pred_numeric <- ifelse(knn_pred == "TREG", 1, 0)

knn_roc_curves <- data.frame()

knn_auc <- data.frame()

for (kk in 1:ncol(knn_pred)) {
  roc_curve <- roc(
    response = test_data$label_as_numeric, 
    predictor = knn_pred_numeric[,kk],
    levels = c(0,1),
    direction = "<"
    )
  
  roc_curve_data <- data.frame(
    k = kk,
    FPR = 1 - roc_curve$specificities,
    TPR = roc_curve$sensitivities
    )
  
  auc_data <- data.frame(
    k = kk,
    AUC = roc_curve$auc
    )
  
  knn_roc_curves <- rbind(knn_roc_curves, roc_curve_data)
  
  knn_auc <- rbind(knn_auc, auc_data)
}

knn_roc_curves$k <- as.factor(knn_roc_curves$k)

ggplot(knn_roc_curves, aes(x = FPR, y = TPR, colour = k)) + 
  geom_line(linewidth = 1) +
  labs(
    title = "ROC Curves for k-NN (k = 1 to 10)",
    x = "False Positive Rate (FPR)",
    y = "True Positive Rate (TPR)",
    color = "k"
  )

ggplot(knn_auc, aes(x = k, y = AUC)) + 
  geom_line(linewidth = 1) +
  labs(
    title = "ROC AUCs for k-NN (k = 1 to 10)",
    x = "k",
    y = "ROC AUC"
  )
```

#### Gradient Boosting Decision Trees (GBDT) (Jonathan)

```{r gbmparams}
#Number of trees
num_trees = 1000
# Define grid of lambda (shrinkage) values to evaluate
lambda_grid <- c(0.001, 0.01,0.03,0.05)

# Initialize a vector to store test errors for each lambda
test_errors <- numeric(length(lambda_grid))
```

```{r dataprepgbm}
test_data_cleaned_gbm <- test_data[, !colnames(test_data) %in% c("label_as_factor", "label")]
```

```{r gbmloop, eval=FALSE}
gbm_optimal_lambda_start <- Sys.time()
# Loop over each lambda value
for (i in seq_along(lambda_grid)) {
  lambda <- lambda_grid[i]
  
  # Train the gbm model with the current lambda (shrinkage) value
  gbm_model <- gbm(label_as_numeric ~. -label - label_as_factor, data = train_data, 
                   distribution = "bernoulli", 
                   n.trees = num_trees, 
                   interaction.depth = 4, 
                   shrinkage = lambda, 
                   cv.folds = 5, 
                   verbose = FALSE)
  
  # Make predictions on the test set using the optimal number of trees
  predictions <- predict(gbm_model, newdata = test_data_cleaned_gbm, n.trees = num_trees)
  
  # Calculate the Mean Squared Error on the test set
  test_errors[i] <- mean((predictions - test_data_cleaned_gbm$label_as_numeric)^2)
}
gbm_optimal_lambda_end <- Sys.time()
gbm_optimal_lambda_total_time <- gbm_optimal_lambda_end - gbm_optimal_lambda_start
print(gbm_optimal_lambda_total_time)
```

```{r gbmtse, eval=FALSE}
# Combine lambda values and test errors into a data frame for plotting
error_df <- data.frame(lambda = lambda_grid, test_error = test_errors)

# Plot the test error over the range of lambda values
ggplot(error_df, aes(x = lambda, y = test_error)) +
  geom_line() +
  geom_point() +
  labs(title = "Test Error vs Lambda (Shrinkage)", 
       x = "Lambda (Shrinkage)", 
       y = "Test Mean Squared Error") +
  theme_minimal()

```

The results above show use that the GLM model with the lowest Test Mean Squared Error, will be the one with the lowest lambda value we assigned of 0.001. This does not necessarily mean that the best Gradient Descent Boosting model that will work best will be the best model for our data, as it may not account for over fitting. However, we will continue using the GBDT model with a shrinkage value of 0.001 to analyze the model and to compare it to our other models.

```{r gbm_model}
# Set a single shrinkage value
lambda <- 0.001

# Measure the time it takes to run 
gbm_start_time <- Sys.time()
# Train the gbm model
smallest_lambda_gbm_model <- gbm(label_as_numeric ~. -label - label_as_factor, 
                 data = train_data, 
                 distribution = "bernoulli", 
                 n.trees = num_trees, 
                 interaction.depth = 4, 
                 shrinkage = lambda, 
                 cv.folds = 5, 
                 verbose = FALSE)

# Get the best number of trees based on cross-validation
best_trees <- gbm.perf(smallest_lambda_gbm_model, method = "cv")

# Measure time after model training
gbm_end_time <- Sys.time()
gbm_model_training_time <- gbm_end_time - gbm_start_time
cat("GBM Model training time:", gbm_model_training_time, "\n")

# Measure time for predictions
gbm_start_time_pred <- Sys.time()
# Make predictions on the test set using the best number of trees
gbm_pred_probs <- predict(smallest_lambda_gbm_model, newdata = test_data_cleaned_gbm, n.trees = best_trees, type = "response")
gbm_pred_classes <- ifelse(gbm_pred_probs > 0.5, 1, 0)

# Measure time after predictions
gbm_end_time_pred <- Sys.time()
gbm_prediction_time <- gbm_end_time_pred - gbm_start_time_pred
cat("Prediction time:", gbm_prediction_time, "\n")
```

```{r gbm_Confusion}
gbm_confusion_matrix <- table(Predicted = gbm_pred_classes, Actual = test_data_cleaned_gbm$label_as_numeric)
print(gbm_confusion_matrix)

gbm_balanced_accuracy <- ((1/2) *(gbm_confusion_matrix[2,2]/(gbm_confusion_matrix[2,2]+gbm_confusion_matrix[1,2]))+ (1/2 *(gbm_confusion_matrix[1,1]/(gbm_confusion_matrix[1,1] + gbm_confusion_matrix[2,1]))))

cat("The balanced accuracy is", gbm_balanced_accuracy, "\n")
```

```{r gbm_Misclass}
gbm_misclassification_rate <- mean(gbm_pred_classes != test_data_cleaned_gbm$label_as_numeric)
cat("Misclassification Rate:", gbm_misclassification_rate, "\n")
```

```{r gbm_Accuracy}
gbm_accuracy <- mean(gbm_pred_classes == test_data_cleaned_gbm$label_as_numeric)
cat("Accuracy:", gbm_accuracy, "\n")
```

```{r gbm_ROC_AUC}
gbm_roc_curve <- roc(test_data_cleaned_gbm$label_as_numeric, gbm_pred_probs)
cat("AUC:", auc(gbm_roc_curve), "\n")
plot(gbm_roc_curve, main = "ROC Curve")
```

```{r gbm_f1_score}
gbm_f1_score <- F1_Score(y_pred = gbm_pred_classes, y_true = test_data_cleaned_gbm$label_as_numeric)
cat("F1 Score:", gbm_f1_score, "\n")
```

The accuracy for the gradient descent decision tree model (where the shrinkage term was equal to 0.001) was 0.904. The AUC was equal to 0.972 and the Test mean squared error was equal to 0.109. Misclassification rate was equal to 0.096. And F1 score was equal to 0.928

#### Random Forest (Jonathan)

```{r tree_init}
## Clean the test data for the Random Forest Classifier task
test_data_cleaned_for_rf <- test_data[, !colnames(test_data) %in% c("label_as_numeric", "label")]

## Running Tree on all the variables on train data
tree.rna_features <- tree(label_as_factor ~ . -label_as_numeric -label, split="deviance", data = train_data)
summary(tree.rna_features)
plot(tree.rna_features)
text(tree.rna_features, pretty = 0, cex = 1)

## Predict on Test variables
tree.pred <-predict(tree.rna_features, test_data, type="class")

## Mis-classification Rate and Confusion Matrix
table(tree.pred,test_data_cleaned_for_rf$label_as_factor)
mean(tree.pred!=test_data_cleaned_for_rf$label_as_factor)

```

```{r treecv}
cv.rna_data <- cv.tree(tree.rna_features)
plot(cv.rna_data$size, cv.rna_data$dev, type ='b')
```

Above we use a tree search to find.... When using the tree search, we get that there are 11 features that are the most important to classifying whether a cell is a T Regulatory Cell or a CD4-positive cell. However, we can see based on the tree that there are certain features that appear more than once. Based on the tree plot, we can determine that the most important regressor that helps determine if a cell is a Regulatory T cell or a CD4-positive cell is the IL7R. From the National Library of medicine ("<https://www.ncbi.nlm.nih.gov/gene/3575>"), IL7R is a protein encoded by a gene that plays a critical role in the development of lymph nodes. Based on the analysis above, we decided to create a Random Forest regression that will have 12 terminal nodes.

```{r rfmodel}
# RF model
start_time_rf <- Sys.time()

rf.rna_features <-randomForest(label_as_factor~. - label_as_numeric -label, data=train_data, mtry=64, importance=TRUE, n.tree = 5000) # RF Model

end_time_rf <- Sys.time()
rf_training_time <- end_time_rf - start_time_rf
cat("Random Forest model training time:", rf_training_time, "\n")

# Predictions
start_time_rf_pred <- Sys.time()
yhat.rf <-predict(rf.rna_features, newdata=test_data)

end_time_rf_pred <- Sys.time()
rf_prediction_time <- end_time_rf_pred - start_time_rf_pred
cat("Random Forest prediction time:", rf_prediction_time, "\n")

```

```{r rf_Confusion}
rf_confusion_matrix <- confusion_matrix <- table(Predicted = yhat.rf, Actual = test_data_cleaned_for_rf$label_as_factor)
print(rf_confusion_matrix)
rf_balanced_accuracy <- ((1/2) *(rf_confusion_matrix[2,2]/(rf_confusion_matrix[2,2]+rf_confusion_matrix[1,2]))+ (1/2 *(rf_confusion_matrix[1,1]/(rf_confusion_matrix[1,1] + rf_confusion_matrix[2,1]))))

cat("The balanced accuracy is", rf_balanced_accuracy, "\n")
```

```{r rf_Misclass}
rf_misclassification_rate <- mean(yhat.rf != test_data_cleaned_for_rf$label_as_factor)
cat("Random Forest Misclassification Rate:", rf_misclassification_rate, "\n")
```

```{r rf_Accuracy}
accuracy_rf <- mean(yhat.rf == test_data_cleaned_for_rf$label_as_factor)
cat("Random Forest Accuracy:", accuracy_rf, "\n")
```

```{r rf_ROC_AUC}
roc_rf <- roc(test_data_cleaned_for_rf$label_as_factor, as.numeric(yhat.rf))
cat("Random Forest AUC:", auc(roc_rf), "\n")
plot(roc_rf, main = "Random Forest ROC Curve")
```

```{r rf_f1_score}
rf_f1_score <- F1_Score(y_pred = yhat.rf, y_true = test_data_cleaned_for_rf$label_as_factor)
cat("F1 Score:", rf_f1_score, "\n")
```

Using a random forest tree model, we see that the accuracy of the model was 0.936 Which is quite accurate! Misclassification Rate was equal to 0.064. The F1 score was equal to 0.9504

#### Support Vector Machine (SVM) (Peter)

```{r}

```

### With PCA

#### Linear Discriminant Analysis (LDA) (Imar)

```{r}

```

#### Logistic classifier (Imar)

```{r}

```

#### Quadratic Discriminant Analysis (QDA) (Chi)

```{r}

```

#### Nearest Neighbor Classifier (k-NN) (Chi)

```{r}

```

#### Gradient Boosting Decision Trees (GBDT) (Jonathan)

```{r Gbm_pca_standartization}
# Standardize the data (important for PCA)
gbm_train_data_scaled <- scale(train_data[, -which(names(train_data) %in% c("label", "label_as_factor"))])
gbm_test_data_scaled <- scale(test_data_cleaned_gbm[,])

gbm_pca_start <- Sys.time()
# Perform PCA
gbm_pca_model <- prcomp(gbm_train_data_scaled, center = TRUE, scale. = TRUE)
gbm_pca_end <- Sys.time()
gbm_pcaa_total_time <- gbm_pca_end - gbm_pca_start

# Get the first 10 principal components
gbm_train_data_pca <- as.data.frame(gbm_pca_model$x[, 1:10])
gbm_train_data_pca$label_as_numeric <- train_data$label_as_numeric  # Add the target variable

# Transform the test data using the PCA model
gbm_test_data_pca <- as.data.frame(predict(gbm_pca_model, newdata = gbm_test_data_scaled)[, 1:10])
gbm_test_data_pca$label_as_numeric <- test_data_cleaned_gbm$label_as_numeric  # Add the target variable
```

#### Random Forest (Jonathan)

```{r}

```

#### Support Vector Machine (SVM) (Peter)

```{r}

```

## T1.3: Improving F1 Score (???)

Train and evaluating three classifiers of our choice with the goal of improving the F1 score (can use methods like bagging, boosting, and regularization).

```{r}

```

## T1.4: Implement predictor (???)

Choose the best approach from those above tested and implement our predictor as a function of our code.

```{r}

```
