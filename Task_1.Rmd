---
title: "Task 1 RmD"
output: html_document
date: "2024-11-12"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn=-1)
```

```{r parameters}
seed = 443
csv_file_one = "data1.csv"
training_size = 0.7 # Proportion of dataset that is to be split for training
packages = list("caret", 
                "randomForest",
                "tree",
                "dplyr",
                "tidyr",
                "caret", 
                "gbm", 
                "MASS",
                "ggplot2",
                "pROC"
)

```

```{r, include=FALSE}
for (package in packages){
  if(!require(package, character.only = TRUE)){
    install.packages(package, character.only = TRUE)
  } else {
    library(package, character.only = TRUE)
  }
}
```

# Task 1: binary classification

```{r train test split}
set.seed(seed)

task_one_df <- read.csv(csv_file_one)

# str(task_one_df)
task_one_df$label_as_factor <- as.factor(task_one_df$label)
task_one_df$label_as_numeric <- as.numeric(task_one_df$label_as_factor) -1 
# Split Data into Test and Train (We should all use the same data)

train_split = as.integer(nrow(task_one_df)*training_size)

train_indices <- sample(1:nrow(task_one_df), train_split)
train_data <- task_one_df[train_indices,]
test_data <- task_one_df[-train_indices,]



```

## T1.1: exploratory data analysis and summary statistics (Peter?)

```{r}
nrow(task_one_df) # The data has 5471 observations
ncol(task_one_df) # The data contains 4125 variables-- 4124 originally, 1 added (label_as_factor)
table(task_one_df$label) # 3356 observations are labelled as CD4+T and 2115 are labelled as TREG

mean_table <- task_one_df %>%

  dplyr::select(-label, -label_as_numeric) %>%

  group_by(label_as_factor) %>%

  summarise(across(everything(), mean, na.rm = TRUE)) %>%

  pivot_longer(cols = -label_as_factor, names_to = 'genes', values_to = 'mean_value') %>%

  pivot_wider(names_from = label_as_factor, values_from = mean_value) %>%

  mutate(diff = abs(`CD4+T` - TREG)) %>%

  mutate(pct_diff = 100 * (diff / ((`CD4+T` + TREG)/2) )) %>%

  arrange(desc(diff))

## There are decent differences between the ranking of absolute difference in means of groups
## and the ranking of percentage difference in means of groups
## perhaps we create box plots of the top 5 of each metric??

### Next steps --> 
  ### (1) instead of absolute value difference, include also
  ### percentage change. then plot box plots for the top 5? 10? variables
  ### that appear to have the clearest distinction between the two labels
  ### these specific variables provide a good baseline for what we expect
  ### our subsequent models to choose for the most important fields
  
## Checking if there are any rows that have missing values... Keep this hashed unless you want to check, because it takes a while to run
# for (i in 1:nrow(task_one_df)) {
#   if (any(is.na(task_one_df[i, ]))) {
#     print(paste("Row", i, "contains NA values."))
#   }
# }

twenty_random_columns <- sample(names(task_one_df)[-1], 20)

## Create boxplots for the random sample of columns
for (column in twenty_random_columns) {
    p <- ggplot(task_one_df, aes_string(x = "label", y = column)) +
        geom_jitter(width = 0.2, height = 0) +  # Use jitter to avoid overplotting
        ggtitle(paste("Scatter Plot of", column, "by Label")) +
        theme_minimal()
    print(p)
}

```

## T1.2: training and evaluating on various classifiers

### Without PCA

#### Linear Discriminant Analysis (LDA) (Imar)

```{r}

```

#### Logistic classifier (Imar)

```{r}

```

#### Quadratic Discriminant Analysis (QDA) (Chi)

```{r qdaTrain}
options(expressions = 10000)
covariates = mean_table$genes[1:1100]

formula = as.formula(
  paste(
    "label_as_factor ~",
    paste(covariates, collapse = " + ")
    ))

qda_fit <- qda(formula, data = train_data)
```

```{r qdaConfusion}
qda_pred <- predict(qda_fit, test_data)$class

table(predicted = qda_pred, observed = test_data$label)
```

```{r qdaMisclass}
qda_misclass_rate = mean(qda_pred != test_data$label)

cat("Misclassification rate:", qda_misclass_rate*100, "%")
```

```{r qdaROC}
qda_pred_numeric <- ifelse(qda_pred == "TREG", 1, 0)
  
roc_curve_qda <- roc(
  response = test_data$label_as_numeric,
  predictor = qda_pred_numeric, 
  levels = c(0,1),
  direction = "<"
  )

plot(roc_curve_qda, col = "red", main = "ROC Curve for QDA")

auc(roc_curve_qda)
```


#### Nearest Neighbor Classifier (k-NN) (Chi)

```{r}

```

#### Gradient Boosting Decision Trees (GBDT) (Jonathan)
```{r}
#Number of trees
num_trees = 1000
# Define grid of lambda (shrinkage) values to evaluate
lambda_grid <- c(0.001, 0.01,0.03,0.05)

# Initialize a vector to store test errors for each lambda
test_errors <- numeric(length(lambda_grid))
```


```{r}

test_data_cleaned <- test_data[, !colnames(test_data) %in% c("label_as_factor", "label")]
# Loop over each lambda value
for (i in seq_along(lambda_grid)) {
  lambda <- lambda_grid[i]
  
  # Train the gbm model with the current lambda (shrinkage) value
  gbm_model <- gbm(label_as_numeric ~. -label - label_as_factor, data = train_data, 
                   distribution = "bernoulli", 
                   n.trees = num_trees, 
                   interaction.depth = 4, 
                   shrinkage = lambda, 
                   cv.folds = 5, 
                   verbose = FALSE)
  
  # Make predictions on the test set using the optimal number of trees
  predictions <- predict(gbm_model, newdata = test_data_cleaned, n.trees = num_trees)
  
  # Calculate the Mean Squared Error on the test set
  test_errors[i] <- mean((predictions - test_data_cleaned$label_as_numeric)^2)
}
```

```{r}
# Combine lambda values and test errors into a data frame for plotting
error_df <- data.frame(lambda = lambda_grid, test_error = test_errors)

# Plot the test error over the range of lambda values
ggplot(error_df, aes(x = lambda, y = test_error)) +
  geom_line() +
  geom_point() +
  labs(title = "Test Error vs Lambda (Shrinkage)", 
       x = "Lambda (Shrinkage)", 
       y = "Test Mean Squared Error") +
  theme_minimal()

```
The results above show use that the GLM model with the lowest Test Mean Squared Error, will be the one with the lowest lambda value we assigned of 0.001. This does not necessarily mean that the best Gradient Descent Boosting model that will work best will be the best model for our data, as it may not account for over fitting. However, we will continue using the GBDT model with a shrinkage value of 0.001 to analyze the model and to compare it to our other models. 

```{r}
# Set a single shrinkage value
lambda <- 0.001

# Train the gbm model
smallest_lambda_gbm_model <- gbm(label_as_numeric ~. -label - label_as_factor, 
                 data = train_data, 
                 distribution = "bernoulli", 
                 n.trees = num_trees, 
                 interaction.depth = 4, 
                 shrinkage = lambda, 
                 cv.folds = 5, 
                 verbose = FALSE)

# Get the best number of trees based on cross-validation
best_trees <- gbm.perf(smallest_lambda_gbm_model, method = "cv")

# Make predictions on the test set using the best number of trees
pred_probs <- predict(smallest_lambda_gbm_model, newdata = test_data_cleaned, n.trees = best_trees, type = "response")
pred_classes <- ifelse(pred_probs > 0.5, 1, 0)

# Evaluate performance
test_mse <- mean((pred_probs - test_data_cleaned$label_as_numeric)^2)
accuracy <- mean(pred_classes == test_data_cleaned$label_as_numeric)
auc <- auc(test_data_cleaned$label_as_numeric, pred_probs)
# f1_score <- MLmetrics::F1_Score(predicted = pred_classes, reference = test_data_cleaned$label_as_numeric)

# Print results
cat("Test MSE:", test_mse, "\n")
cat("Accuracy:", accuracy, "\n")
cat("AUC:", auc, "\n")
# cat("F1 Score:", f1_score, "\n")

```
The accuracy for the gradient descent decision tree model (where the shrinkage term was equal to 0.001) was 0.904. The Area under the curve was equal to 0.972 and the Test mean squared error was equal to 0.109. 


#### Random Forest (Jonathan)
```{r}
## Clean the test data for the Random Forest Classifier task
test_data_cleaned_for_rf <- test_data[, !colnames(test_data) %in% c("label_as_numeric", "label")]

## Running Tree on all the variables on train data
tree.rna_features <- tree(label_as_factor ~ . -label_as_numeric -label, split="deviance", data = train_data)
summary(tree.rna_features)
plot(tree.rna_features)
text(tree.rna_features, pretty = 0, cex = 1)

## Predict on Test variables
tree.pred <-predict(tree.rna_features, test_data, type="class")

## Mis-classification Rate and Confusion Matrix
table(tree.pred,test_data_cleaned_for_rf$label_as_factor)
mean(tree.pred!=test_data_cleaned_for_rf$label_as_factor)

```

```{r}
cv.rna_data <- cv.tree(tree.rna_features)
plot(cv.rna_data$size, cv.rna_data$dev, type ='b')
```

Above we use a  tree search to find....  When using the GBDT, we get that there are 11 features that are the most important to classifying whether a cell is a T Regulatory Cell or a CD4-positive cell. However, we can see based on the tree that there are certain features that appear more than once. Based on the tree plot, we can determine that the most important regressor that helps determine if a cell is a Regulatory T cell or a CD4-positive cell is the IL7R. From the National Library of medicine ("https://www.ncbi.nlm.nih.gov/gene/3575"), IL7R is a protein encoded by a gene that plays a critical role in the development of lymph nodes. Based on the analysis above, we decided to create a Random Forest regression that will have 14 terminal nodes. 

```{r}
# Define the columns to exclude
exclude_columns <- c("label_as_numeric", "label")

# Get the feature columns programmatically
feature_columns <- setdiff(colnames(train_data), exclude_columns)

rna_features.test <- test_data_cleaned_for_rf$label_as_factor
rf.rna_features <-randomForest(label_as_factor~. - label_as_numeric -label, data=train_data, mtry=14, importance=TRUE, n.tree = 5000)
yhat.rf <-predict(rf.rna_features, newdata=test_data)

# Calculate precision, recall, F1-score (using `caret` package)
library(caret)
performance_metrics <- confusionMatrix(yhat.rf,test_data_cleaned_for_rf$label_as_factor)
print(performance_metrics)

```


Using a random forest tree model, we see that the accuracy of the model was 0.0.891 percent. Which is quite accurate! 

#### Support Vector Machine (SVM) (Peter)

```{r}

```

### With PCA

#### Linear Discriminant Analysis (LDA) (Imar)

```{r}

```

#### Logistic classifier (Imar)

```{r}

```

#### Quadratic Discriminant Analysis (QDA) (Chi)

```{r}

```

#### Nearest Neighbor Classifier (k-NN) (Chi)

```{r}

```

#### Gradient Boosting Decision Trees (GBDT) (Jonathan)

```{r}

```

#### Random Forest (Jonathan)

```{r}

```

#### Support Vector Machine (SVM) (Peter)

```{r}

```

## T1.3: Improving F1 Score (???)

Train and evaluating three classifiers of our choice with the goal of improving
the F1 score (can use methods like bagging, boosting, and regularization).

```{r}

```


## T1.4: Implement predictor (???)

Choose the best approach from those above tested and implement our predictor
as a function of our code.

```{r}

```


