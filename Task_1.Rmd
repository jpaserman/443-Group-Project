---
title: "Task 1 RmD"
output:
  pdf_document: default
  html_document: default
date: "2024-11-12"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn=-1)
```

```{r parameters}
seed = 443
csv_file_one = "data1.csv"
training_size = 0.8 # Proportion of dataset that is to be split for training
packages = list("class", 
                "randomForest",
                "tree",
                "dplyr",
                "tidyr",
                "gbm", 
                "MASS",
                "ggplot2",
                "pROC",
                "MLmetrics",
                "caret"
)

# For knn-training
kk_vector = c(2:7) 
```

```{r, include=FALSE}
for (package in packages){
  if(!require(package, character.only = TRUE)){
    install.packages(package, character.only = TRUE)
  } else {
    library(package, character.only = TRUE)
  }
}
```

# Task 1: binary classification

```{r train_test_split}
set.seed(seed)

task_one_df <- read.csv(csv_file_one)

# str(task_one_df)
task_one_df$label_as_factor <- as.factor(task_one_df$label)
task_one_df$label_as_numeric <- as.numeric(task_one_df$label_as_factor) -1 #Labels cells that are T-Reg cells as 1 and CD4 cells as 0
# Split Data into Test and Train (We should all use the same data)

train_split = as.integer(nrow(task_one_df)*training_size)

train_indices <- sample(1:nrow(task_one_df), train_split)
train_data <- task_one_df[train_indices,]
test_data <- task_one_df[-train_indices,]
```

## T1.1: exploratory data analysis and summary statistics (Peter?)

```{r EDA}
nrow(task_one_df) # The data has 5471 observations
ncol(task_one_df) # The data contains 4125 variables-- 4124 originally, 1 added (label_as_factor)
table(task_one_df$label) # 3356 observations are labelled as CD4+T and 2115 are labelled as TREG

mean_table <- task_one_df %>%

  dplyr::select(-label, -label_as_numeric) %>%

  group_by(label_as_factor) %>%

  summarise(across(everything(), mean, na.rm = TRUE)) %>%

  pivot_longer(cols = -label_as_factor, names_to = 'genes', values_to = 'mean_value') %>%

  pivot_wider(names_from = label_as_factor, values_from = mean_value) %>%

  mutate(diff = abs(`CD4+T` - TREG)) %>%

  mutate(pct_diff = 100 * (diff / ((`CD4+T` + TREG)/2) )) %>%

  arrange(desc(diff))

## There are decent differences between the ranking of absolute difference in means of groups
## and the ranking of percentage difference in means of groups
## perhaps we create box plots of the top 5 of each metric??

### Next steps --> 
  ### (1) instead of absolute value difference, include also
  ### percentage change. then plot box plots for the top 5? 10? variables
  ### that appear to have the clearest distinction between the two labels
  ### these specific variables provide a good baseline for what we expect
  ### our subsequent models to choose for the most important fields
  
## Checking if there are any rows that have missing values... Keep this hashed unless you want to check, because it takes a while to run
# for (i in 1:nrow(task_one_df)) {
#   if (any(is.na(task_one_df[i, ]))) {
#     print(paste("Row", i, "contains NA values."))
#   }
# }

twenty_random_columns <- sample(names(task_one_df)[-1], 20)

## Create boxplots for the random sample of columns
for (column in twenty_random_columns) {
    p <- ggplot(task_one_df, aes_string(x = "label", y = column)) +
        geom_jitter(width = 0.2, height = 0) +  # Use jitter to avoid overplotting
        ggtitle(paste("Scatter Plot of", column, "by Label")) +
        theme_minimal()
    print(p)
}

```

## T1.2: training and evaluating on various classifiers

### Without PCA

#### Linear Discriminant Analysis (LDA) (Imar)

```{r lda_model}

lda_fit <- lda(label_as_factor ~ . -label -label_as_numeric, data=train_data)

lda_pred_posterior <- predict(lda_fit, test_data)
lda_pred <- predict(lda_fit, test_data)$class
head(predict(lda_fit, test_data)$posterior)
```

LDA Evaluation

```{r lda_confusion}
lda_confusion <- table(lda_pred, truth = test_data$label_as_factor)
lda_confusion
```

```{r lda_accuracy}
lda_accuracy <- mean(lda_pred == test_data$label)
cat("Accuracy of LDA:", lda_accuracy, "\n")
```

```{r lda_balanced_accuracy}
lda_balanced_accuracy <- ((1/2) *(lda_confusion[2,2]/(lda_confusion[2,2]+lda_confusion[1,2]))+ (1/2 *(lda_confusion[1,1]/(lda_confusion[1,1] + lda_confusion[2,1]))))
cat("Balanced Accuracy of LDA:", lda_balanced_accuracy, "\n")
```

```{r lda_f1_score}
lda_f1_score <- F1_Score(y_true= test_data$label, y_pred = lda_pred)
cat("F1 Score of LDA:", lda_f1_score, "\n")
```

```{r lda_ROC_AUC}
roc_curve_lda <- roc(
  test_data$label_as_factor, predict(lda_fit, test_data)$posterior[,2], 
  levels= c("TREG", "CD4+T"), direction =">")
plot(roc_curve_lda, col="blue", main = "ROC Curve for LDA")

lda_auc <- auc(roc_curve_lda)
cat("The AUC of the ROC of LDA:", lda_auc, "\n")
```

```{r lda_Misclass}
misclas_lda <- mean(lda_pred != test_data$label_as_factor)
cat("Misclassification error rate of LDA:", misclas_lda, "\n")
```

#### Logistic classifier (Imar)

```{r logreg_Model}
set.seed(440)

# Calculate the number of rows to sample (75% of the dataset)
sample_size <- floor(0.75 * nrow(task_one_df))

# Randomly sample the indices for the 75% of the data
sample_indices <- sample(1:nrow(task_one_df), sample_size)

# Create the resampled dataset using the sampled indices
resampled_data <- task_one_df[sample_indices, ]

# View the resampled data
head(resampled_data)

log_train_split = as.integer(nrow(task_one_df)*0.50)
log_train_indices <- sample(1:nrow(task_one_df), log_train_split)
log_train_data <- task_one_df[log_train_indices,]
log_test_data <- task_one_df[-log_train_indices,]

head(log_train_indices)

dim(log_train_data)
log_train_data <- resampled_data[, !(names(resampled_data) %in% c("label_as_factor", "label"))]
dim(log_train_data)

head(log_train_data)
dim(train_data)
head(train_data)

glm_start_time <- Sys.time()
glm_fit_log <- glm(label_as_numeric ~ ., data= log_train_data, family = binomial)
glm_end_time <- Sys.time()
glm_time_to_run <- glm_end_time - glm_start_time
cat("Logistic Model training time:", glm_time_to_run, "\n")
summary(glm_fit_log)
#for 80% training data - Warning: glm.fit: algorithm did not converge
glm_probs <- predict(glm_fit_log, newdata = log_test_data, type = "response")
#Warning: prediction from rank-deficient fit; attr(*, "non-estim") has doubtful cases
glm_pred <- ifelse(glm_probs > 0.5, "TREG", "CD4+T")
```

Logistic Regression Evaluation

```{r logreg_confusion}
log_confusion <- table(prediction = glm_pred, truth = test_data$label_as_factor)
log_confusion
```

```{r logreg_accuracy}
log_accuracy <- mean(glm_pred == test_data$label)
cat("Accuracy of Logistic Regression:", log_accuracy, "\n")
```

```{r logreg_balanced_accuracy}
log_balanced_accuracy <- ((1/2) *(log_confusion[2,2]/(log_confusion[2,2]+log_confusion[1,2]))+ (1/2 *(log_confusion[1,1]/(log_confusion[1,1] + log_confusion[2,1]))))
cat("Balanced Accuracy of Logistic Regression:", log_balanced_accuracy, "\n")
```

```{r logreg_f1_score}
log_f1_score <- F1_Score(y_true= test_data$label, y_pred = glm_pred)
cat("F1 Score for Logistic Regression:", log_f1_score, "\n")
```

```{r logreg_ROC_AUC}
roc_curve_log <- roc(test_data$label_as_factor, glm_probs, levels=c("TREG", "CD4+T"), direction = ">")
plot(roc_curve_log, col="blue", main="ROC Curve for Logistic Regression")

log_auc <- auc(roc_curve_log)
cat("The AUC of the ROC of Logistic Regression:", log_auc, "\n")
```

```{r logreg_Misclass}
misclass_log <- mean(glm_pred != test_data$label_as_factor)
cat("The Misclassification rate of Logistic Regression:", misclass_log, "\n")
```

#### Quadratic Discriminant Analysis (QDA) (Chi)

```{r qda_Train}
## CAUTION: currently hard-coded numbers here. To be changed (Chi)
options(expressions = 10000)
qda_covariates = mean_table$genes[1:1100]
## CAUTION: currently hard-coded numbers here. To be changed (Chi)

qda_formula = as.formula(
  paste(
    "label_as_factor ~",
    paste(qda_covariates, collapse = " + ")
    ))

qda_fit <- qda(qda_formula, data = train_data)
```

```{r qda_Confusion}
qda_pred <- predict(qda_fit, test_data)$class

table(predicted = qda_pred, observed = test_data$label)
```

```{r qda_Misclass}
qda_misclass_rate = mean(qda_pred != test_data$label)

cat("Misclassification rate:", qda_misclass_rate*100, "%")
```

```{r qda_ROC}
qda_pred_numeric <- ifelse(qda_pred == "TREG", 1, 0)
  
qda_roc_curve <- roc(
  response = test_data$label_as_numeric,
  predictor = qda_pred_numeric, 
  levels = c(0,1),
  direction = "<"
  )

plot(qda_roc_curve, col = "red", main = "ROC Curve for QDA")

qda_auc <- auc(qda_roc_curve)
qda_auc
```

#### Nearest Neighbor Classifier (k-NN) (Chi)

```{r kNN_train, eval = FALSE}
knn_train_X <- train_data %>% 
  dplyr::select(-label, -label_as_factor, -label_as_numeric)

knn_train_Y <- train_data$label_as_factor

knn_test_X <- test_data %>% 
  dplyr::select(-label, -label_as_factor, -label_as_numeric)

knn_start_time <- Sys.time()
knn_pred <- sapply(kk_vector,
                   function(k)
                     {knn(train = knn_train_X, test = knn_test_X, cl = knn_train_Y, k = k)}
                   )
knn_end_time <- Sys.time()

knn_train_runtime <- knn_end_time - knn_start_time
print(knn_train_runtime) #50 mins for k=1 to k=10 

rm(knn_train_X, knn_train_Y, knn_test_X)
```

```{r kNN_save_load, include = FALSE}
write.csv(knn_pred, file = "knn_predictions.csv", row.names = TRUE)

knn_pred <- read.csv("knn_predictions.csv") %>% 
  dplyr::select(-X)

## CAUTION: currently hard-coded numbers here. To be changed (Chi)
colnames(knn_pred) <- paste0("k=", 1:10)
## CAUTION: currently hard-coded numbers here. To be changed (Chi)
```

```{r kNN_Confusion}
for (i in kk_vector){
  print(paste("k =", i))
  print(table(predicted = knn_pred[,i], observed = test_data$label_as_factor))
}

knn_test_Y <- 
```

```{r kNN_Misclass}
knn_misclass = c(rep(0,length(kk_vector)))

for (i in kk_vector){
  knn_misclass[i] = mean(knn_pred[,i] != test_data$label_as_factor)
}
plot(knn_misclass, type = "l")
```

```{r kNN_ROC}
knn_pred_numeric <- ifelse(knn_pred == "TREG", 1, 0)

knn_roc_curves <- data.frame()

knn_auc <- data.frame()

for (kk in 1:ncol(knn_pred)) {
  roc_curve <- roc(
    response = test_data$label_as_numeric, 
    predictor = knn_pred_numeric[,kk],
    levels = c(0,1),
    direction = "<"
    )
  
  roc_curve_data <- data.frame(
    k = kk,
    FPR = 1 - roc_curve$specificities,
    TPR = roc_curve$sensitivities
    )
  
  auc_data <- data.frame(
    k = kk,
    AUC = roc_curve$auc
    )
  
  knn_roc_curves <- rbind(knn_roc_curves, roc_curve_data)
  
  knn_auc <- rbind(knn_auc, auc_data)
}

knn_roc_curves$k <- as.factor(knn_roc_curves$k)

ggplot(knn_roc_curves, aes(x = FPR, y = TPR, colour = k)) + 
  geom_line(linewidth = 1) +
  labs(
    title = "ROC Curves for k-NN (k = 1 to 10)",
    x = "False Positive Rate (FPR)",
    y = "True Positive Rate (TPR)",
    color = "k"
  )

ggplot(knn_auc, aes(x = k, y = AUC)) + 
  geom_line(linewidth = 1) +
  labs(
    title = "ROC AUCs for k-NN (k = 1 to 10)",
    x = "k",
    y = "ROC AUC"
  )
```

#### Gradient Boosting Decision Trees (GBDT) (Jonathan)

```{r gbmparams}
#Number of trees
num_trees = 1000
# Define grid of lambda (shrinkage) values to evaluate
lambda_grid <- c(0.001, 0.01,0.03,0.05)

# Initialize a vector to store test errors for each lambda
test_errors <- numeric(length(lambda_grid))
```

```{r dataprepgbm}
test_data_cleaned_gbm <- test_data[, !colnames(test_data) %in% c("label_as_factor", "label")]
```

```{r gbmloop, eval=FALSE}
gbm_optimal_lambda_start <- Sys.time()
# Loop over each lambda value
for (i in seq_along(lambda_grid)) {
  lambda <- lambda_grid[i]
  
  # Train the gbm model with the current lambda (shrinkage) value
  gbm_model <- gbm(label_as_numeric ~. -label - label_as_factor, data = train_data, 
                   distribution = "bernoulli", 
                   n.trees = num_trees, 
                   interaction.depth = 4, 
                   shrinkage = lambda, 
                   cv.folds = 5, 
                   verbose = FALSE)
  
  # Make predictions on the test set using the optimal number of trees
  predictions <- predict(gbm_model, newdata = test_data_cleaned_gbm, n.trees = num_trees)
  
  # Calculate the Mean Squared Error on the test set
  test_errors[i] <- mean((predictions - test_data_cleaned_gbm$label_as_numeric)^2)
}
gbm_optimal_lambda_end <- Sys.time()
gbm_optimal_lambda_total_time <- gbm_optimal_lambda_end - gbm_optimal_lambda_start
print(gbm_optimal_lambda_total_time)
```

```{r gbmtse, eval=FALSE}
# Combine lambda values and test errors into a data frame for plotting
error_df <- data.frame(lambda = lambda_grid, test_error = test_errors)

# Plot the test error over the range of lambda values
ggplot(error_df, aes(x = lambda, y = test_error)) +
  geom_line() +
  geom_point() +
  labs(title = "Test Error vs Lambda (Shrinkage)", 
       x = "Lambda (Shrinkage)", 
       y = "Test Mean Squared Error") +
  theme_minimal()

```

The results above show use that the GLM model with the lowest Test Mean Squared Error, will be the one with the lowest lambda value we assigned of 0.001. This does not necessarily mean that the best Gradient Descent Boosting model that will work best will be the best model for our data, as it may not account for over fitting. However, we will continue using the GBDT model with a shrinkage value of 0.001 to analyze the model and to compare it to our other models.

```{r gbm_model}
# Set a single shrinkage value
lambda <- 0.001

# Measure the time it takes to run 
gbm_start_time <- Sys.time()
# Train the gbm model
smallest_lambda_gbm_model <- gbm(label_as_numeric ~. -label - label_as_factor, 
                 data = train_data, 
                 distribution = "bernoulli", 
                 n.trees = num_trees, 
                 interaction.depth = 4, 
                 shrinkage = lambda, 
                 cv.folds = 5, 
                 verbose = FALSE)

# Get the best number of trees based on cross-validation
best_trees <- gbm.perf(smallest_lambda_gbm_model, method = "cv")

# Measure time after model training
gbm_end_time <- Sys.time()
gbm_model_training_time <- gbm_end_time - gbm_start_time
cat("GBM Model training time:", gbm_model_training_time, "\n")

# Measure time for predictions
gbm_start_time_pred <- Sys.time()
# Make predictions on the test set using the best number of trees
gbm_pred_probs <- predict(smallest_lambda_gbm_model, newdata = test_data_cleaned_gbm, n.trees = best_trees, type = "response")
gbm_pred_classes <- ifelse(gbm_pred_probs > 0.5, 1, 0)

# Measure time after predictions
gbm_end_time_pred <- Sys.time()
gbm_prediction_time <- gbm_end_time_pred - gbm_start_time_pred
cat("Prediction time:", gbm_prediction_time, "\n")
```

```{r gbm_Confusion}
gbm_confusion_matrix <- table(Predicted = gbm_pred_classes, Actual = test_data_cleaned_gbm$label_as_numeric)
print(gbm_confusion_matrix)

gbm_balanced_accuracy <- ((1/2) *(gbm_confusion_matrix[2,2]/(gbm_confusion_matrix[2,2]+gbm_confusion_matrix[1,2]))+ (1/2 *(gbm_confusion_matrix[1,1]/(gbm_confusion_matrix[1,1] + gbm_confusion_matrix[2,1]))))

cat("The balanced accuracy is", gbm_balanced_accuracy, "\n")
```

```{r gbm_Misclass}
gbm_misclassification_rate <- mean(gbm_pred_classes != test_data_cleaned_gbm$label_as_numeric)
cat("Misclassification Rate:", gbm_misclassification_rate, "\n")
```

```{r gbm_Accuracy}
gbm_accuracy <- mean(gbm_pred_classes == test_data_cleaned_gbm$label_as_numeric)
cat("Accuracy:", gbm_accuracy, "\n")
```

```{r gbm_ROC_AUC}
gbm_roc_curve <- roc(test_data_cleaned_gbm$label_as_numeric, gbm_pred_probs)
cat("AUC:", auc(gbm_roc_curve), "\n")
plot(gbm_roc_curve, main = "ROC Curve")
```

```{r gbm_f1_score}
gbm_f1_score <- F1_Score(y_pred = gbm_pred_classes, y_true = test_data_cleaned_gbm$label_as_numeric)
cat("F1 Score:", gbm_f1_score, "\n")
```

The accuracy for the gradient descent decision tree model (where the shrinkage term was equal to 0.001) was 0.904. The AUC was equal to 0.972 and the Test mean squared error was equal to 0.109. Misclassification rate was equal to 0.096. And F1 score was equal to 0.928

#### Random Forest (Jonathan)

```{r tree_init}
## Clean the test data for the Random Forest Classifier task
test_data_cleaned_for_rf <- test_data[, !colnames(test_data) %in% c("label_as_numeric", "label")]

## Running Tree on all the variables on train data
tree.rna_features <- tree(label_as_factor ~ . -label_as_numeric -label, split="deviance", data = train_data)
summary(tree.rna_features)
plot(tree.rna_features)
text(tree.rna_features, pretty = 0, cex = 1)

## Predict on Test variables
tree.pred <-predict(tree.rna_features, test_data, type="class")

## Mis-classification Rate and Confusion Matrix
table(tree.pred,test_data_cleaned_for_rf$label_as_factor)
mean(tree.pred!=test_data_cleaned_for_rf$label_as_factor)

```

```{r treecv}
cv.rna_data <- cv.tree(tree.rna_features)
plot(cv.rna_data$size, cv.rna_data$dev, type ='b')
```

Above we use a tree search to find.... When using the tree search, we get that there are 11 features that are the most important to classifying whether a cell is a T Regulatory Cell or a CD4-positive cell. However, we can see based on the tree that there are certain features that appear more than once. Based on the tree plot, we can determine that the most important regressor that helps determine if a cell is a Regulatory T cell or a CD4-positive cell is the IL7R. From the National Library of medicine ("<https://www.ncbi.nlm.nih.gov/gene/3575>"), IL7R is a protein encoded by a gene that plays a critical role in the development of lymph nodes. Based on the analysis above, we decided to create a Random Forest regression that will have 12 terminal nodes.

```{r rfmodel}
# RF model
start_time_rf <- Sys.time()

rf.rna_features <-randomForest(label_as_factor~. - label_as_numeric -label, data=train_data, mtry=64, importance=TRUE, n.tree = 5000) # RF Model

end_time_rf <- Sys.time()
rf_training_time <- end_time_rf - start_time_rf
cat("Random Forest model training time:", rf_training_time, "\n")

# Predictions
start_time_rf_pred <- Sys.time()
yhat.rf <-predict(rf.rna_features, newdata=test_data)

end_time_rf_pred <- Sys.time()
rf_prediction_time <- end_time_rf_pred - start_time_rf_pred
cat("Random Forest prediction time:", rf_prediction_time, "\n")

```

```{r rf_Confusion}
rf_confusion_matrix <- confusion_matrix <- table(Predicted = yhat.rf, Actual = test_data_cleaned_for_rf$label_as_factor)
print(rf_confusion_matrix)
rf_balanced_accuracy <- ((1/2) *(rf_confusion_matrix[2,2]/(rf_confusion_matrix[2,2]+rf_confusion_matrix[1,2]))+ (1/2 *(rf_confusion_matrix[1,1]/(rf_confusion_matrix[1,1] + rf_confusion_matrix[2,1]))))

cat("The balanced accuracy is", rf_balanced_accuracy, "\n")
```

```{r rf_Misclass}
rf_misclassification_rate <- mean(yhat.rf != test_data_cleaned_for_rf$label_as_factor)
cat("Random Forest Misclassification Rate:", rf_misclassification_rate, "\n")
```

```{r rf_Accuracy}
accuracy_rf <- mean(yhat.rf == test_data_cleaned_for_rf$label_as_factor)
cat("Random Forest Accuracy:", accuracy_rf, "\n")
```

```{r rf_ROC_AUC}
roc_rf <- roc(test_data_cleaned_for_rf$label_as_factor, as.numeric(yhat.rf))
cat("Random Forest AUC:", auc(roc_rf), "\n")
plot(roc_rf, main = "Random Forest ROC Curve")
```

```{r rf_f1_score}
rf_f1_score <- F1_Score(y_pred = yhat.rf, y_true = test_data_cleaned_for_rf$label_as_factor)
cat("F1 Score:", rf_f1_score, "\n")
```

Using a random forest tree model, we see that the accuracy of the model was 0.936 Which is quite accurate! Misclassification Rate was equal to 0.064. The F1 score was equal to 0.9504

#### Support Vector Machine (SVM) (Peter)

```{r}

```

### With PCA

#### Linear Discriminant Analysis (LDA) (Imar)

```{r}
# CAUTION TO DO: Still need to refer to use train data and not again CreateDataPartition off 80% of already 80% train data

# Step 1: Scale the data (excluding the target variable)
lda2_train_data <- train_data
lda2_train_data <- lda2_train_data[, !(names(lda2_train_data) %in% c("label_as_factor", "label_as_numeric"))]
lda2_train_data$label <- ifelse(lda2_train_data$label == "TREG", 1, 0)

scaled_data <- lda2_train_data[, -which(names(lda2_train_data) == "label")]  # Exclude the 'label' column
scaled_data <- scale(scaled_data)  # scale the features to have mean=0 and sd=1

# Step 2: Apply PCA and reduce to 10 components
pca_model <- prcomp(scaled_data, center = TRUE, scale. = TRUE)

# Check the variance explained by the principal components
summary(pca_model)

# Extract the first 10 principal components
pca_data <- data.frame(pca_model$x[, 1:10])  # Keep only the first 10 principal components

# Step 3: Add the target variable to the PCA-transformed data
pca_data$label <- lda2_train_data$label  # Add the target labels

# Step 4: Split the data into training and testing sets using createDataPartition
set.seed(123)  # Set a seed for reproducibility
lda_pca_train_indices <- createDataPartition(pca_data$label, p = 0.8, list = FALSE)
lda_pca_train_data <- pca_data[lda_pca_train_indices, ]
lda_pca_test_data <- pca_data[-lda_pca_train_indices, ]

# Step 5: Train the LDA model using the PCA-transformed training data
lda_model <- lda(label ~ ., data = lda_pca_train_data)

# Step 6: Make predictions on the test set
lda_predictions <- predict(lda_model, lda_pca_test_data)

# Step 7: Evaluate the model performance using confusion matrix
lda_pca_test_data$label <- as.factor(lda_pca_test_data$label)
confusion_mat <- confusionMatrix(lda_predictions$class, lda_pca_test_data$label)
levels(lda_predictions); levels(lda_pca_test_data)
# Print the confusion matrix and accuracy
print(confusion_mat)

# Optional: Visualize the results of PCA (for two principal components)
# Create a 2D plot (for visualizing the first two PCs)
ggplot(data = pca_data, aes(x = PC1, y = PC2, color = factor(label_as_numeric))) +
  geom_point() +
  labs(title = "PCA - First 2 Principal Components", x = "PC1", y = "PC2") +
  theme_minimal()

```

#### Logistic classifier (Imar)

```{r}
log_pca_start <- Sys.time()
log_pca_model <- prcomp(train_data_log, scale = TRUE)
log_pca_end <- Sys.time()
log_pca_total_time <- log_pca_end - log_pca_start
log_pca_total_time

summary(log_pca_model)

names(log_pca_model)
log_pca_model$center
log_pca_model$scale
log_pca_model$rotation
dim(log_pca_model)
```

#### Quadratic Discriminant Analysis (QDA) (Chi)

```{r}

```

#### Nearest Neighbor Classifier (k-NN) (Chi)

```{r}

```

#### Gradient Boosting Decision Trees (GBDT) (Jonathan)

```{r Gbm_pca_standartization}
# Standardize the data (important for PCA)
gbm_train_data_scaled <- scale(train_data[, -which(names(train_data) %in% c("label", "label_as_factor"))])
gbm_test_data_scaled <- scale(test_data_cleaned_gbm[,])

gbm_pca_start <- Sys.time()
# Perform PCA
gbm_pca_model <- prcomp(gbm_train_data_scaled, center = TRUE, scale. = TRUE)
gbm_pca_end <- Sys.time()
gbm_pcaa_total_time <- gbm_pca_end - gbm_pca_start

# Get the first 10 principal components
gbm_train_data_pca <- as.data.frame(gbm_pca_model$x[, 1:10])
gbm_train_data_pca$label_as_numeric <- train_data$label_as_numeric  # Add the target variable

# Transform the test data using the PCA model
gbm_test_data_pca <- as.data.frame(predict(gbm_pca_model, newdata = gbm_test_data_scaled)[, 1:10])
gbm_test_data_pca$label_as_numeric <- test_data_cleaned_gbm$label_as_numeric  # Add the target variable
```

#### Random Forest (Jonathan)

```{r}

```

#### Support Vector Machine (SVM) (Peter)

```{r}

```

## T1.3: Improving F1 Score (???)

Train and evaluating three classifiers of our choice with the goal of improving the F1 score (can use methods like bagging, boosting, and regularization (cross-fold validation, ridge/lasso)).

#### Naive Bayes or QDA? - confirm theory to identify techniques of improvement (Chi)

```{r}

```

#### Logistic Regression (w talk about why not adaBoost - confirm with GPT, improve model with ridge/lasso (regularisation), check on ) (Imar)

```{r}

```

#### Random Forest (It's already bagged, use cross-validation) (Imar)

Hyperparameter tuning (cross-validation): Number of trees (n_estimators): Increasing the number of trees generally improves accuracy, but can slow down the model. Maximum tree depth (max_depth): Controls how deep each tree can grow, preventing overfitting with a smaller depth. Minimum samples per leaf (min_samples_leaf): Sets the minimum number of samples required to split a node, preventing overfitting. Number of features considered at each split (max_features): Controls the randomness in the feature selection process.

```{r}

```

## T1.4: Implement predictor (???)

Choose the best approach from those above tested and implement our predictor as a function of our code.

```{r}

```
