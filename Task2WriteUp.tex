% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Task2WriteUp},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Task2WriteUp}
\author{}
\date{\vspace{-2.5em}2024-12-09}

\begin{document}
\maketitle

\section{Task 2: Feature Selection}\label{task-2-feature-selection}

\subsection{Abstract Task 2}\label{abstract-task-2}

The focus of the following analysis is to attempt to achieve high
classification accuracy, while using a minimal number of features on
very highly dimensional data. We will use a combination of six different
feature selection and classification techniques to accomplish the goal.
The models and methods used are a combination of methods taught in our
ST443 Machine Learning and Data Mining Course taught at the London
School of Economics and Political Sciences, and other techniques used
from personal research.

\subsection{Data Description}\label{data-description}

We were given one dataset that contains binary features that describe
the three-dimensional properties of a molecule in a compound or a random
probe across different compounds. The data contains 800 observations,
which represent 800 compounds, and 100,000 columns (50,000 real
variables and 50,000 random probes). The first column named \(label\)
represents whether a compound bound to a target site on
thrombin\footnote{Thrombin is an enzyme that plays a key role in blood
  clotting and other biological processes.}.

\subsection{Exploratory Data Analysis}\label{exploratory-data-analysis}

-mention traintest split as well -imbalanced data -var threshold

\subsection{Models}\label{models}

\paragraph{Lasso}\label{lasso}

The first model we attempted to run was logistic classifier with a
Lasso-Penalty term. The lasso penalty term is a common regularization
technique used for feature selection as it shrinks some of the
coefficient estimates to be exactly equal to zero, effectively removing
them from the model. The ability to shrink coefficients to zero depends
on the magnitude of the tuning parameter, lambda (λ), which we tune in
our model. The best way to choose an optimal λ is through cross
validation, which we apply in our models. For our data, we believed that
the lasso coefficient will be most meaningful on a logistic classifier,
this is because our predicted variable is binary which takes values of
either 1 or -1 (we convert -1 to 0 for simpler interpretation of
probabilities). In our approach, we use a five-fold cross validation,
with a grid of 100 different values of λ. We also use weights to account
for the highly imbalanced data. For each λ, we calculate the predicted
probability where we then decide to classify anything with a probability
of 0.5 or more to the ``\emph{1}'' class else, the ``\emph{0}'' class.
Then using those predictions we calculate the balanced accuracy, and
count the number of selected features for each lambda. Since we want to
account for best balanced accuracy, and lowest amount of features, we
use a scoring formula where \emph{score = balanced accuracy -
0.001}**(number of features)*. This way we penalize results that have
too many features. We then extract the lambda with the best score, train
the data again using that lambda, then test the model using our test
data and report the number of features selected. The table below shows
different results for different predictors including the model with the
worst score, worst balanced accuracy on the cross-validation, the best
score:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2152}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1772}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3671}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2405}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Lambda Value
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Number of Selected Features
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Balanced Accuracy
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Worst Score & 0.1917 & 234 & 0.8189 \\
Best Score & 0.0475081 & 47 & 0.8189 \\
Worst BA & 0.2719 & 1 & 0.8189 \\
\end{longtable}

\paragraph{Forward Stepwise Selection
(FSS)}\label{forward-stepwise-selection-fss}

\paragraph{Random Forest}\label{random-forest}

\paragraph{Elastic Net}\label{elastic-net}

After the results obtained from the logistic classification method with
a lasso penalty term, we thought that it could be interesting to compare
with the results of an elastic net. An elastic net is another
regularization technique which combines both the Lasso (L1) and the
Ridge (L2)\footnote{The Ridge Regularization techinique is one that
  shrinks the coefficients on the predictors, but never to 0, so it
  keeps all coefficients in a model.} penalties. The elastic net penalty
depends on two main parameters, λ, (the strength of the penalty term)
and \(\alpha\) (the mix between Lasso and Ridge). We use a process very
similar to the one used for the Lasso penalty term on the logistic
regression, only we also try different values of \(\alpha\). The table
below shows results for the best scores, worst scores, and worst
balanced accuracy on the validation set:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1848}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1413}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1522}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3152}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2065}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Alpha Value
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Lambda Value
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Number of Selected Features
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Balanced Accuracy
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Worst Score & 0.4 & 0.1205 & 101 & 0.8257 \\
Best Score & 0.49 & 0.0955 & 88 & 0.8608 \\
Worst BA (cv) & 0.46 & 0.1205 & 75 & 0.8608 \\
\end{longtable}

\paragraph{XGBoost}\label{xgboost}

\paragraph{Support Vector Machines
(SVM)}\label{support-vector-machines-svm}

\subsection{Results}\label{results}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2561}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2439}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1829}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1463}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1707}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Num. of Features
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Bal. Accuracy
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Accuracy
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
F1
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Logistic Lasso & \textbf{47} & 0.8189 & 0.925 & 0.6 \\
FSS & 208 & 0.7805 & 0.9188 & 0.5517 \\
Random Forest & 100 & 0.8223 & 0.9313 & 0.6207 \\
Log. Elastic Net. & 88 & \textbf{0.8608} & \textbf{0.9375} &
\textbf{0.6667} \\
XGBoost & 73 & 0.8257 & \textbf{0.9375} & 0.6429 \\
SVM (Linear Kernel) & 1 & 0.8257 & 0.9375 & 0.6429 \\
\end{longtable}

\subsection{Conclusion}\label{conclusion}

\paragraph{Strengths and Weaknesses}\label{strengths-and-weaknesses}

\paragraph{Future Research}\label{future-research}

\subsection{Refrences}\label{refrences}

Guyon, I., Gunn, S., Ben-Hur, A. and Dror, G. (2004) Result Analysis of
the NIPS 2003 Feature Selection Challenge. In Advances in Neural
Information Processing Systems (Saul, L., Weiss, Y. and Bottou, L.,
eds.), vol.~17, MIT Press.

Shlobin NA, Har-Even M, Itsekson-Hayosh Z, Harnof S, Pick CG. Role of
Thrombin in Central Nervous System Injury and Disease. Biomolecules.
2021 Apr 12;11(4):562. doi: 10.3390/biom11040562. PMID: 33921354; PMCID:
PMC8070021.

\end{document}
