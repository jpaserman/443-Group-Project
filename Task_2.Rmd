---
title: "Task 2 RmD"
output: html_document
date: "2024-11-19"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn=-1)
```

```{r parameters}
seed = 443
csv_file_two = "data2.csv.gz"
training_size = 0.8 # Proportion of dataset that is to be split for training
packages = list("caret", 
                "randomForest", 
                "tree",
                "glmnet",
                "leaps",
                "xgboost"
                )

```

```{r, include=FALSE}
for (package in packages){
  if(!require(package, character.only = TRUE)){
    install.packages(package, character.only = TRUE)
  } else {
    library(package, character.only = TRUE)
  }
}
```

# Task 2: feature selection

```{r load_data}
set.seed(seed)

# load_data_start_time <- Sys.time()

task_two_df <- read.csv(csv_file_two)

# load_data_end_time <- Sys.time()
# 
# load_data_runtime <- load_data_end_time - load_data_start_time
# print(load_data_runtime) # 1.3 mins


# check_na_start_time <- Sys.time()
# 
# # Checking if there are any rows that have missing values... Keep this hashed unless you want to check, because it takes a while to run
# for (i in 1:nrow(task_two_df)) {
#   if (any(is.na(task_two_df[i, ]))) {
#     print(paste("Row", i, "contains NA values."))
#   }
# }
# 
# check_na_end_time <- Sys.time() # 16.8 mins

# check_na_runtime <- check_na_end_time - check_na_start_time
# print(check_na_runtime) 


# task_two_df$label_as_factor <- as.factor(task_two_df$label)

```

```{r EDA}

nrow(task_two_df) # Our data originally has 800 observations (compounds)
ncol(task_two_df) # Our data originally has 100,001 features (1 label as well as 50000 real variables and 50000 random probes, randomly permuted)

# Construct dataset of solely the binary features
task_two_df_no_label <- task_two_df[, -1]

# For each feature, determine the number of observations with "1s" and the variance of the feature
totals_vector <- sapply(task_two_df_no_label, sum)
variance_vector <- sapply(task_two_df_no_label, var)

quantile(totals_vector)
quantile(variance_vector)

# Histogram of the totals and the variance
hist(totals_vector, breaks = seq(min(totals_vector), max(totals_vector), by = 1),
     main = "Histogram of number of compounds per feature")
hist(variance_vector, breaks = seq(min(variance_vector), max(variance_vector), length.out = 1000),
     main = "Histogram of variance of compounds per feature")

### We notice that there are numerous features that are present in almost no observations
### As such, we investigate the possiblity of implementing a variance threshold to 
### restrict the number of features we retain
sum(totals_vector == 0)

# If we implement a variance threshold of 0.01 (corresponding to a feature only being present
# in a maximum of 8 out of 800 observations), we would remove 69,879 features
above_var_threshold <- totals_vector[variance_vector > .01]
min(above_var_threshold)
sum(variance_vector <= .01)

```

```{r drop_columns less than 0.01 variance}
# Remove features that have less than or equal to 0.01 variance
cols_below_var_threshold <- apply(task_two_df, 2, var) <= 0.01
table(cols_below_var_threshold)
task_two_df <- task_two_df[, !cols_below_var_threshold]

# After removing features with low variance, our data prior to feature selection 
# has 800 observations and now only 30,122 features
nrow(task_two_df)
ncol(task_two_df)

# Labels represent whether given compound binds to target site on thrombin 
# Possible values are -1 (compound did not bind) and 1 (compound did bind)
table(task_two_df$label) # 772 compounds did not bind and 78 compounds did bind

```

```{r train_test_split}
# Split Data into Test and Train (We should all use the same data)
train_split = as.integer(nrow(task_two_df)*training_size)

train_indices <- sample(1:nrow(task_two_df), train_split)
train_data <- task_two_df[train_indices,]
test_data <- task_two_df[-train_indices,]
```

## T2.1: exploratory data analysis and summary statistics (??? -- need to expand)

Our partial EDA is in above chunk of code where we plot histograms of the totals and the variance amongst each of the 100,000 original features. Should this still be expanded?

## T2.2: train and evaluate feature selection methods

Train and evaluate feature selection methods with the goal of achieving high balanced accuracy using a minimal number of selected features. Use three different approaches of your choice that were covered in the course.

### Lasso (Jonathan)

```{r lasso with balanced accuracy}

lasso_test_data <-test_data
lasso_train_data <-train_data

lasso_test_data$label[lasso_test_data$label == -1] <- 0
lasso_train_data$label[lasso_train_data$label == -1] <- 0
# Prepare the data
test_data_clean_matrix <- as.matrix(lasso_test_data[, -1])
train_data_matrix <- as.matrix(lasso_train_data[, -1])
train_labels <- lasso_train_data$label
test_labels <- lasso_test_data$label

# Define the lambda grid
grid <- 10^seq(1, -4, length = 100)

# Function to calculate balanced accuracy
calculate_balanced_accuracy <- function(confusion) {
  if (all(dim(confusion) == c(2, 2))) {
    balanced_acc <- (1/2) * (confusion[2,2] / (confusion[2,2] + confusion[1,2])) + 
                    (1/2) * (confusion[1,1] / (confusion[1,1] + confusion[2,1]))
  } else {
    balanced_acc <- NA
  }
  return(balanced_acc)
}

# Adding weights because of the unbalanced data
class_weights <- ifelse(train_labels == 1, 1, sum(train_labels == 1) / sum(train_labels == 0))


# Perform cross-validation with a sequence of lambda values
start_task_two_lasso <- Sys.time()
cv_fit <- cv.glmnet(train_data_matrix, train_labels, alpha = 1, lambda = grid, nfolds = 5, family = "binomial", weights = class_weights)
end_task_two_lasso <- Sys.time()
total_lasso_time <- end_task_two_lasso - start_task_two_lasso
print(total_lasso_time)

# Extract the lambda values and their corresponding balanced accuracies
lambdas <- cv_fit$lambda
cv_results <- sapply(lambdas, function(lambda) {
  pred.cv <- predict(cv_fit, newx = train_data_matrix, s = lambda, family = "binomial", type ="response")
  pred.cv_binary <- ifelse(pred.cv > 0.5, 1, 0)
  confusion <- table(Predicted = pred.cv_binary, Actual = train_labels)
  balanced_acc <- calculate_balanced_accuracy(confusion)
  num_predictors <- sum(coef(cv_fit, s = lambda) != 0) - 1
  return(c(balanced_acc, num_predictors))
})

balanced_accuracies <- cv_results[1, ]
num_predictors <- cv_results[2, ]

# Find the lambda with the highest balanced accuracy
best_lambda_index <- which.max(balanced_accuracies)
best_lambda <- lambdas[best_lambda_index]
best_balanced_accuracy <- balanced_accuracies[best_lambda_index]
best_num_predictors <- num_predictors[best_lambda_index]

# Fit the final model using the best lambda
final_model <- glmnet(train_data_matrix, train_labels, alpha = 1, lambda = lambdas[best_lambda_index], family = "binomial", weights = class_weights)
pred.test <- predict(final_model, newx = test_data_clean_matrix, family ="binomial", type = "response")
pred.test_binary <- ifelse(pred.test > 0.5, 1, 0)
confusion_test <- table(Predicted = pred.test_binary, Actual = test_labels)
test_balanced_accuracy <- calculate_balanced_accuracy(confusion_test)

cat("Best Lambda:", best_lambda, "\n")
cat("Highest Balanced Accuracy on CV:", best_balanced_accuracy, "\n")
cat("Number of Predictors for Best Lambda:", best_num_predictors, "\n")
cat("Balanced Accuracy on Test Data:", test_balanced_accuracy, "\n")
```

### Forward stepwise selection (FSS) (???)

```{r, results='hold'}
fss_start_time <- Sys.time()

regfit_fwd <-  regsubsets(label ~ .,
                        data = task_two_df_first_523,
                        nvmax = 170,
                        method = "forward")
reg_summary <- summary(regfit_fwd)

fss_end_time <- Sys.time()
fss_runtime <- fss_end_time - fss_start_time
print(fss_runtime)

par(mfrow=c(2,2))
plot(reg_summary$rss, xlab="Number of Variables", ylab="RSS")

# Plot adjusted R2 vs Number of Variables
plot(reg_summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="l")
# which_max() function is used to identify the location of the maximum point of a vector
best_model = which.max(reg_summary$adjr2)
# Plot a red dot to indicate the model with the largest adjusted R2
points(best_model, reg_summary$adjr2[best_model], col="red", cex=2, pch=20)

## In a similar fashion, we can plot Cp and BIC
plot(reg_summary$cp, xlab="Number of Variables", ylab="Cp", type="l")
best_model = which.min(reg_summary$cp)
points(best_model, reg_summary$cp[best_model], col="red", cex=2, pch=20)

plot(reg_summary$bic, xlab="Number of Variables", ylab="BIC", type="l")
best_model = which.min(reg_summary$bic)
points(best_model, reg_summary$bic[best_model], col="red", cex=2, pch=20)

```

```{r, fig.height = 20, fig.width = 50, fig.align = "center"}
par(mfrow = c(2, 2))
plot(regfit_fwd, scale = "r2")
plot(regfit_fwd, scale = "adjr2")
plot(regfit_fwd, scale = "Cp")
plot(regfit_fwd, scale = "bic")
```

### Random forest (???)

```{r}

```

## T2.3: additional feature selection methods

Train and evaluate up to three additional feature selection methods, other than those in T2.2, which may include methods not covered in the course, with the goal of achieving high balanced accuracy using a minimal number of selected features. Provide an explanation of each new method.

For T2.2 and T2.3, your evaluation results should include the balanced accuracy and the number of selected features achieved by each method for various numbers of selected features. You may also compare the balanced accuracy achieved by different methods for a similar number of selected features.

### Elastic net (not covered in class) (???)

```{r}

```

### [[Tree-Based models]] (i.e., either XGBoost or LightGBM) (Jonathan)

```{r xgboost}
# Replace the labels of -1 to 0 for train and test data
xgb_train_data <- train_data
xgb_test_data <- test_data
xgb_test_data$label[xgb_test_data$label == -1] <- 0
xgb_train_data$label[xgb_train_data$label == -1] <- 0

xgb_train_matrix <- xgb.DMatrix(data = as.matrix(xgb_train_data[,]), label = xgb_train_data$label)
xgb_test_matrix <- xgb.DMatrix(data = as.matrix(xgb_test_data[,]), label = xgb_test_data$label)

# Add weights to account for imbalanced data
scale_pos_weight = sum(xgb_train_data$label == 0) / sum(xgb_train_data$label == 1)

xgb_model <- xgb.cv(
      data = xgb_train_matrix,
      eta = 0.01,
      max_depth = 6,
      nrounds = 5000,
      objective = "binary:logistic",
      verbose = 0,
      scale_pos_weight = scale_pos_weight,
      nfold = 5,
      metrics = "auc"
      )
    
# Making predictions on the test set
xgb_predictions <- predict(xgb_model, newdata = xgb_test_matrix)
xgb_predictions <- ifelse(xgb_predictions > 0.5, 1, 0)
    
# Calculating the Mean Squared Error on the test set
mse <- mean((xgb_predictions - xgb_test_data$label)^2)
cat("Test Mean Squared Error:", mse, "\n")
    
xgb_confusion_matrix <- table(Predicted = xgb_predictions, Actual =  xgb_test_data$label)
print(xgb_confusion_matrix)

xgb_balanced_accuracy <- ((1/2) *(xgb_confusion_matrix[2,2]/(xgb_confusion_matrix[2,2]+xgb_confusion_matrix[1,2]))+ (1/2 *(xgb_confusion_matrix[1,1]/(xgb_confusion_matrix[1,1] + xgb_confusion_matrix[2,1]))))

cat("The balanced accuracy is", xgb_balanced_accuracy, "\n")
cat("Lambda:", lambda, "Depth:", depth, "Balanced Accuracy:", xgb_balanced_accuracy, "\n")


xgb_model_end <- Sys.time()
xgb_model_total_time <- xgb_model_end - xgb_model_start 
cat("The xgboost model took ", xgb_model_total_time, " for the task 2 data \n")

# Get feature importance
importance_matrix <- xgb.importance(feature_names = colnames(xgb_train_matrix), model = best_model)
print(importance_matrix)

# Plot feature importance
xgb.plot.importance(importance_matrix)
```



### (Sparse?) SVM (???)

Is the non-sparse SVM a feature-selection method?

```{r}

```
