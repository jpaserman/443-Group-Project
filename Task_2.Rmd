---
title: "Task 2 RmD"
output: html_document
date: "2024-11-19"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn=-1)
```

```{r parameters}
seed = 443
csv_file_two = "data2.csv.gz"
training_size = 0.8 # Proportion of dataset that is to be split for training
packages = list("caret", 
                "randomForest", 
                "tree",
                "glmnet",
                "leaps",
                "xgboost",
                "tibble",
                "dplyr",
                "pROC",
                "MLmetrics",
                "ranger",
                "kernlab",
                "ggplot"
                )

```

```{r, include=FALSE}
for (package in packages){
  if(!require(package, character.only = TRUE)){
    install.packages(package, character.only = TRUE)
  } else {
    library(package, character.only = TRUE)
  }
}
```

```{r functions}
fun_calc_balanced_accuracy <- function(confusion_matrix, pos_label, neg_label){
  tp <- confusion_matrix[pos_label, pos_label]
  tn <- confusion_matrix[neg_label, neg_label]
  fp <- confusion_matrix[pos_label, neg_label]
  fn <- confusion_matrix[neg_label, pos_label]
  
  balanced_accuracy <- 0.5 * (tp / (tp + fn) + tn / (tn + fp)) 
  
  return(balanced_accuracy)
}

```

# Task 2: feature selection

```{r load_data}
set.seed(seed)

# load_data_start_time <- Sys.time()

task_two_df <- read.csv(csv_file_two)

# load_data_end_time <- Sys.time()
# 
# load_data_runtime <- load_data_end_time - load_data_start_time
# print(load_data_runtime) # 1.3 mins


# check_na_start_time <- Sys.time()
# 
# # Checking if there are any rows that have missing values... Keep this hashed unless you want to check, because it takes a while to run
# for (i in 1:nrow(task_two_df)) {
#   if (any(is.na(task_two_df[i, ]))) {
#     print(paste("Row", i, "contains NA values."))
#   }
# }
# 
# check_na_end_time <- Sys.time() # 16.8 mins

# check_na_runtime <- check_na_end_time - check_na_start_time
# print(check_na_runtime) 


```

```{r EDA}
# Our data originally has 800 observations (compounds)
# Our data originally has 100,001 features (1 label as well as 50000 real variables and 50000 random probes, randomly permuted)
dim(task_two_df)

# Construct dataset of solely the binary features
task_two_df_no_label <- task_two_df[, -1]

# For each feature, determine the number of observations with "1s" and the variance of the feature
totals_vector <- sapply(task_two_df_no_label, sum)
variance_vector <- sapply(task_two_df_no_label, var)

quantile(totals_vector)
quantile(variance_vector)

# Histogram of the totals and the variance
hist(totals_vector, breaks = seq(min(totals_vector), max(totals_vector), by = 1),
     main = "Histogram of number of compounds per feature")
hist(variance_vector, breaks = seq(min(variance_vector), max(variance_vector), length.out = 1000),
     main = "Histogram of variance of compounds per feature")

### We notice that there are numerous features that are present in almost no observations
### As such, we investigate the possiblity of implementing a variance threshold to 
### restrict the number of features we retain
sum(totals_vector == 0)

# If we implement a variance threshold of 0.01 (corresponding to a feature only being present
# in a maximum of 8 out of 800 observations), we would remove 69,879 features
above_var_threshold <- totals_vector[variance_vector > .01]
min(above_var_threshold)
sum(variance_vector <= .01)

```

```{r drop_columns less than 0.01 variance, cache=TRUE}
# Remove features that have less than or equal to 0.01 variance
cols_below_var_threshold <- apply(task_two_df, 2, var) <= 0.01
table(cols_below_var_threshold)
task_two_df <- task_two_df[, !cols_below_var_threshold]

# After removing features with low variance, our data prior to feature selection 
# has 800 observations and now only 30,122 features
dim(task_two_df)

# Labels represent whether given compound binds to target site on thrombin 
# Possible values are -1 (compound did not bind) and 1 (compound did bind)
table(task_two_df$label) # 772 compounds did not bind and 78 compounds did bind

```



```{r select_top_predictors, cache=TRUE}
# Determine the features that are most highly correlated with the label
cor_with_label <-  data.frame(cor(task_two_df, task_two_df$label))
cor_with_label <- tibble::rownames_to_column(cor_with_label, "feature")

top_1000_cor <- cor_with_label %>% 
  
  mutate(abs_cor = abs(`cor.task_two_df..task_two_df.label.`)) %>% 
  
  top_n(1001, abs_cor)
  
# We will utilize this subset of features for models that are running into 
# difficulties with computation/memory size (such as FSS)
top_1000_vars <- top_1000_cor$feature

```


```{r train_test_split}
set.seed(seed) # Not sure why, but was required to reset the seed in this location

# Split Data into Test and Train (We should all use the same data)
train_split = as.integer(nrow(task_two_df)*training_size)

train_indices <- sample(1:nrow(task_two_df), train_split)
train_data_2 <- task_two_df[train_indices,]
test_data_2 <- task_two_df[-train_indices,]

table(train_data_2$label) # The training data has 575 compounds not bind and 65 compounds do bind
table(test_data_2$label) # The testing data has 147 compounds not bind and 13 compounds do bind

# We will utilize this subset of features for models that are running into 
# difficulties with computation/memory size (such as FSS)
train_data_2_top_subset <- train_data_2[, top_1000_vars]
test_data_2_top_subset <- test_data_2[, top_1000_vars]

```

## T2.1: exploratory data analysis and summary statistics (??? -- need to expand)

Our partial EDA is in above chunk of code where we plot histograms of the totals and the variance amongst each of the 100,000 original features. Should this still be expanded?

## T2.2: train and evaluate feature selection methods

Train and evaluate feature selection methods with the goal of achieving high balanced accuracy using a minimal number of selected features. Use three different approaches of your choice that were covered in the course.

### Lasso

```{r lasso with balanced accuracy}
set.seed(seed)
lasso_test_data <-test_data_2
lasso_train_data <-train_data_2

# Replace -1 with 0 for interpretation of logistic function results
lasso_test_data$label[lasso_test_data$label == -1] <- 0
lasso_train_data$label[lasso_train_data$label == -1] <- 0
# Prepare the data
test_data_clean_matrix <- as.matrix(lasso_test_data[, -1])
train_data_matrix <- as.matrix(lasso_train_data[, -1])
train_labels <- lasso_train_data$label
test_labels <- lasso_test_data$label

# lambda grid that will loop over 100 different values of lambda 
grid <- 10^seq(1, -4, length = 100)

# Function to calculate balanced accuracy
calculate_balanced_accuracy <- function(confusion) {
  if (all(dim(confusion) == c(2, 2))) {
    balanced_acc <- (1/2) * (confusion[2,2] / (confusion[2,2] + confusion[1,2])) + 
                    (1/2) * (confusion[1,1] / (confusion[1,1] + confusion[2,1]))
  } else {
    balanced_acc <- NA
  }
  return(balanced_acc)
}

# Adding weights because of the imbalanced data
class_weights <- ifelse(train_labels == 1, 1, sum(train_labels == 1) / sum(train_labels == 0))


# Performing cross-validation with a sequence of lambda values
start_task_two_lasso <- Sys.time()
cv_fit <- cv.glmnet(train_data_matrix, train_labels, alpha = 1, lambda = grid, nfolds = 5, family = "binomial", weights = class_weights)
end_task_two_lasso <- Sys.time()
total_lasso_time <- end_task_two_lasso - start_task_two_lasso
print(total_lasso_time)

# Extract the lambda values and their corresponding balanced accuracies
lambdas <- cv_fit$lambda
cv_results <- sapply(lambdas, function(lambda) {
  pred.cv <- predict(cv_fit, newx = train_data_matrix, s = lambda, family = "binomial", type ="response") # Predicting model on train data, logistic function, for each lambda
  pred.cv_binary <- ifelse(pred.cv > 0.5, 1, 0)
  confusion <- table(Predicted = pred.cv_binary, Actual = train_labels)
  balanced_acc <- calculate_balanced_accuracy(confusion)
  num_predictors <- sum(coef(cv_fit, s = lambda) != 0) - 1 # Counting the coefficients, subtracting the intercept coef. 
  custom_scores <- balanced_acc - (0.001 * num_predictors)
  return(c(balanced_acc, num_predictors, custom_scores))
})
# Saving the balanced accuracies from the new matrix formed
balanced_accuracies <- cv_results[1, ]
# Saving the number of predictors from the new matrix formed
num_predictors <- cv_results[2, ]
custom_scores <- cv_results[3, ]

# Find the lambda with the highest balanced accuracy
best_lambda_index <- which.max(custom_scores) #Max balanaced accuracy for the predictions. 
best_lambda <- lambdas[best_lambda_index] # Extracting the best lambda
best_balanced_accuracy <- balanced_accuracies[best_lambda_index] # Taking the best balanced accuracy 
best_num_predictors <- num_predictors[best_lambda_index] # Taking the best number of predictors for the given lambda 

# Fit the final model using the best lambda, in terms of lambda that returned highest bal. accuracy 
final_model <- glmnet(train_data_matrix, train_labels, alpha = 1, lambda = lambdas[best_lambda_index], family = "binomial", weights = class_weights) # Lasso on train, with best balanced accuracy
pred.test <- predict(final_model, newx = test_data_clean_matrix, family ="binomial", type = "response") # Predictions for test data
pred.test_binary <- ifelse(pred.test > 0.5, 1, 0) # Classification for probabilities, > 0.5 will be 1, and less will be classified as 0 (which is actually-1). 
confusion_test <- table(Predicted = pred.test_binary, Actual = test_labels) # Confusion matrix 
test_balanced_accuracy <- calculate_balanced_accuracy(confusion_test) # Balanced accuracy on test data

cat("Best Lambda:", best_lambda, "\n")
cat("Highest Balanced Accuracy on CV:", best_balanced_accuracy, "\n")
cat("Number of Predictors for Best Lambda:", best_num_predictors, "\n")
cat("Balanced Accuracy on Test Data:", test_balanced_accuracy, "\n")
```

```{r lowest_score_model}
# Find the lambda with the highest balanced accuracy
worst_lambda_index <- which.min(custom_scores) #Max balanaced accuracy for the predictions. 
worst_lambda <- lambdas[worst_lambda_index] # Extracting the best lambda
worst_balanced_accuracy <- balanced_accuracies[worst_lambda_index] # Taking the best balanced accuracy 
worst_num_predictors <- num_predictors[worst_lambda_index] # Taking the best number of predictors for the given lambda 

# Fit the final model using the best lambda, in terms of lambda that returned highest bal. accuracy 
worst_final_model <- glmnet(train_data_matrix, train_labels, alpha = 1, lambda = lambdas[worst_lambda_index], family = "binomial", weights = class_weights) # Lasso on train, with best balanced accuracy
worst_pred.test <- predict(worst_final_model, newx = test_data_clean_matrix, family ="binomial", type = "response") # Predictions for test data
worst_pred.test_binary <- ifelse(pred.test > 0.5, 1, 0) # Classification for probabilities, > 0.5 will be 1, and less will be classified as 0 (which is actually-1). 
worst_confusion_test <- table(Predicted = worst_pred.test_binary, Actual = test_labels) # Confusion matrix 
worst_test_balanced_accuracy <- calculate_balanced_accuracy(worst_confusion_test) # Balanced accuracy on test data

cat("Worst score Lambda:", worst_lambda, "\n")
cat("lowest Balanced Accuracy on CV:", worst_balanced_accuracy, "\n")
cat("Number of Predictors for worst score:", worst_num_predictors, "\n")
cat("Balanced Accuracy on Test Data:", worst_test_balanced_accuracy, "\n")
cat("Score:", custom_scores[worst_lambda_index], "\n")

```

```{r worst_lambda_bal_acc}
# Find the lambda with the highest balanced accuracy
worst_lambda_index <- which.min(balanced_accuracies) #Max balanaced accuracy for the predictions. 
worst_lambda <- lambdas[worst_lambda_index] # Extracting the best lambda
worst_balanced_accuracy <- balanced_accuracies[worst_lambda_index] # Taking the best balanced accuracy 
worst_num_predictors <- num_predictors[worst_lambda_index] # Taking the best number of predictors for the given lambda 

# Fit the final model using the best lambda, in terms of lambda that returned highest bal. accuracy 
worst_final_model <- glmnet(train_data_matrix, train_labels, alpha = 1, lambda = lambdas[worst_lambda_index], family = "binomial", weights = class_weights) # Lasso on train, with best balanced accuracy
worst_pred.test <- predict(worst_final_model, newx = test_data_clean_matrix, family ="binomial", type = "response") # Predictions for test data
worst_pred.test_binary <- ifelse(pred.test > 0.5, 1, 0) # Classification for probabilities, > 0.5 will be 1, and less will be classified as 0 (which is actually-1). 
worst_confusion_test <- table(Predicted = worst_pred.test_binary, Actual = test_labels) # Confusion matrix 
worst_test_balanced_accuracy <- calculate_balanced_accuracy(worst_confusion_test) # Balanced accuracy on test data

cat("Worst score Lambda:", worst_lambda, "\n")
cat("lowest Balanced Accuracy on CV:", worst_balanced_accuracy, "\n")
cat("Number of Predictors for worst score:", worst_num_predictors, "\n")
cat("Balanced Accuracy on Test Data:", worst_test_balanced_accuracy, "\n")
```



```{r lasso_Accuracy}
lasso_accuracy <- mean(pred.test_binary == lasso_test_data$label)
cat("Lasso Accuracy:", lasso_accuracy, "\n") # .93125
```

```{r lasso_ROC_AUC}
lasso_roc <- roc(lasso_test_data$label, pred.test_binary)
lasso_auc <- auc(lasso_roc)
cat("Lasso AUC:", lasso_auc, "\n") #
plot(lasso_roc, main = "Lasso ROC Curve") # Make the limits of the axes 0-1
```

```{r lasso_f1_score}
lasso_f1_score <- F1_Score(y_pred = pred.test_binary, y_true = lasso_test_data$label, positive = "1")
cat("F1 Score:", lasso_f1_score, "\n") 
```

#### Linear lasso
```{r lin_lasso_model}
# Rename datasets for linear lasso regression
linear_lasso_test_data <- test_data_2
linear_lasso_train_data <- train_data_2

# Prepare the data
linear_lasso_test_data_clean_matrix <- as.matrix(linear_lasso_test_data[, -1])
linear_lasso_train_data_matrix <- as.matrix(linear_lasso_train_data[, -1])
linear_lasso_train_labels <- linear_lasso_train_data$label
linear_lasso_test_labels <- linear_lasso_test_data$label

# Lambda grid that will loop over 100 different values of lambda
linear_lasso_grid <- 10^seq(1, -4, length = 100)

# Function to calculate balanced accuracy
linear_lasso_calculate_balanced_accuracy <- function(confusion) {
  if (all(dim(confusion) == c(2, 2))) {
    balanced_acc <- (1/2) * (confusion[2, 2] / (confusion[2, 2] + confusion[1, 2])) + 
                    (1/2) * (confusion[1, 1] / (confusion[1, 1] + confusion[2, 1]))
  } else {
    balanced_acc <- NA
  }
  return(balanced_acc)
}

# Adding weights because of the imbalanced data
linear_lasso_class_weights <- ifelse(linear_lasso_train_labels == 1, 1, 
                                     sum(linear_lasso_train_labels == 1) / sum(linear_lasso_train_labels == -1))

# Performing cross-validation with a sequence of lambda values
linear_lasso_start_time <- Sys.time()
linear_lasso_cv_fit <- cv.glmnet(linear_lasso_train_data_matrix, linear_lasso_train_labels, 
                                 alpha = 1, lambda = linear_lasso_grid, nfolds = 5, 
                                 weights = linear_lasso_class_weights)
linear_lasso_end_time <- Sys.time()
linear_lasso_total_time <- linear_lasso_end_time - linear_lasso_start_time
print(linear_lasso_total_time)

# Extract the lambda values and their corresponding balanced accuracies
linear_lasso_lambdas <- linear_lasso_cv_fit$lambda
linear_lasso_cv_results <- sapply(linear_lasso_lambdas, function(lambda) {
  pred.cv <- predict(linear_lasso_cv_fit, newx = linear_lasso_train_data_matrix, s = lambda) # Linear regression predictions
  pred.cv_binary <- ifelse(pred.cv > 0, 1, -1) # Classification threshold for linear predictions
  confusion <- table(Predicted = pred.cv_binary, Actual = linear_lasso_train_labels)
  balanced_acc <- linear_lasso_calculate_balanced_accuracy(confusion)
  num_predictors <- sum(coef(linear_lasso_cv_fit, s = lambda) != 0) - 1 # Number of coefficients
  custom_scores <- balanced_acc - (0.001 * num_predictors)
  return(c(balanced_acc, num_predictors, custom_scores))
})

# Saving the balanced accuracies, number of predictors, and custom scores
linear_lasso_balanced_accuracies <- linear_lasso_cv_results[1, ]
linear_lasso_num_predictors <- linear_lasso_cv_results[2, ]
linear_lasso_custom_scores <- linear_lasso_cv_results[3, ]

# Find the lambda with the highest custom score
linear_lasso_best_lambda_index <- which.max(linear_lasso_custom_scores)
linear_lasso_best_lambda <- linear_lasso_lambdas[linear_lasso_best_lambda_index]
linear_lasso_best_balanced_accuracy <- linear_lasso_balanced_accuracies[linear_lasso_best_lambda_index]
linear_lasso_best_num_predictors <- linear_lasso_num_predictors[linear_lasso_best_lambda_index]

# Fit the final model using the best lambda
linear_lasso_final_model <- glmnet(linear_lasso_train_data_matrix, linear_lasso_train_labels, 
                                   alpha = 1, lambda = linear_lasso_best_lambda, 
                                   weights = linear_lasso_class_weights)

# Predictions for test data
linear_lasso_pred_test <- predict(linear_lasso_final_model, 
                                  newx = linear_lasso_test_data_clean_matrix) # Linear regression predictions
linear_lasso_pred_test_binary <- ifelse(linear_lasso_pred_test > 0, 1, -1) # Classification threshold
linear_lasso_confusion_test <- table(Predicted = linear_lasso_pred_test_binary, 
                                     Actual = linear_lasso_test_labels)
linear_lasso_test_balanced_accuracy <- linear_lasso_calculate_balanced_accuracy(linear_lasso_confusion_test)

# Output the results
cat("Linear Lasso Best Lambda:", linear_lasso_best_lambda, "\n")
cat("Linear Lasso Highest Balanced Accuracy on CV:", linear_lasso_best_balanced_accuracy, "\n")
cat("Linear Lasso Number of Predictors for Best Lambda:", linear_lasso_best_num_predictors, "\n")
cat("Linear Lasso Balanced Accuracy on Test Data:", linear_lasso_test_balanced_accuracy, "\n")

```

```{r lin_lasso_Accuracy}
linear_lasso_accuracy <- mean(linear_lasso_pred_test_binary == linear_lasso_test_data$label)
cat("Lasso Accuracy:", linear_lasso_accuracy, "\n") # 0.925
```

```{r lin_lasso_ROC_AUC}
linear_lasso_roc <- roc(linear_lasso_test_data$label, linear_lasso_pred_test_binary)
linear_lasso_auc <- auc(linear_lasso_roc)
cat("Lasso AUC:", linear_lasso_auc, "\n") # 
plot(linear_lasso_roc, main = "Lasso ROC Curve") # Make the limits of the axes 0-1
```

```{r lin_lasso_f1_score}
linear_lasso_f1_score <- F1_Score(y_pred = linear_lasso_pred_test_binary, y_true = linear_lasso_test_data$label, positive = "1")
cat("F1 Score:", linear_lasso_f1_score, "\n") 
```



### Forward stepwise selection (FSS)

```{r fss_Training, results='hold'}
set.seed(seed)
fss_test_data <- test_data_2_top_subset
fss_train_data <- train_data_2_top_subset

# Perform FSS iteratively to generate 500 models each with 1, 2, 3, ..., 500 features
fss_start_time <- Sys.time()

regfit_fwd <-  regsubsets(label ~ .,
                        data = fss_train_data,
                        nvmax = 500,
                        method = "forward")
reg_summary <- summary(regfit_fwd)

fss_end_time <- Sys.time()
fss_runtime <- fss_end_time - fss_start_time
print(fss_runtime)

par(mfrow=c(1,2))

# Plot adjusted R2 vs Number of Variables
plot(reg_summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="l")
# which_max() function is used to identify the location of the maximum point of a vector
best_model_adjr2 = which.max(reg_summary$adjr2) #
# Plot a red dot to indicate the model with the largest adjusted R2
points(best_model_adjr2, reg_summary$adjr2[best_model_adjr2], col="red", cex=2, pch=20)
print(best_model_adjr2)

## In a similar fashion, we can plot BIC
plot(reg_summary$bic, xlab="Number of Variables", ylab="BIC", type="l")
best_model_bic = which.min(reg_summary$bic)
points(best_model_bic, reg_summary$bic[best_model_bic], col="red", cex=2, pch=20)
print(best_model_bic)

```

The best model according to maximizing RSS on the training data has 383 features
The best model according to minimizing BIC on the training data has 208 features

We will proceed with the model recommended by minimizing BIC (208 features).

```{r fss_Plots, fig.height = 20, fig.width = 50, fig.align = "center"}
par(mfrow = c(1, 2))
plot(regfit_fwd, scale = "adjr2")
plot(regfit_fwd, scale = "bic")

```

```{r fss_Prediction}
# There is no `predict()` method for `regsubsets()`
# we write our own version of the predict function as
predict_regsubsets = function(object, newdata, id) {
  form = as.formula(object$call[[2]])   #label~.
  mat = model.matrix(form, newdata)
  coef_i = coef(object, id = id)
  mat[, names(coef_i)] %*% coef_i
}

# test the function
fss_test_data["value"] <- predict_regsubsets(regfit_fwd, fss_test_data, id=best_model_bic)
fss_test_data["label_pred"] <- ifelse(fss_test_data["value"] < 0, -1, 1)

```

```{r fss_Confusion}
fss_confusion_matrix <- table(predict = fss_test_data$label_pred, truth = fss_test_data$label)
print(fss_confusion_matrix)
print(fss_confusion_matrix[2,2])
fss_balanced_accuracy <- fun_calc_balanced_accuracy(fss_confusion_matrix,
                                                    pos_label = "1",
                                                    neg_label = "-1")
cat("The balanced accuracy is", fss_balanced_accuracy, "\n") # 0.7804

```

```{r fss_Misclass}
fss_misclass <- mean(fss_test_data$label_pred != fss_test_data$label)
cat("FSS Misclassification Rate:", fss_misclass, "\n") # 0.08125

```

```{r fss_Accuracy}
fss_accuracy <- mean(fss_test_data$label_pred == fss_test_data$label)
cat("fss Accuracy:", fss_accuracy, "\n") # 0.91875

```

```{r fss_ROC_AUC}
fss_roc <- roc(fss_test_data$label, fss_test_data$label_pred)
fss_auc <- auc(fss_roc)
cat("fss AUC:", fss_auc, "\n") # 0.7804
plot(fss_roc, main = "fss ROC Curve") # Make the limits of the axes 0-1

```

```{r fss_f1_score}
fss_f1_score <- F1_Score(y_pred = fss_test_data$label_pred, y_true = fss_test_data$label, positive = "1")
cat("F1 Score:", fss_f1_score, "\n") # 0.5517241

```


### Random forest

```{r rf_Training_randomForest}
set.seed(seed)

rf_test_data_top_subset <- test_data_2_top_subset
rf_train_data_top_subset <- train_data_2_top_subset

rf_train_data_top_subset$label <- as.factor(rf_train_data_top_subset$label) 
rf_test_data_top_subset$label <- as.factor(rf_test_data_top_subset$label) 

# RF model
start_time_rf <- Sys.time()

rf.rna_features_rf <- randomForest(label ~ ., data = rf_train_data_top_subset, mtry = 100, num.trees = 5, importance = TRUE) # RF Model

end_time_rf <- Sys.time()
rf_training_time <- end_time_rf - start_time_rf
cat("Random Forest model (randomForest) training time:", rf_training_time, "\n")

rf_importance <- randomForest::importance(rf.rna_features_rf)
rf_importance_vars <- sum(abs(rf_importance[, 3]) > 0)
cat("Number of features utilized in Random Forest model (randomForest):", rf_importance_vars, "\n") # This model has 785 "important" variables based on the metric of "MeanDecreaseAccuracy"

```

```{r rf_Training_ranger}
set.seed(seed)

rf_test_data <- as.data.frame(test_data_2)
rf_train_data <- as.data.frame(train_data_2)

rf_train_data$label <- as.factor(rf_train_data$label) 
rf_test_data$label <- as.factor(rf_test_data$label)

x <- rf_train_data[, -which(colnames(rf_train_data) == "label")]
y <- rf_train_data$label

# RF model
start_time_rf <- Sys.time()

rf.rna_features_ranger <- ranger::ranger(x = x, y = y, mtry = 5000, num.trees = 5, importance = "impurity") # RF Model

end_time_rf <- Sys.time()
rf_training_time <- end_time_rf - start_time_rf
cat("Random Forest model (ranger) training time:", rf_training_time, "\n")

rf_importance_ranger_vars <- sum(abs(rf.rna_features_ranger$variable.importance) > 0) 
cat("Number of features utilized in Random Forest model (randomForest):", rf_importance_ranger_vars, "\n") # This model has 100 "important" variables when utilizing the ranger library

```

We run into memory issues when trying to run randomForest::randomForest on the entirety of the task 2 training data (with 30,121 features). As such, we proceed with only running the random forest on the subset of the top 1000 features that are most highly correlated with the label. After researching, found that conducting a random forest with ranger::ranger is much more computationally efficient (https://www.css.cornell.edu/faculty/dgr2/_static/files/R_html/CompareRandomForestPackages.html#4_Conclusion). When we use ranger:ranger we are able to build our randomForest on the entirity of the training data!

```{r rf_Prediction}
# Predictions
start_time_rf_pred <- Sys.time()
yhat.rf <- predict(rf.rna_features_rf, newdata=rf_test_data_top_subset)

end_time_rf_pred <- Sys.time()
rf_prediction_time <- end_time_rf_pred - start_time_rf_pred
cat("Random Forest (randomForest) prediction time:", rf_prediction_time, "\n")

start_time_rf_pred <- Sys.time()
yhat.rf_ranger <- predict(rf.rna_features_ranger, data=rf_test_data)

end_time_rf_pred <- Sys.time()
rf_prediction_time <- end_time_rf_pred - start_time_rf_pred
cat("Random Forest (ranger) prediction time:", rf_prediction_time, "\n")

```

```{r rf_confusion}
rf_confusion_matrix <- table(Predicted = yhat.rf, Actual = rf_test_data_top_subset$label)
print(rf_confusion_matrix)
rf_balanced_accuracy <- fun_calc_balanced_accuracy(rf_confusion_matrix,
                                                   pos_label = "1",
                                                   neg_label = "-1")

cat("The balanced accuracy (randomForest) is:", rf_balanced_accuracy, "\n") # 0.7974882


predicted_values <- yhat.rf_ranger$predictions

rf_confusion_matrix_ranger <- table(Predicted = yhat.rf_ranger$predictions, Actual = rf_test_data$label)
print(rf_confusion_matrix_ranger)
rf_balanced_accuracy_ranger <- fun_calc_balanced_accuracy(rf_confusion_matrix_ranger,
                                                   pos_label = "1",
                                                   neg_label = "-1")

cat("The balanced accuracy (ranger) is:", rf_balanced_accuracy_ranger, "\n") # 0.8223443 

```

We notice that both the random forest built on only the subset of top 1000 features (randomForest), as well as the random forest built on the entire training data (ranger) have moderately successful results. This provides us with confidence in our approach of utilizing the subset of the top 1000 features within training data for the FSS model. Moving forward for the random forest, we will utilize the ranger:ranger random forest built on the entire training data as it performs better in terms of balanced accuracy as well as having less than half the number of features (368 as opposed to 785).  

```{r rf_misclass}
rf_misclassification_rate <- mean(yhat.rf_ranger$predictions != rf_test_data$label)
cat("Random Forest Misclassification Rate:", rf_misclassification_rate, "\n") # 0.06875 

```

```{r rf_accuracy}
rf_accuracy <- mean(yhat.rf_ranger$predictions == rf_test_data$label)
cat("Random Forest Accuracy:", rf_accuracy, "\n") # 0.93125 

```

```{r rf_roc_auc}
roc_rf <- roc(rf_test_data$label, as.numeric(yhat.rf_ranger$prediction))
cat("Random Forest AUC:", auc(roc_rf), "\n")
plot(roc_rf, main = "Random Forest ROC Curve") # 0.8223443 

```

```{r rf_f1_score}
rf_f1_score <- F1_Score(y_pred = yhat.rf_ranger$prediction, y_true = rf_test_data$label, positive = "1")
cat("F1 Score:", rf_f1_score, "\n") # 0.6206897

```


## T2.3: additional feature selection methods

Train and evaluate up to three additional feature selection methods, other than those in T2.2, which may include methods not covered in the course, with the goal of achieving high balanced accuracy using a minimal number of selected features. Provide an explanation of each new method.

For T2.2 and T2.3, your evaluation results should include the balanced accuracy and the number of selected features achieved by each method for various numbers of selected features. You may also compare the balanced accuracy achieved by different methods for a similar number of selected features.

### Elastic net

```{r}
set.seed(seed)
elastic_net_train_data <- train_data_2
elastic_net_test_data <- test_data_2

# Replace -1 with 0 for compatibility with logistic regression
elastic_net_train_data$label[elastic_net_train_data$label == -1] <- 0
elastic_net_test_data$label[elastic_net_test_data$label == -1] <- 0

# Prepare matrices and labels
elastic_net_train_data_matrix <- as.matrix(elastic_net_train_data[, -1])  # Exclude label column
elastic_net_test_data_clean_matrix <- as.matrix(elastic_net_test_data[, -1])  # Exclude label column
train_labels <- elastic_net_train_data$label
test_labels <- elastic_net_test_data$label

# Define a range of alpha values to loop over
alpha_values <- seq(0.4, 0.8, by = 0.03)

# Initialize a list to store results for each alpha
results <- list()

# Loop over alpha values
for (alpha in alpha_values) {
  cat("Testing alpha =", alpha, "\n")
  
  # Perform cross-validation for the current alpha
  elastic_cv_fit <- cv.glmnet(elastic_net_train_data_matrix, train_labels, alpha = alpha, lambda = grid, 
                              nfolds = 5, family = "binomial", weights = class_weights)
  
  # Extract the lambda values and their corresponding balanced accuracies
  elastic_lambdas <- elastic_cv_fit$lambda
  elastic_cv_results <- sapply(elastic_lambdas, function(lambda) {
    pred.cv <- predict(elastic_cv_fit, newx = elastic_net_train_data_matrix, s = lambda, 
                       family = "binomial", type = "response") # Predicting probabilities
    elastic_pred.cv_binary <- ifelse(pred.cv > 0.5, 1, 0) # Classify probabilities
    confusion <- table(Predicted = elastic_pred.cv_binary, Actual = train_labels)
    balanced_acc <- calculate_balanced_accuracy(confusion) # Calculate balanced accuracy
    num_predictors <- sum(coef(elastic_cv_fit, s = lambda) != 0) - 1 # Number of coefficients
    custom_score <- balanced_acc - (0.001 * num_predictors)
    return(c(balanced_acc, num_predictors, custom_score))
  })
  
  elastic_balanced_accuracies <- elastic_cv_results[1, ]
  elastic_num_predictors <- elastic_cv_results[2, ]
  custom_scores <- elastic_cv_results[3, ]
  
  # Find the lambda with the highest balanced accuracy for this alpha
  elastic_best_lambda_index <- which.max(custom_scores)
  elastic_best_lambda <- elastic_lambdas[elastic_best_lambda_index]
  elastic_best_balanced_accuracy <- elastic_balanced_accuracies[elastic_best_lambda_index]
  elastic_best_num_predictors <- elastic_num_predictors[elastic_best_lambda_index]
  elastic_best_custom_score <- custom_scores[elastic_best_lambda_index]
  
  # Store results for this alpha
  results[[as.character(alpha)]] <- list(
    alpha = alpha,
    best_lambda = elastic_best_lambda,
    best_balanced_accuracy = elastic_best_balanced_accuracy,
    num_predictors = elastic_best_num_predictors,
    custom_score = elastic_best_custom_score
  )
}

# Find the overall best combination of alpha and lambda
best_result <- NULL
for (alpha in names(results)) {
  if (is.null(best_result) || results[[alpha]]$best_balanced_accuracy > best_result$best_balanced_accuracy) {
    best_result <- results[[alpha]]
  }
}

cat("Overall Best Alpha:", best_result$alpha, "\n")
cat("Best Lambda for Best Alpha:", best_result$best_lambda, "\n")
cat("Highest Balanced Accuracy:", best_result$best_balanced_accuracy, "\n")
cat("Number of Predictors for Best Combination:", best_result$num_predictors, "\n")
cat("Custom Score for Best Combination:", best_result$custom_score, "\n")
# Train the final model using the best alpha and lambda
elastic_final_model <- glmnet(elastic_net_train_data_matrix, train_labels, alpha = best_result$alpha, 
                              lambda = best_result$best_lambda, family = "binomial", weights = class_weights)
elastic_pred.test <- predict(elastic_final_model, newx = elastic_net_test_data_clean_matrix, 
                             family = "binomial", type = "response")
elastic_pred.test_binary <- ifelse(elastic_pred.test > 0.5, 1, 0)
elastic_confusion_test <- table(Predicted = elastic_pred.test_binary, Actual = test_labels)
elastic_test_balanced_accuracy <- calculate_balanced_accuracy(elastic_confusion_test)

cat("Balanced Accuracy on Test Data with Best Alpha and Lambda:", elastic_test_balanced_accuracy, "\n")
```
```{r elastic_Accuracy}
elastic_accuracy <- mean(elastic_pred.test_binary == elastic_net_test_data$label)
cat("Elastic Net Accuracy:", elastic_accuracy, "\n") 
```

```{r elastic_ROC_AUC}
elastic_roc <- roc(elastic_net_test_data$label, elastic_pred.test_binary)
elastic_auc <- auc(elastic_roc)
cat("Elastic net AUC:", elastic_auc, "\n") #
plot(elastic_roc, main = "Elastic net ROC Curve") # Make the limits of the axes 0-1
```

```{r elastic_f1_score}
elastic_f1_score <- F1_Score(y_pred = elastic_pred.test_binary, y_true = elastic_net_test_data$label, positive = "1")
cat("F1 Score:", elastic_f1_score, "\n") 
```

```{r elastic worst_score}
# Extract the combination of alpha and lambda that gives the worst score
worst_score_result <- NULL
for (alpha in names(results)) {
  if (is.null(worst_score_result) || results[[alpha]]$custom_score < worst_score_result$custom_score) {
    worst_score_result <- results[[alpha]]
  }
}

cat("Worst Score Alpha:", worst_score_result$alpha, "\n")
cat("Worst Lambda for Worst Score:", worst_score_result$best_lambda, "\n")
cat("Balanced Accuracy for Worst Score:", worst_score_result$best_balanced_accuracy, "\n")
cat("Number of Predictors for Worst Score:", worst_score_result$num_predictors, "\n")
cat("Custom Score for Worst Score:", worst_score_result$custom_score, "\n")

# Train the final model using the worst alpha and lambda
elastic_worst_model <- glmnet(elastic_net_train_data_matrix, train_labels, alpha = worst_score_result$alpha, 
                              lambda = worst_score_result$best_lambda, family = "binomial", weights = class_weights)
elastic_worst_pred_test <- predict(elastic_worst_model, newx = elastic_net_test_data_clean_matrix, 
                                   family = "binomial", type = "response")
elastic_worst_pred_test_binary <- ifelse(elastic_worst_pred_test > 0.5, 1, 0)
elastic_worst_confusion_test <- table(Predicted = elastic_worst_pred_test_binary, Actual = test_labels)
elastic_worst_test_balanced_accuracy <- calculate_balanced_accuracy(elastic_worst_confusion_test)

cat("Balanced Accuracy on Test Data with Worst Alpha and Lambda:", elastic_worst_test_balanced_accuracy, "\n")
```

```{r elastic worst balanced acc}
# Extract the combination of alpha and lambda that gave the worst balanced accuracy on the CV
worst_balanced_accuracy_result <- NULL
for (alpha in names(results)) {
  if (is.null(worst_balanced_accuracy_result) || results[[alpha]]$best_balanced_accuracy < worst_balanced_accuracy_result$best_balanced_accuracy) {
    worst_balanced_accuracy_result <- results[[alpha]]
  }
}

cat("Worst Balanced Accuracy Alpha:", worst_balanced_accuracy_result$alpha, "\n")
cat("Worst Lambda for Worst Balanced Accuracy:", worst_balanced_accuracy_result$best_lambda, "\n")
cat("Worst Balanced Accuracy on CV:", worst_balanced_accuracy_result$best_balanced_accuracy, "\n")
cat("Number of Predictors for Worst Balanced Accuracy:", worst_balanced_accuracy_result$num_predictors, "\n")
cat("Custom Score for Worst Balanced Accuracy:", worst_balanced_accuracy_result$custom_score, "\n")

# Train the final model using the worst balanced accuracy alpha and lambda
elastic_worst_bal_acc_model <- glmnet(elastic_net_train_data_matrix, train_labels, alpha = worst_balanced_accuracy_result$alpha, 
                                      lambda = worst_balanced_accuracy_result$best_lambda, family = "binomial", weights = class_weights)
elastic_worst_bal_acc_pred_test <- predict(elastic_worst_bal_acc_model, newx = elastic_net_test_data_clean_matrix, 
                                           family = "binomial", type = "response")
elastic_worst_bal_acc_pred_test_binary <- ifelse(elastic_worst_bal_acc_pred_test > 0.5, 1, 0)
elastic_worst_bal_acc_confusion_test <- table(Predicted = elastic_worst_bal_acc_pred_test_binary, Actual = test_labels)
elastic_worst_bal_acc_test_balanced_accuracy <- calculate_balanced_accuracy(elastic_worst_bal_acc_confusion_test)

cat("Balanced Accuracy on Test Data with Worst Balanced Accuracy Alpha and Lambda:", elastic_worst_bal_acc_test_balanced_accuracy, "\n")

```



#### Linear Elastic Net
```{r lin_elastic_net model} 
set.seed(seed)
# Rename datasets for linear elastic net regression
linear_elastic_net_train_data <- train_data_2
linear_elastic_net_test_data <- test_data_2

# Prepare the data
linear_elastic_net_train_data_matrix <- as.matrix(linear_elastic_net_train_data[, -1])  # Exclude label column
linear_elastic_net_test_data_clean_matrix <- as.matrix(linear_elastic_net_test_data[, -1])  # Exclude label column
linear_train_labels <- linear_elastic_net_train_data$label
linear_test_labels <- linear_elastic_net_test_data$label

# Define a range of alpha values to loop over
linear_alpha_values <- seq(0.4, 0.8, by = 0.03)

# Initialize a list to store results for each alpha
linear_results <- list()

# Loop over alpha values
for (alpha in linear_alpha_values) {
  cat("Testing alpha =", alpha, "\n")
  
  # Perform cross-validation for the current alpha
  linear_elastic_cv_fit <- cv.glmnet(linear_elastic_net_train_data_matrix, linear_train_labels, 
                                     alpha = alpha, lambda = grid, nfolds = 5, 
                                     weights = class_weights)
  
  # Extract the lambda values and their corresponding balanced accuracies
  linear_elastic_lambdas <- linear_elastic_cv_fit$lambda
  linear_elastic_cv_results <- sapply(linear_elastic_lambdas, function(lambda) {
    pred.cv <- predict(linear_elastic_cv_fit, newx = linear_elastic_net_train_data_matrix, s = lambda)  # Linear predictions
    pred.cv_binary <- ifelse(pred.cv > 0, 1, -1)  # Classification threshold for linear predictions
    confusion <- table(Predicted = pred.cv_binary, Actual = linear_train_labels)
    balanced_acc <- linear_lasso_calculate_balanced_accuracy(confusion)  # Calculate balanced accuracy
    num_predictors <- sum(coef(linear_elastic_cv_fit, s = lambda) != 0) - 1  # Number of coefficients
    custom_score <- balanced_acc - (0.001 * num_predictors)
    return(c(balanced_acc, num_predictors, custom_score))
  })
  
  linear_elastic_balanced_accuracies <- linear_elastic_cv_results[1, ]
  linear_elastic_num_predictors <- linear_elastic_cv_results[2, ]
  linear_custom_scores <- linear_elastic_cv_results[3, ]
  
  # Find the lambda with the highest balanced accuracy for this alpha
  linear_elastic_best_lambda_index <- which.max(linear_custom_scores)
  linear_elastic_best_lambda <- linear_elastic_lambdas[linear_elastic_best_lambda_index]
  linear_elastic_best_balanced_accuracy <- linear_elastic_balanced_accuracies[linear_elastic_best_lambda_index]
  linear_elastic_best_num_predictors <- linear_elastic_num_predictors[linear_elastic_best_lambda_index]
  linear_elastic_best_custom_score <- linear_custom_scores[linear_elastic_best_lambda_index]
  
  # Store results for this alpha
  linear_results[[as.character(alpha)]] <- list(
    alpha = alpha,
    best_lambda = linear_elastic_best_lambda,
    best_balanced_accuracy = linear_elastic_best_balanced_accuracy,
    num_predictors = linear_elastic_best_num_predictors,
    custom_score = linear_elastic_best_custom_score
  )
}

# Find the overall best combination of alpha and lambda
linear_best_result <- NULL
for (alpha in names(linear_results)) {
  if (is.null(linear_best_result) || linear_results[[alpha]]$best_balanced_accuracy > linear_best_result$best_balanced_accuracy) {
    linear_best_result <- linear_results[[alpha]]
  }
}

cat("Overall Best Alpha:", linear_best_result$alpha, "\n")
cat("Best Lambda for Best Alpha:", linear_best_result$best_lambda, "\n")
cat("Highest Balanced Accuracy:", linear_best_result$best_balanced_accuracy, "\n")
cat("Number of Predictors for Best Combination:", linear_best_result$num_predictors, "\n")
cat("Custom Score for Best Combination:", linear_best_result$custom_score, "\n")

# Train the final model using the best alpha and lambda
linear_elastic_final_model <- glmnet(linear_elastic_net_train_data_matrix, linear_train_labels, 
                                     alpha = linear_best_result$alpha, 
                                     lambda = linear_best_result$best_lambda, 
                                     weights = class_weights)

# Predictions for test data
linear_elastic_pred_test <- predict(linear_elastic_final_model, newx = linear_elastic_net_test_data_clean_matrix)  # Linear predictions
linear_elastic_pred_test_binary <- ifelse(linear_elastic_pred_test > 0, 1, -1)  # Classification threshold

# Confusion matrix and balanced accuracy for test data
linear_elastic_confusion_test <- table(Predicted = linear_elastic_pred_test_binary, 
                                       Actual = linear_test_labels)
linear_elastic_test_balanced_accuracy <- linear_lasso_calculate_balanced_accuracy(linear_elastic_confusion_test)

# Output the results
cat("Linear Elastic Net Best Lambda:", linear_elastic_best_lambda, "\n")
cat("Linear Elastic Net Highest Balanced Accuracy on CV:", linear_elastic_best_balanced_accuracy, "\n")
cat("Linear Elastic Net Number of Predictors for Best Lambda:", linear_elastic_best_num_predictors, "\n")
cat("Linear Elastic Net Balanced Accuracy on Test Data:", linear_elastic_test_balanced_accuracy, "\n")

```


```{r linear_elastic_Accuracy}
lin_elastic_accuracy <- mean(linear_elastic_pred_test_binary == linear_elastic_net_test_data$label)
cat("Elastic Net Accuracy:", lin_elastic_accuracy, "\n") 
```

```{r linear_elastic_ROC_AUC}
lin_elastic_roc <- roc(linear_elastic_net_test_data$label, linear_elastic_pred_test_binary)
lin_elastic_auc <- auc(lin_elastic_roc)
cat("Elastic net AUC:", lin_elastic_auc, "\n") #
plot(lin_elastic_roc, main = "Elastic net ROC Curve") # Make the limits of the axes 0-1
```

```{r linear_elastic_f1_score}
lin_elastic_f1_score <- F1_Score(y_pred = linear_elastic_pred_test_binary, y_true = linear_elastic_net_test_data$label, positive = "1")
cat("F1 Score:", lin_elastic_f1_score, "\n") 
```




### XGBoost

```{r xgboost}
set.seed(seed)
# Replace -1 with 0 in labels
xgb_train_data <- train_data_2
xgb_test_data <- test_data_2
xgb_test_data$label[xgb_test_data$label == -1] <- 0
xgb_train_data$label[xgb_train_data$label == -1] <- 0

# Prepare matrices for xgboost test
xgb_train_matrix <- xgb.DMatrix(data = as.matrix(xgb_train_data[, -1]), label = xgb_train_data$label)
xgb_test_matrix <- xgb.DMatrix(data = as.matrix(xgb_test_data[, -1]), label = xgb_test_data$label)

# Set scale_pos_weight for imbalanced data
scale_pos_weight <- sum(xgb_train_data$label == 0) / sum(xgb_train_data$label == 1)

# Function to calculate balanced accuracy
calculate_balanced_accuracy <- function(confusion) {
  if (all(dim(confusion) == c(2, 2))) {
    balanced_acc <- (1/2) * (confusion[2, 2] / (confusion[2, 2] + confusion[1, 2])) + 
                    (1/2) * (confusion[1, 1] / (confusion[1, 1] + confusion[2, 1]))
  } else {
    balanced_acc <- NA
  }
  return(balanced_acc)
}

# Hyperparameter tuning grid 
grid_search <- expand.grid(
  max_depth = c(3, 5, 6),
  eta = c(0.01, 0.05, 0.1, 0.5),
  nrounds = c(500, 1000)
)

best_model <- NULL
xgb_best_balanced_accuracy <- 0
selected_features <- NULL

xgb_model_start <- Sys.time()
# Looping the xgboost mnodel over different parameters
for (i in 1:nrow(grid_search)) {
  params <- list(
    eta = grid_search$eta[i],
    max_depth = grid_search$max_depth[i],
    objective = "binary:logistic",
    scale_pos_weight = scale_pos_weight,
    eval_metric = "logloss",
    nthread = 4
  )
  
  xgb_cv <- xgb.cv(
    params = params,
    data = xgb_train_matrix,
    nrounds = grid_search$nrounds[i],
    nfold = 5,
    verbose = 0,
    early_stopping_rounds = 50,
    prediction = TRUE
  )
  
  # Calculate balanced accuracy for each fold
  balanced_accuracies <- sapply(1:5, function(fold) {
    fold_indices <- xgb_cv$folds[[fold]]
    fold_predictions <- ifelse(xgb_cv$pred[fold_indices] > 0.5, 1, 0) #Predictions and classifications
    fold_confusion_matrix <- table(Predicted = fold_predictions, Actual = xgb_train_data$label[fold_indices])
    calculate_balanced_accuracy(fold_confusion_matrix)
  })
  
  xgb_mean_balanced_accuracy <- mean(balanced_accuracies, na.rm = TRUE)
  
  # Train final model if current balanced accuracy is better
  if (!is.na(xgb_mean_balanced_accuracy) && xgb_mean_balanced_accuracy > xgb_best_balanced_accuracy) {
    xgb_best_balanced_accuracy <- xgb_mean_balanced_accuracy
    best_model <- xgb.train(
      params = params,
      data = xgb_train_matrix,
      nrounds = xgb_cv$best_iteration
    )
    
    # Get feature importance for the best model
    importance_matrix <- xgb.importance(feature_names = colnames(xgb_train_data[, -1]), model = best_model)
    selected_features <- importance_matrix$Feature
  }
}

# Test predictions and new balanced accuracy
xgb_predictions <- predict(best_model, newdata = xgb_test_matrix)
xgb_predictions <- ifelse(xgb_predictions > 0.5, 1, 0)

xgb_confusion_matrix <- table(Predicted = xgb_predictions, Actual = xgb_test_data$label)

# Calculate Balanced Accuracy
sensitivity <- xgb_confusion_matrix[2, 2] / (xgb_confusion_matrix[2, 2] + xgb_confusion_matrix[1, 2])
specificity <- xgb_confusion_matrix[1, 1] / (xgb_confusion_matrix[1, 1] + xgb_confusion_matrix[2, 1])
final_balanced_accuracy <- (sensitivity + specificity) / 2

# Output results
cat("Final Balanced Accuracy for XGboost:", final_balanced_accuracy, "\n")
cat("Selected Features Count XGboost:", length(selected_features), "\n")
cat("The best parameters for our xgboost model were: eta :", best_model$params$eta, " and max depths: ", best_model$params$max_depth, "\n")

xgb_model_end <- Sys.time()
xgb_model_total_time <- xgb_model_end - xgb_model_start
cat("Time taken for xgb model training and selection:", xgb_model_total_time, "\n")

# Plot Feature Importance

xgb.plot.importance(importance_matrix)


```

```{r xgb_Accuracy}
xgb_accuracy <- mean(xgb_predictions == xgb_test_data$label)
cat("XGboost Accuracy:", xgb_accuracy, "\n") 
```

```{r xgb_ROC_AUC}
xgb_roc <- roc(xgb_test_data$label, xgb_predictions)
xgb_auc <- auc(xgb_roc)
cat("XGboost AUC:", xgb_auc, "\n") #
plot(xgb_roc, main = "XGboost ROC Curve") # Make the limits of the axes 0-1
```

```{r xgb_f1_score}
xgb_f1_score <- F1_Score(y_pred = xgb_predictions, y_true = xgb_test_data$label, positive = "1")
cat("F1 Score:", xgb_f1_score, "\n") 
```





### Recursive Feature Elimination (RFE) SVM 


```{r sve_rfe_Training}
set.seed(seed)

svm_rfe_test_data <- test_data_2
svm_rfe_train_data <- train_data_2

# Give factors values that are valid for column names
y_train <- as.character(svm_rfe_train_data$label)
y_train[y_train == "-1"] <- "no_bind"
y_train[y_train == "1"] <- "bind"
y_train <- as.factor(y_train)

# Separate out the explanatory variables
x_train <- svm_rfe_train_data[, -which(colnames(svm_rfe_train_data) == "label")]
x_train <- as.data.frame(x_train)
dim(x_train) # (640 30121)

# Remove variables in the training data that have constant values to allow for future scaling
nzv <- nearZeroVar(x_train, freqCut = 99/1) #update the name of this variable and tune the parameter for the cutoff (currently at 95%?) (98/2)
if (length(nzv) > 0) {
  x_train <- x_train[, -nzv]
}
# x_train <- x_train[, apply(x, 2, function(column) length(unique(column)) > 1)]
dim(x_train) # (640 12378)

# SVM RFE model
start_time_sve_rfe <- Sys.time()

# Preprocess the data: centering and scaling the predictors  ### update this comment
preProc <- preProcess(x_train, method = c("center", "scale"))
x_train <- predict(preProc, x_train)

# Define control setup for RFE, utilize cross-validation with 10-folds
ctrl <- rfeControl(functions = caretFuncs, method = "cv", number = 10)

seq_sizes_1 <- seq(3, 30, by = 3)
seq_sizes_2 <- seq(35, 100, by = 5)
seq_sizes_3 <- seq(110, 200, by = 10)
seq_sizes_4 <- seq(300, 1300, by = 100)

# RFE with linear kernel SVM
svm_rfe <- rfe(x = x_train, y = y_train, sizes = c(1, seq_sizes_1, seq_sizes_2, seq_sizes_3, seq_sizes_4), rfeControl = ctrl, method = "svmLinear") # tune the sizes parameter


end_time_svm_rfe <- Sys.time()
svm_rfe_training_time <- end_time_svm_rfe - start_time_sve_rfe
cat("SVM with RFE model training time:", svm_rfe_training_time, "\n")

print(svm_rfe)

```

```{r svm_rfe_Prediction}
# Extract the features identified by the SVM with RFE
start_time_svm_rfe_pred <- Sys.time()
svm_rfe_vars <- predictors(svm_rfe)

# Limit the training and test data to solely the identified features 
x_test <- svm_rfe_test_data[, svm_rfe_vars, drop = FALSE]
y_test <- as.factor(svm_rfe_test_data$label)

x_train <- svm_rfe_train_data[, svm_rfe_vars, drop = FALSE]
y_train <- as.factor(svm_rfe_train_data$label)

# Train the SVM on solely the identified features
final_svm <- train(x_train, y_train, method = "svmLinear", trControl = trainControl(method = "none"))

# Predictions
yhat.svm_rfe <- predict(final_svm, newdata = x_test)
end_time_svm_rfe_pred <- Sys.time()
svm_rfe_prediction_time <- end_time_svm_rfe_pred - start_time_svm_rfe_pred
cat("SVM with RFE prediction time:", svm_rfe_prediction_time, "\n")

```


```{r svm_rfe_confusion}
svm_rfe_confusion_matrix <- table(Predicted = yhat.svm_rfe, Actual = y_test)
print(svm_rfe_confusion_matrix)
svm_rfe_balanced_accuracy <- fun_calc_balanced_accuracy(svm_rfe_confusion_matrix,
                                                        pos_label = "1",
                                                        neg_label = "-1")

cat("The balanced accuracy for SVM with RFE is:", svm_rfe_balanced_accuracy, "\n") # 0.8257457 (0.8574045)

```

```{r svm_rfe_misclass}
svm_rfe_misclassification_rate <- mean(yhat.svm_rfe != y_test)
cat("SVM with RFE Misclassification Rate:", svm_rfe_misclassification_rate, "\n") # 0.0625 (0.06875)

```

```{r svm_rfe_accuracy}
svm_rfe_accuracy <- mean(yhat.svm_rfe == y_test)
cat("SVM with RFE Accuracy:", svm_rfe_accuracy, "\n") # 0.9375 (0.93125)

```

```{r svm_rfe_roc_auc}
roc_svm_rfe <- roc(y_test, as.numeric(yhat.svm_rfe))
cat("SVM with RFE AUC:", auc(roc_svm_rfe), "\n")
plot(roc_svm_rfe, main = "SVM with RFE ROC Curve") # 0.8257457 (0.8574045)

```

```{r svm_rfe_f1_score}
svm_rfe_f1_score <- F1_Score(y_pred = yhat.svm_rfe, y_true = y_test, positive = "1")
cat("F1 Score:", svm_rfe_f1_score, "\n") # 0.6428571 (0.6451613)

```

